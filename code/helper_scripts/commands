from datetime import datetime

models = [
    '"bert-base-multilingual-uncased"',
    '"jplu/tf-xlm-roberta-base"',
    '"./robeczech/noeol-210323/"']
ls = "0.03"
lr = "5e-5"
lr_list = ["1e-5","2e-5","5e-5"]
debug = False
test = False
model = '"bert-base-multilingual-cased"'
batch = 64
fine_lr = 0
warmup = 0
checkp = 0
layers = None

schedules = [None, '"c:1"', '"i:1"']

s1 = "qsub -cwd"
s2 = "-q gpu.q -l gpu=1,mem_free=16G,h_data=16G -pe smp 2 $HOME/env_diplomka/bin/python3 morpho_tagger_2.py ~doubrap1/pdt/pdt-3.5 --embeddings ~doubrap1/embeddings/cs_lindat4gw/forms.vectors-w5-d300-ns5.npz"
s3 = "-q gpu.q -l gpu=2,mem_free=32G,h_data=32G -pe smp 2 $HOME/env_diplomka/bin/python3 bert_finetunning_simple.py ~doubrap1/pdt/pdt-3.5"

class Experiment:
    def __init__(self, name):
        now = datetime.now()
        timestr = now.strftime("%m%d_%H%M%S")
        self.name = "com_out2/$(date +%m%d-%H%M)" + "_" + name


def generate(name):
    e = Experiment(name)
    global checkp
    output = [s1]
    output.append("-o " + e.name + "_o" + str(checkp))
    output.append("-e " + e.name + "_e" + str(checkp))
    output.append("-N " + "tl_" + str(checkp))


    if name == "simple":
        output.append(s3)
    else:
        output.append(s2)

    if debug:
        output.append("--debug 1")

    output.append("--checkp " + '"ch' + str(checkp) + '"')
    checkp = checkp + 1
    if layers != None:
        output.append("--layers " + '"' + layers + '"' )

    if warmup != None and warmup != 0:
        output.append("--warmup_decay " + warmup)
    return output


def generate_baseline():
    name = "baseline"
    return generate(name)


def generate_LS():
    name = "base_ls"
    output = generate(name)
    output.append("--label_smoothing " + ls)

    return output


def generate_bertemb(model):
    name = "embed"
    output = generate(name)
    output.append("--label_smoothing " + ls)
    output.append("--bert " + model)

    if warmup != None:
        output.append("--epochs 60:1e-3")
    else:
        output.append("--epochs 40:1e-3,20:1e-4")
    return output

def generate_bertfine(model,lr=None):
    name = "bertfine"
    if len(model.split(":")) == 1:
        name = "full"
    output = generate(name)
    output.append("--label_smoothing " + ls)
    output.append("--bert_model " + model)
    output.append("--cont " + "1")
    if lr is  None:
        if warmup != None:
            output.append('--epochs "5:3e-5"')
        else:
            output.append('--epochs "2:1e-6"')
    else:
        output.append('--epochs "10:' + str(lr) + '"')
    output.append("--accu " + "16")
    output.append("--batch_size " + str(batch))
    return output

def generate_bertsimple(model, epochs_lr):
    name = "simple"
    output = generate(name)
    output.append("--label_smoothing " + ls)
    output.append("--bert " + model)
    output.append("--cont " + "1")
    output.append('--factors "Lemmas,Tags"')
    output.append("--epochs " + epochs_lr)
    output.append("--batch_size " + str(batch))
    output.append("--accu 16")
    return output

def generate_id(expe):
    output = []
    ####name
    if model == models[0]:
        output.append('"mBERT"')
    elif model == models[1]:
        output.append('"roBERTa"')

    ###expe
    output.append(expe)

    if (fine_lr > 0):
        output.append('"f' + str(fine_lr) + '"')
    elif (warmup != None):
        output.append('"warm"')
    else:
        output.append('"base"')
    output.append('"' + str(lr) + '"')
    output.append('"' + str(batch) + '"')
    output.append('"' + "all" + '"')

    # print("fi=`ls -t outputs | head -1`")
    start = "grep  '^Test' outputs/$fi | awk -F'[ ,]' 'BEGIN{OFS=" + '"\\t";} {print ' + ",".join(
        output) + "," + "$4, $13, $16, $19, $22;}' >> final_results"

    print(start)

#TODO warmup a optimizers
####################### bez bert≈Ø #######################################
print(" ".join(generate_baseline()))
#generate_id('"base"')
print(" ".join(generate_LS()))
#generate_id('"ls"')

################### embeddings ########################################
for m in models:
    fine_lr = 0
    model = m
    batch = 64
    ################### embeddings ########################################
    for w in schedules:
        warmup = w
        print(" ".join(generate_bertemb(model)))
        #generate_id('"embed"')


################### finetunning ######################
for l in [None, "att"]:
    layers = l
    for m in models:
        model = m
        for w in schedules:
            warmup = w
            batch = 4
            print(" ".join(generate_bertfine('"checkpoints/ch' + str(checkp - len(models)*len(schedules)) +  ":" + model[1:-1] + '"')))
            #generate_id('"fine"')

################### simple ######################
layers = None
for m in models:
    model = m
    #TODO k modelu pridat jeste cestu k checkpointu
    for w in [None, '"c:1"', '"i:1"']:
        warmup = w
        batch = 4
        print(" ".join(generate_bertsimple(model, '"5:3e-5"')))
        #generate_id('"fine"')


################### full ######################

layers = None
for l in lr_list:
    for m in models:
        model = m
        for w in schedules:
            warmup = w
            batch = 4
            print(" ".join(generate_bertfine(model, l)))
            #generate_id('"fine"')