# import os
#
import tensorflow as tf
# import tensorflow_datasets
# import torch
# from torch.utils.data import TensorDataset
#
# import transform_data as td
# from utils import (convert_examples_to_features,
#                         output_modes, processors)
# from sklearn import model_selection
# import random
#
# #https://github.com/huggingface/transformers/issues/1354
# #https://github.com/huggingface/transformers/issues/2251
#
# #use own dataset: https://github.com/ThilinaRajapakse/pytorch-transformers-classification
# #novejsi: https://github.com/ThilinaRajapakse/simpletransformers
#
# #first trained on glue: https://www.tensorflow.org/datasets/catalog/glue
#
# #TODO needed:
# '''
# tensorboardX
# tensorboard
# scikit-learn
# seqeval
# '''
#
# from transformers import (
#     BertConfig,
#     BertForSequenceClassification,
#     BertTokenizer,
#     TFBertForSequenceClassification,
#     glue_convert_examples_to_features,
#     glue_processors,
# )
#
#
# # script parameters
# BATCH_SIZE = 32
# EVAL_BATCH_SIZE = BATCH_SIZE * 2
# USE_XLA = False
# USE_AMP = False
# EPOCHS = 3
#
# #sst-2 is dataset for sentiment
# TASK = "sst-2"
#
# if TASK == "sst-2":
#     TFDS_TASK = "sst2"
# elif TASK == "sts-b":
#     TFDS_TASK = "stsb"
# else:
#     TFDS_TASK = TASK
#
# num_labels = 3
# print(num_labels)
#
# tf.config.optimizer.set_jit(USE_XLA)
# tf.config.optimizer.set_experimental_options({"auto_mixed_precision": USE_AMP})
#
# # Load tokenizer and model from pretrained model/vocabulary. Specify the number of labels to classify (2+: classification, 1: regression)
# config = BertConfig.from_pretrained("bert-base-cased", num_labels=num_labels)
# tokenizer = BertTokenizer.from_pretrained("bert-base-cased")
# model = TFBertForSequenceClassification.from_pretrained("bert-base-cased", config=config)
#
#
# #Get train and validation datasets
# all = td.get_items()
# train, test = model_selection.train_test_split(all)
#
# train_examples = len(train)
# valid_examples = len(test)
#
# # Prepare dataset for GLUE as a tf.data.Dataset instance
# train_dataset = glue_convert_examples_to_features(train, tokenizer, 128, label_list=['0','1','2'], output_mode='classification')
# valid_dataset = glue_convert_examples_to_features(test, tokenizer, 128, label_list=['0','1','2'], output_mode='classification')
#
#
#
#
# # MNLI expects either validation_matched or validation_mismatched
# # valid_dataset = glue_convert_examples_to_features(data["validation"], tokenizer, 128, TASK)
# #random.shuffle(train_dataset)
#
#
# # Prepare training: Compile tf.keras model with optimizer, loss and learning rate schedule
# opt = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08)
# if USE_AMP:
#     # loss scaling is currently required when using mixed precision
#     opt = tf.keras.mixed_precision.experimental.LossScaleOptimizer(opt, "dynamic")
#
#
# if num_labels == 1:
#     loss = tf.keras.losses.MeanSquaredError()
# else:
#     loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
#
# metric = tf.keras.metrics.SparseCategoricalAccuracy("accuracy")
# model.compile(optimizer=opt, loss=loss, metrics=[metric])
#
# # Train and evaluate using tf.keras.Model.fit()
# train_steps = train_examples// BATCH_SIZE
# valid_steps = valid_examples // EVAL_BATCH_SIZE
#
#
# history = model.fit(
#     train_dataset,
#     epochs=EPOCHS,
#     steps_per_epoch=train_steps,
#     validation_data=valid_dataset,
#     validation_steps=valid_steps,
# )
#
# # Save TF2 model
# os.makedirs("./save/", exist_ok=True)
# model.save_pretrained("./save/")
#
# if TASK == "mrpc":
#     # Load the TensorFlow model in PyTorch for inspection
#     # This is to demo the interoperability between the two frameworks, you don't have to
#     # do this in real life (you can run the inference on the TF model).
#     pytorch_model = BertForSequenceClassification.from_pretrained("./save/", from_tf=True)
#
#     # Quickly test a few predictions - MRPC is a paraphrasing task, let's see if our model learned the task
#     sentence_0 = "This research was consistent with his findings."
#     sentence_1 = "His findings were compatible with this research."
#     sentence_2 = "His findings were not compatible with this research."
#     inputs_1 = tokenizer.encode_plus(sentence_0, sentence_1, add_special_tokens=True, return_tensors="pt")
#     inputs_2 = tokenizer.encode_plus(sentence_0, sentence_2, add_special_tokens=True, return_tensors="pt")
#
#     del inputs_1["special_tokens_mask"]
#     del inputs_2["special_tokens_mask"]
#
#     pred_1 = pytorch_model(**inputs_1)[0].argmax().item()
#     pred_2 = pytorch_model(**inputs_2)[0].argmax().item()
#     print("sentence_1 is", "a paraphrase" if pred_1 else "not a paraphrase", "of sentence_0")
#     print("sentence_2 is", "a paraphrase" if pred_2 else "not a paraphrase", "of sentence_0")


import numpy as np
import transformers

config = transformers.BertConfig.from_pretrained("bert-base-uncased")
tokenizer = transformers.BertTokenizer.from_pretrained("bert-base-uncased")
model = transformers.TFBertForSequenceClassification.from_pretrained("bert-base-uncased", config=config)

dataset = [
    "Podmínkou koexistence jedince druhu Homo sapiens a společenství druhu Canis lupus je sjednocení akustické signální soustavy.",
    "U závodů na zpracování obilí, řízených mytologickými bytostmi je poměrně nízká produktivita práce vyvážena naprostou spolehlivostí.",
    "Vodomilní obratlovci nepatrných rozměrů nejsou ničím jiným, než vodomilnými obratlovci.",
]

batch = [tokenizer.encode(sentence) for sentence in dataset]
max_length = max(len(sentence) for sentence in batch)
batch_ids = np.zeros([len(batch), max_length], dtype=np.int32)
batch_masks = np.zeros([len(batch), max_length], dtype=np.int32)
for i in range(len(batch)):
    batch_ids[i, :len(batch[i])] = batch[i]
    batch_masks[i, :len(batch[i])] = 1

result = model([batch_ids, batch_masks])
print(result)

opt = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
metric = tf.keras.metrics.SparseCategoricalAccuracy("accuracy")
model.compile(optimizer=opt, loss=loss, metrics=[metric])

