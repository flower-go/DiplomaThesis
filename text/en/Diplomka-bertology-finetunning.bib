Automatically generated by Mendeley Desktop 1.19.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Kondratyuk2019,
abstract = {We present UDify, a multilingual multi-task model capable of accurately predicting universal part-of-speech, morphological features, lemmas, and dependency trees simultaneously for all 124 Universal Dependencies treebanks across 75 languages. By leveraging a multilingual BERT self-attention model pretrained on 104 languages, we found that fine-tuning it on all datasets concatenated together with simple softmax classifiers for each UD task can result in state-of-the-art UPOS, UFeats, Lemmas, UAS, and LAS scores, without requiring any recurrent or language-specific components. We evaluate UDify for multilingual learning, showing that low-resource languages benefit the most from cross-linguistic annotations. We also evaluate for zero-shot learning, with results suggesting that multilingual training provides strong UD predictions even for languages that neither UDify nor BERT have ever been trained on. Code for UDify is available at https://github.com/hyperparticle/udify.},
archivePrefix = {arXiv},
arxivId = {1904.02099},
author = {Kondratyuk, Dan and Straka, Milan},
eprint = {1904.02099},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Kondratyuk, Straka - 2019 - 75 Languages, 1 Model Parsing Universal Dependencies Universally.pdf:pdf},
journal = {EMNLP-IJCNLP 2019 - 2019 Conf. Empir. Methods Nat. Lang. Process. 9th Int. Jt. Conf. Nat. Lang. Process. Proc. Conf.},
month = {apr},
pages = {2779--2795},
publisher = {Association for Computational Linguistics},
title = {{75 Languages, 1 Model: Parsing Universal Dependencies Universally}},
url = {http://arxiv.org/abs/1904.02099},
year = {2019}
}
@article{Ettinger2019,
abstract = {Pre-training by language modeling has become a popular and successful approach to NLP tasks, but we have yet to understand exactly what linguistic capacities these pre-training processes confer upon models. In this paper we introduce a suite of diagnostics drawn from human language experiments, which allow us to ask targeted questions about the information used by language models for generating predictions in context. As a case study, we apply these diagnostics to the popular BERT model, finding that it can generally distinguish good from bad completions involving shared category or role reversal, albeit with less sensitivity than humans, and it robustly retrieves noun hypernyms, but it struggles with challenging inferences and role-based event prediction -- and in particular, it shows clear insensitivity to the contextual impacts of negation.},
archivePrefix = {arXiv},
arxivId = {1907.13528},
author = {Ettinger, Allyson},
eprint = {1907.13528},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Ettinger - 2019 - What BERT is not Lessons from a new suite of psycholinguistic diagnostics for language models.pdf:pdf},
journal = {arXiv},
month = {jul},
publisher = {arXiv},
title = {{What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models}},
url = {http://arxiv.org/abs/1907.13528},
year = {2019}
}
@article{Michel2019,
abstract = {Attention is a powerful and ubiquitous mechanism for allowing neural models to focus on particular salient pieces of information by taking their weighted average when making predictions. In particular, multi-headed attention is a driving force behind many recent state-of-the-art NLP models such as Transformer-based MT models and BERT. These models apply multiple attention mechanisms in parallel, with each attention "head" potentially focusing on different parts of the input, which makes it possible to express sophisticated functions beyond the simple weighted average. In this paper we make the surprising observation that even if models have been trained using multiple heads, in practice, a large percentage of attention heads can be removed at test time without significantly impacting performance. In fact, some layers can even be reduced to a single head. We further examine greedy algorithms for pruning down models, and the potential speed, memory efficiency, and accuracy improvements obtainable therefrom. Finally, we analyze the results with respect to which parts of the model are more reliant on having multiple heads, and provide precursory evidence that training dynamics play a role in the gains provided by multi-head attention.},
archivePrefix = {arXiv},
arxivId = {1905.10650},
author = {Michel, Paul and Levy, Omer and Neubig, Graham},
eprint = {1905.10650},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Michel, Levy, Neubig - 2019 - Are Sixteen Heads Really Better than One.pdf:pdf},
journal = {arXiv},
month = {may},
publisher = {arXiv},
title = {{Are Sixteen Heads Really Better than One?}},
url = {http://arxiv.org/abs/1905.10650},
year = {2019}
}
@article{Tenney2019,
abstract = {Pre-trained text encoders have rapidly advanced the state of the art on many NLP tasks. We focus on one such model, BERT, and aim to quantify where linguistic information is captured within the network. We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lower-level decisions on the basis of disambiguating information from higher-level representations.},
archivePrefix = {arXiv},
arxivId = {1905.05950},
author = {Tenney, Ian and Das, Dipanjan and Pavlick, Ellie},
eprint = {1905.05950},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Tenney, Das, Pavlick - 2019 - BERT Rediscovers the Classical NLP Pipeline.pdf:pdf},
journal = {ACL 2019 - 57th Annu. Meet. Assoc. Comput. Linguist. Proc. Conf.},
keywords = {necitovane},
mendeley-tags = {necitovane},
month = {may},
pages = {4593--4601},
publisher = {Association for Computational Linguistics (ACL)},
title = {{BERT Rediscovers the Classical NLP Pipeline}},
url = {http://arxiv.org/abs/1905.05950},
year = {2019}
}
@article{Yang2020,
abstract = {Pre-trained language models, such as BERT, have achieved significant accuracy gain in many natural language processing tasks. Despite its effectiveness, the huge number of parameters makes training a BERT model computationally very challenging. In this paper, we propose an efficient multi-stage layerwise training (MSLT) approach to reduce the training time of BERT. We decompose the whole training process into several stages. The training is started from a small model with only a few encoder layers and we gradually increase the depth of the model by adding new encoder layers. At each stage, we only train the top (near the output layer) few encoder layers which are newly added. The parameters of the other layers which have been trained in the previous stages will not be updated in the current stage. In BERT training, the backward computation is much more time-consuming than the forward computation, especially in the distributed training setting in which the backward computation time further includes the communication time for gradient synchronization. In the proposed training strategy, only top few layers participate in backward computation, while most layers only participate in forward computation. Hence both the computation and communication efficiencies are greatly improved. Experimental results show that the proposed method can achieve more than 110{\%} training speedup without significant performance degradation.},
archivePrefix = {arXiv},
arxivId = {2011.13635},
author = {Yang, Cheng and Wang, Shengnan and Yang, Chao and Li, Yuechuan and He, Ru and Zhang, Jingqiao},
eprint = {2011.13635},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Yang et al. - 2020 - Progressively Stacking 2.0 A Multi-stage Layerwise Training Method for BERT Training Speedup.pdf:pdf},
journal = {arXiv},
month = {nov},
publisher = {arXiv},
title = {{Progressively Stacking 2.0: A Multi-stage Layerwise Training Method for BERT Training Speedup}},
url = {http://arxiv.org/abs/2011.13635},
year = {2020}
}
@article{Yang2019b,
abstract = {Transformer-based pre-trained language models have proven to be effective for learning contextualized language representation. However, current approaches only take advantage of the output of the encoder's final layer when fine-tuning the downstream tasks. We argue that only taking single layer's output restricts the power of pre-trained representation. Thus we deepen the representation learned by the model by fusing the hidden representation in terms of an explicit HIdden Representation Extractor (HIRE), which automatically absorbs the complementary representation with respect to the output from the final layer. Utilizing RoBERTa as the backbone encoder, our proposed improvement over the pre-trained models is shown effective on multiple natural language understanding tasks and help our model rival with the state-of-the-art models on the GLUE benchmark.},
archivePrefix = {arXiv},
arxivId = {1911.01940},
author = {Yang, Junjie and Zhao, Hai},
eprint = {1911.01940},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Yang, Zhao - 2019 - Deepening Hidden Representations from Pre-trained Language Models.pdf:pdf},
month = {nov},
title = {{Deepening Hidden Representations from Pre-trained Language Models}},
url = {http://arxiv.org/abs/1911.01940},
year = {2019}
}
@techreport{Pruksachatkun,
abstract = {While pretrained models such as BERT have shown large gains across natural language understanding tasks, their performance can be improved by further training the model on a data-rich intermediate task, before fine-tuning it on a target task. However, it is still poorly understood when and why intermediate-task training is beneficial for a given target task. To investigate this, we perform a large-scale study on the pretrained RoBERTa model with 110 intermediate-target task combinations. We further evaluate all trained models with 25 probing tasks meant to reveal the specific skills that drive transfer. We observe that intermediate tasks requiring high-level inference and reasoning abilities tend to work best. We also observe that target task performance is strongly correlated with higher-level abilities such as coreference resolution. However , we fail to observe more granular correlations between probing and target task performance , highlighting the need for further work on broad-coverage probing benchmarks. We also observe evidence that the forgetting of knowledge learned during pretraining may limit our analysis, highlighting the need for further work on transfer learning methods in these settings.},
archivePrefix = {arXiv},
arxivId = {2005.00628v2},
author = {Pruksachatkun, Yada and Phang, Jason and Liu, Haokun and {Mon Htut}, Phu and Zhang, Xiaoyi and Pang, Richard Yuanzhe and Vania, Clara and Kann, Katharina and Bowman, Samuel R},
booktitle = {arxiv.org},
eprint = {2005.00628v2},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Pruksachatkun et al. - Unknown - Intermediate-Task Transfer Learning with Pretrained Models for Natural Language Understanding When and.pdf:pdf},
title = {{Intermediate-Task Transfer Learning with Pretrained Models for Natural Language Understanding: When and Why Does It Work?}},
url = {http://data.quora.com/First-Quora-DatasetRelease-}
}
@article{Chronopoulou2019,
abstract = {A growing number of state-of-the-art transfer learning methods employ language models pretrained on large generic corpora. In this paper we present a conceptually simple and effective transfer learning approach that addresses the problem of catastrophic forgetting. Specifically, we combine the task-specific optimization function with an auxiliary language model objective, which is adjusted during the training process. This preserves language regularities captured by language models, while enabling sufficient adaptation for solving the target task. Our method does not require pretraining or finetuning separate components of the network and we train our models end-to-end in a single step. We present results on a variety of challenging affective and text classification tasks, surpassing well established transfer learning methods with greater level of complexity.},
archivePrefix = {arXiv},
arxivId = {1902.10547},
author = {Chronopoulou, Alexandra and Baziotis, Christos and Potamianos, Alexandros},
eprint = {1902.10547},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Chronopoulou, Baziotis, Potamianos - 2019 - An Embarrassingly Simple Approach for Transfer Learning from Pretrained Language Models.pdf:pdf},
journal = {NAACL HLT 2019 - 2019 Conf. North Am. Chapter Assoc. Comput. Linguist. Hum. Lang. Technol. - Proc. Conf.},
keywords = {moznosti,necitovane},
mendeley-tags = {moznosti,necitovane},
month = {feb},
pages = {2089--2095},
publisher = {Association for Computational Linguistics (ACL)},
title = {{An Embarrassingly Simple Approach for Transfer Learning from Pretrained Language Models}},
url = {http://arxiv.org/abs/1902.10547},
volume = {1},
year = {2019}
}
@article{Tenney2019a,
abstract = {Contextualized representation models such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018) have recently achieved state-of-the-art results on a diverse array of downstream NLP tasks. Building on recent token-level probing work, we introduce a novel edge probing task design and construct a broad suite of sub-sentence tasks derived from the traditional structured NLP pipeline. We probe word-level contextual representations from four recent models and investigate how they encode sentence structure across a range of syntactic, semantic, local, and long-range phenomena. We find that existing models trained on language modeling and translation produce strong representations for syntactic phenomena, but only offer comparably small improvements on semantic tasks over a non-contextual baseline.},
archivePrefix = {arXiv},
arxivId = {1905.06316},
author = {Tenney, Ian and Xia, Patrick and Chen, Berlin and Wang, Alex and Poliak, Adam and McCoy, R Thomas and Kim, Najoung and {Van Durme}, Benjamin and Bowman, Samuel R. and Das, Dipanjan and Pavlick, Ellie},
eprint = {1905.06316},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Tenney et al. - 2019 - What do you learn from context Probing for sentence structure in contextualized word representations.pdf:pdf},
journal = {arXiv},
month = {may},
publisher = {arXiv},
title = {{What do you learn from context? Probing for sentence structure in contextualized word representations}},
url = {http://arxiv.org/abs/1905.06316},
year = {2019}
}
@techreport{Sun,
abstract = {Language model pre-training has proven to be useful in learning universal language representations. As a state-of-the-art language model pre-training model, BERT (Bidirectional En-coder Representations from Transformers) has achieved amazing results in many language understanding tasks. In this paper, we conduct exhaustive experiments to investigate different fine-tuning methods of BERT on text classification task and provide a general solution for BERT fine-tuning. Finally, the proposed solution obtains new state-of-the-art results on eight widely-studied text classification datasets. 1},
archivePrefix = {arXiv},
arxivId = {1905.05583v3},
author = {Sun, Chi and Qiu, Xipeng and Xu, Yige and Huang, Xuanjing},
eprint = {1905.05583v3},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Sun et al. - Unknown - How to Fine-Tune BERT for Text Classification.pdf:pdf},
keywords = {bertology,practise},
mendeley-tags = {bertology,practise},
title = {{How to Fine-Tune BERT for Text Classification?}},
url = {https://github.}
}
@article{Clark2019,
abstract = {Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data. Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classifiers). Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to BERT. BERT's attention heads exhibit patterns such as attending to delimiter tokens, specific positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors. We further show that certain attention heads correspond well to linguistic notions of syntax and coreference. For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy. Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in BERT's attention.},
archivePrefix = {arXiv},
arxivId = {1906.04341},
author = {Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D.},
eprint = {1906.04341},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Clark et al. - 2019 - What Does BERT Look At An Analysis of BERT's Attention.pdf:pdf},
journal = {arXiv},
month = {jun},
publisher = {arXiv},
title = {{What Does BERT Look At? An Analysis of BERT's Attention}},
url = {http://arxiv.org/abs/1906.04341},
year = {2019}
}
@inproceedings{Ruder2018,
abstract = {Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24{\%} on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100Ã— more data. We open-source our pretrained models and code1},
author = {Howard, Jeremy and Ruder, Sebastian},
booktitle = {ACL 2018 - 56th Annu. Meet. Assoc. Comput. Linguist. Proc. Conf. (Long Pap.},
doi = {10.18653/v1/p18-1031},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Howard, Ruder - 2018 - Universal language model fine-tuning for text classification(2).pdf:pdf},
title = {{Universal language model fine-tuning for text classification}},
volume = {1},
year = {2018}
}
@article{Sun2019,
abstract = {Pre-trained language models such as BERT have proven to be highly effective for natural language processing (NLP) tasks. However, the high demand for computing resources in training such models hinders their application in practice. In order to alleviate this resource hunger in large-scale model training, we propose a Patient Knowledge Distillation approach to compress an original large model (teacher) into an equally-effective lightweight shallow network (student). Different from previous knowledge distillation methods, which only use the output from the last layer of the teacher network for distillation, our student model patiently learns from multiple intermediate layers of the teacher model for incremental knowledge extraction, following two strategies: ({\$}i{\$}) PKD-Last: learning from the last {\$}k{\$} layers; and ({\$}ii{\$}) PKD-Skip: learning from every {\$}k{\$} layers. These two patient distillation schemes enable the exploitation of rich information in the teacher's hidden layers, and encourage the student model to patiently learn from and imitate the teacher through a multi-layer distillation process. Empirically, this translates into improved results on multiple NLP tasks with significant gain in training efficiency, without sacrificing model accuracy.},
archivePrefix = {arXiv},
arxivId = {1908.09355},
author = {Sun, Siqi and Cheng, Yu and Gan, Zhe and Liu, Jingjing},
eprint = {1908.09355},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Sun et al. - 2019 - Patient Knowledge Distillation for BERT Model Compression.pdf:pdf},
journal = {EMNLP-IJCNLP 2019 - 2019 Conf. Empir. Methods Nat. Lang. Process. 9th Int. Jt. Conf. Nat. Lang. Process. Proc. Conf.},
month = {aug},
pages = {4323--4332},
publisher = {Association for Computational Linguistics},
title = {{Patient Knowledge Distillation for BERT Model Compression}},
url = {http://arxiv.org/abs/1908.09355},
year = {2019}
}
@article{Huan2021,
author = {Huan, Liu and Zhixiong, Zhang and Yufei, Wang and Huan, Liu and Zhixiong, Zhang and Yufei, Wang},
doi = {10.11925/INFOTECH.2096-3467.2020.0965},
issn = {2096-3467},
journal = {Data Anal. Knowl. Discov.},
keywords = {BERT,Knowledge Integration,Model Compression,Pre-Training},
month = {feb},
number = {1},
pages = {3--15},
title = {{A Review on Main Optimization Methods of BERT}},
url = {http://manu44.magtech.com.cn/Jwk{\_}infotech{\_}wk3/EN/abstract/abstract4997.shtml},
volume = {5},
year = {2021}
}
@article{Rosa2019,
abstract = {We use the English model of BERT and explore how a deletion of one word in a sentence changes representations of other words. Our hypothesis is that removing a reducible word (e.g. an adjective) does not affect the representation of other words so much as removing e.g. the main verb, which makes the sentence ungrammatical and of "high surprise" for the language model. We estimate reducibilities of individual words and also of longer continuous phrases (word n-grams), study their syntax-related properties, and then also use them to induce full dependency trees.},
archivePrefix = {arXiv},
arxivId = {1906.11511},
author = {Rosa, Rudolf and Mare{\v{c}}ek, David},
eprint = {1906.11511},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Rosa, Mare{\v{c}}ek - 2019 - Inducing Syntactic Trees from BERT Representations.pdf:pdf},
journal = {arXiv},
month = {jun},
publisher = {arXiv},
title = {{Inducing Syntactic Trees from BERT Representations}},
url = {http://arxiv.org/abs/1906.11511},
year = {2019}
}
@article{Rogers2020,
abstract = {Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model. We review the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue and approaches to compression. We then outline directions for future research.},
archivePrefix = {arXiv},
arxivId = {2002.12327},
author = {Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
eprint = {2002.12327},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Rogers, Kovaleva, Rumshisky - 2020 - A Primer in BERTology What we know about how BERT works(2).pdf:pdf},
journal = {arXiv},
month = {feb},
publisher = {arXiv},
title = {{A Primer in BERTology: What we know about how BERT works}},
url = {http://arxiv.org/abs/2002.12327},
year = {2020}
}
