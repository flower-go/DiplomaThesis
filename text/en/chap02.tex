\chapter{Implementation analysis}
The main purpose of this chapter is to offer the technical description of the code accompanying this work for better reproducibility and possible further experiments on every of presented tasks. This chapter describes an implementation of all language models and other related code. It also presents all used libraries and technologies and this chapter is concluded with a presentation of experiment types common for training of both tasks presented in following chapters.
\par
All code forms an attachment of this work and is also publicly available in GitHub \footnote{https://github.com/flower-go/DiplomaThesis}.

\section{Experiments}
Firstly, all possibilities of experiment settings will be presented as the rest of the chapter serves for a specification of their implementation and running. Theoretical possibilities of training settings are described in detail in the previous section \ref{sub:howto}, but this part will describe particular training experiment settings, that are same for both downstream tasks \footnote{Name in the bracket in titles and name of options refers to the name used in the tables of results and graphs.}. 
\subsection{General experiment settings (EXPE)}
Training is performed in one of following settings:
\begin{itemize}
\item \textbf{base}: Baseline implementation (described separately for each task, typically without using advanced language models).
\item \textbf{ls}: This uses same setting as baseline implementation but with label smoothing.
\item \textbf{embed}: BERT-like language model is used only to generate static embeddings in advanced. These embeddings are not further trained.
\item \textbf{fine}: Fine-tuning consist in dividing the training time into two parts. Firstly, rest of the model is trained with BERT layers frozen (not trained), so it is same as the \textit{embed} settings. In the second part, the whole model is fine-tuned together.
\item \textbf{simpl}e: Model architecture is reduced to bert layers with a simple classification head. This is basic setting for all sentiment analysis experiments but for tagging and lemmatization all previous choices, in contrast to this setting, are performed with more sophisticated classification head.
\item \textbf{full}: This options means training the whole model from the beginning (in contrast to \textit{fine} option), but the classification head is not simplified (in contrast \textit{simple} option).
\end{itemize}
%TODO zmenit u sentimentu mít všude simple nebo full a jinej sloupec na data?
\subsection{Training data}
Tagging and lemmatization tasks use same set of data for every experiments, co there is no need for separate description. Sentiment analysis task, however, uses three possible options for a selection of training data:
\begin{itemize}
\item \textbf{mall|facebook|csfd}: Model is trained and evaluated on the (sub)set of czech datasets.
\item \textbf{zero}: Model is trained on english sentiment analysis dataset, but evaluated on czech data.
\item \textbf{eng}: Model is trained on the combination of czech and english training data (and evaluated again on the czech data). %TODO pridat jeste jednotlive ceske datasety
\end{itemize}
\subsection{Learning rate scheduling type (LRTYPE)}
Most experiments are expected to perform better with some kind of learning rate scheduling. This work implements three types of learning rate scheduling:
\begin{itemize}
\item \textbf{simple} \textit{Simple} option indicates no more complex learning rate scheduling than setting in advance different learning rates for different epochs.
\item \textbf{isrd} %TODO citovat 
\textit{isrd} means inverse square root learning rate decay defined by formula: $1/\sqrt{max(n,k)}$ where $k$ is the number of so-called \textit{warmup steps} and $n$ is the current iteration. This leads to constant learning rate for first $k$ steps and decayed learning rate in rest of iterations.
\item \textbf{cos}: Another learning rate scheduling used in this work is \textit{cosine decay} %TODO citovat
which applies following formula: $$lr=lr_{min}^{i} + \frac{1}{2}(lr_{max}^{i} - lr_{min}^{i})(1+cos(\frac{T_{curr}}{T_i}\pi)),$$ where $lr_{min}^{i}$ and $lr_{min}^{i}$ is the range of the learning rate, $T_i$ is the number of epochs after which the learning rate is restarted, i.e. increased to the $lr_{max}^{i}$ value and $T_{curr}$ is the current epoch number.
\end{itemize}

\subsection{Model layers selected for embeddings (LAYERS)}
As discussed in the previous chapter, is unclear how to extract best embeddings from the language model, especially which layers to take into account. By selecting the most promising ways, following two hyperparameters are used in this work:
\begin{itemize}
\item \textbf{four}: Last four of the model are averaged to obtain final embeddings.
\item \textbf{att}: This setting performs weighted sum of all model layers and the weights are trained during training together with the rest of the model.
\end{itemize}
Experiments are also performed with different learning rates (LR), batch size (BATCH) and a number of epochs (EPOCH). 
Following section will explain technical details needed for running scripts.

\section{Code description}
This section will describe the code -- technologies and hardware used for experiments, where to find scripts for replicating experiments and how to run them.
\subsection{Technologies description}
All code is implemented in Python (v3.6.9). All dependencies and used libraries are listed in the requirements.txt file,
%TODO odkaz a pridat do githubu
%TODO něco o pythonu - jakoze se hodne pouziva ve statistice
but I should specifically mention some used libraries. Python is a popular language for machine learning, because of easy use and many available libraries, which allows to focus on high-level problem solving instead of technical details.
\subsubsection{Tensorflow and Keras}
The main library used for developing models in this work is Tensorflow \citep{tensorflow2015-whitepaper}. This library provides lots of tools for machine learning, especially for neural networks. Keras is a wrapper library over Tensorflow and provides easy use of the most common machine learning scenarios \citep{keras}. Tensorflow together with PyTorch \citep{NEURIPS2019_9015} is probably the most frequently used library for deep learning, both providing similar functionality. The reason behind this choice of Tensorflow is the fact, that this thesis builds on the previous work and uses the code developed in Tensorflow.
\subsubsection{Transformers} 
One of the most useful libraries in this work is Transformers library from Hugging Face \citep{Wolf2019HuggingFacesTS}, which contains pretrained BERT models and tools for their usage as tokenizers.
\subsubsection{Pandas}
Pandas library \citep{reback2020pandas} serves well for data analysis as it provides data structures like DataFrame, that provides named columns, advanced data indexation, selection, merging, joining, reshaping and other functionality similar to tools provided by e.g.. SQL databases. It does not only provide a rich set of tools but they are also developed with an emphasis on performance optimization.
\subsubsection{Scikit-learn}
Scikit-learn \citep{scikit-learn}  is another useful python library specialized on machine learning. In contrast to tensorflow, scikit-learn focuses on classical machine learning, not on neural networks, providing all important variants of machine learning models as well as supporting tools for training, e.g. cross validation or various metrics.
\subsubsection{Numpy}
Numpy \citep{harris2020array} is an library which provides powerful multidimensional arrays with many predefined operations. It is fast and it is common to use it for numerical operations over number arrays.
\subsubsection{Jupyter Notebook}
Jupyter notebook \citep{jupyter} is a web application for development. In this work, jupyter notebook is used for providing the trained models for exploration. %TODO doplnit odkaz na tu kapitolu
Jupyter suits well for this purpose because it, in addition to a possibility of running a separate parts of code in different cells, also supports visualisations and markdown formatted text and it can be useful especially for explanatory purposes.
\subsubsection{Artificial Intelligence Cluster}
%TODO dopsat neco o tom
\subsubsection{Other Resources}
%TODO coze?

\subsection{code structure}
All code belonging to each of tasks is in the separate directory as can be seen on picture. %TODO udelat obrazek s adresarovou strukturou a barevne oznacit co se ma spoustet
\subsubsection{Runnable scripts}
All code for \textbf{tagging and lemmatization} is placed in folder \texttt{morphodita\_research}. Main files are \texttt{morpho\_tagger\_2.py} and \texttt{bert\_finetunning\_simple.py}, which serves for running all experiments relating to tagging and lemmatization. Script arguments are described in more detail in tables \ref{Tab:com_args}, \ref{Tab:mt_com_args} and \ref{Tab:mt2_args}.

%spolecne pro vsechny:
\begin{table}
\centering
\label{Tab:com_args}
\begin{tabular}{ |p{3cm}|p{3,5cm}|p{6cm}| } 
 \hline
 Argument & Values & Description \\ 
 \hline \hline
 accu & int & Accumulation of gradient. Effective batch size is batch\_size * accu.  \\\hline
batch\_size & int & Batch size (without accumulation). \\ \hline
bert & string & Name of the bert model (from huggingFace library) or path to the model.  \\ \hline
  checkp & String & Name of the saved model weights. Saving weights is used instead of the whole model because of some technical issues.  \\ \hline
  debug & 0/1 & Debug mode loads small debug data if available. \\ \hline
  label\_smoothing & decimal number & Coefficient for label smoothing. \\ \hline
  dropout & float &  Dropout amount applied on various places of the network.  \\ \hline
 epochs & "x:l1,y:l2"  & This will perform x epochs with learning rate l1 and y epochs with learning rate l2.   \\ \hline
 layers & None/"att" & If "att", all bert-like model layers are combined with learned weights.  \\ \hline
 warmup\_decay & None /"i:x"/"c:x" & If not None, training will incorporate inverse square root decay or cosine decay for x episodes.  \\ \hline
 fine\_lr & float & Different learning rate for the classification head.  \\ \hline
 \hline
\end{tabular}
\caption{A list of arguments common to all scripts.} 
\end{table}


\begin{table}
\centering
\label{Tab:mt_com_args}
\begin{tabular}{ |p{3cm}|p{3,5cm}|p{6cm}| } 
 \hline
 Argument & Values & Description \\ 
 \hline \hline
 beta\_2 & float & An argument for the optimizer. \\ \hline
 cle\_dim & int & Dimension of character-level embeddings.  \\ \hline
 cont & 0/1 & Evaluating on the test data every epoch.  \\ \hline
 exp & string & Name of logs files.  \\ \hline
 factors & "Lemmas,Tags" & Factors to be predicted -- Lemmas, Tags or both. \\ \hline
word\_dropout & float & Probability of masking a word in the sentence during training.  \\ \hline

\hline

\end{tabular}
\caption{A list of arguments common to both scripts for tagging and lemmatization.} 
\end{table}


\begin{table}
\centering
\label{Tab:mt2_args}
\begin{tabular}{ |p{3cm}|p{3,5cm}|p{6cm}| } 
 \hline
 Argument & Values & Description \\ 
 \hline \hline
 data & string &  Input data directory. Data are supposed to be divided into train, dev and test \texttt{.txt} files. \\ \hline
 char\_dropout &  float &  Dropout for characters. \\ \hline
embeddings & string & Path to pre-comuputed embeddings for use. \\ \hline
factor\_layers & int & Number of dense-and-dropout blocks for each of factors.  \\ \hline
lemma\_re\_strip &  string & Regular expression for suffix to be stripped from lemma. \\ \hline
lemma\_rule\_min & int & Minimal occurences to keep a lemma.  \\ \hline
predict & string & Script returns only a prediction with model from the path given in this argument.  \\ \hline
rnn\_cell & "LSTM"/"GRU" & Type of rnn cell to use. \\ \hline
rnn\_cell\_dim & int & Dimension for rnn cells.  \\ \hline
rnn\_layers& int & Number of recurrent cell layers.  \\ \hline
we\_dim & int & Dimension of word embeddings.  \\ \hline  


bert\_model & string & Trained checkpoint for loading. Training will continue from this checkpoint. \\ \hline

test\_only & string & Path to the model, which will be load and weights will be printed.  \\ \hline
 \hline
\end{tabular}
\caption{A list of arguments specific to morpho\_tagger\_2.py, with detailed description.} 
\end{table}

\textbf{Sentiment analysis} experiments are runnable from \texttt{sentiment\_analysis.py} with arguments as described in tables \ref{Tab:com_args} and \ref{Tab:sent_args}.

\begin{table}
\centering
\label{Tab:sent_args}
\begin{tabular}{ |p{2cm}|p{4,5cm}|p{6cm}| } 
 \hline
 Argument & Values & Description \\ 
 \hline \hline
 datasets & \{mall,csfd,facebook\} &  Names of the input czech datasets, separated by comma. \\ \hline
 english & float & A percentage of result training data which should be formed by english IMDB dataset.\\ \hline
 
 freeze & {0,1} & 1 means, that bert layers will not be trained. \\ \hline
 seed & int & Inicialization of random seed. \\ \hline
 kfold & "k:i" & Data will be splitted into k folds and i-th fold will be used for test. It serves for running k-fold cross-validation in parallel runs. \\ \hline
 
 \hline
\end{tabular}
\caption{Arguments for \texttt{sentiment\_analysis.py} script.} 
\end{table}


\section{User guide}






