\chapter{Implementation analysis}
The main purpose of this chapter is to offer the technical description of the code accompanying this work for better reproducibility and possible further experiments on every of presented tasks. This chapter describes an implementation of all language models, other related code and also presents all used libraries and technologies.
% and this chapter is concluded with a presentation of experiment types common for training of both tasks presented in following chapters.
\par
All code forms an attachment of this work and is also publicly available in GitHub \footnote{https://github.com/flower-go/DiplomaThesis}.


\section{Code description}
This section will describe the code -- technologies and hardware used for experiments, where to find scripts for replicating experiments and how to run them.
\subsection{Technologies description}
All code is implemented in Python (v3.6.9). All dependencies and used libraries are listed in the requirements.txt file,
%TODO odkaz a pridat do githubu
%TODO nÄ›co o pythonu - jakoze se hodne pouziva ve statistice
but I should specifically mention some used libraries. Python is a popular language for machine learning, because of easy use and many available libraries, which allows to focus on high-level problem solving instead of technical details.
\subsubsection{Tensorflow and Keras}
The main library used for developing models in this work is Tensorflow \citep{tensorflow2015-whitepaper}. This library provides lots of tools for machine learning, especially for neural networks. Keras is a wrapper library over Tensorflow and provides easy use of the most common machine learning scenarios \citep{keras}. Tensorflow together with PyTorch \citep{NEURIPS2019_9015} is probably the most frequently used library for deep learning, both providing similar functionality. The reason behind this choice of Tensorflow is the fact, that this thesis builds on the previous work and uses the code developed in Tensorflow.
\subsubsection{Transformers} 
One of the most useful libraries in this work is Transformers library from Hugging Face \citep{Wolf2019HuggingFacesTS}, which contains pretrained BERT models and tools for their usage as tokenizers.
\subsubsection{Pandas}
Pandas library \citep{reback2020pandas} serves well for data analysis as it provides data structures like DataFrame, that provides named columns, advanced data indexation, selection, merging, joining, reshaping and other functionality similar to tools provided by e.g.. SQL databases. It does not only provide a rich set of tools but they are also developed with an emphasis on performance optimization.
\subsubsection{Scikit-learn}
Scikit-learn \citep{scikit-learn}  is another useful python library specialized on machine learning. In contrast to tensorflow, scikit-learn focuses on classical machine learning, not on neural networks, providing all important variants of machine learning models as well as supporting tools for training, e.g. cross validation or various metrics.
\subsubsection{Numpy}
Numpy \citep{harris2020array} is an library which provides powerful multidimensional arrays with many predefined operations. It is fast and it is common to use it for numerical operations over number arrays.
\subsubsection{Jupyter Notebook}
Jupyter notebook \citep{jupyter} is a web application for development. In this work, jupyter notebook is used for providing the trained models for exploration. %TODO doplnit odkaz na tu kapitolu
Jupyter suits well for this purpose because it, in addition to a possibility of running a separate parts of code in different cells, also supports visualisations and markdown formatted text and it can be useful especially for explanatory purposes.
\subsubsection{Artificial Intelligence Cluster}
%TODO dopsat neco o tom
\subsubsection{Other Resources}
%TODO coze?

\subsection{code structure}
All code belonging to each of tasks is in the separate directory as can be seen on picture. %TODO udelat obrazek s adresarovou strukturou a barevne oznacit co se ma spoustet
\subsubsection{Runnable scripts}
All code for \textbf{tagging and lemmatization} is placed in folder \texttt{morphodita\_research}. Main files are \texttt{morpho\_tagger\_2.py} and \texttt{bert\_finetunning\_simple.py}, which serves for running all experiments relating to tagging and lemmatization. Script arguments are described in more detail in tables \ref{Tab:com_args}, \ref{Tab:mt_com_args} and \ref{Tab:mt2_args}.

%spolecne pro vsechny:
\begin{table}
\centering
\label{Tab:com_args}
\begin{tabular}{ |p{3cm}|p{3,5cm}|p{6cm}| } 
 \hline
 Argument & Values & Description \\ 
 \hline \hline
 accu & int & Accumulation of gradient. Effective batch size is batch\_size * accu.  \\\hline
batch\_size & int & Batch size (without accumulation). \\ \hline
bert & string & Name of the bert model (from huggingFace library) or path to the model.  \\ \hline
  checkp & String & Name of the saved model weights. Saving weights is used instead of the whole model because of some technical issues.  \\ \hline
  debug & 0/1 & Debug mode loads small debug data if available. \\ \hline
  label\_smoothing & decimal number & Coefficient for label smoothing. \\ \hline
  dropout & float &  Dropout amount applied on various places of the network.  \\ \hline
 epochs & "x:l1,y:l2"  & This will perform x epochs with learning rate l1 and y epochs with learning rate l2.   \\ \hline
 layers & None/"att" & If "att", all bert-like model layers are combined with learned weights.  \\ \hline
 warmup\_decay & None /"i:x"/"c:x" & If not None, training will incorporate inverse square root decay or cosine decay for x episodes.  \\ \hline
 fine\_lr & float & Different learning rate for the classification head.  \\ \hline
 \hline
\end{tabular}
\caption{A list of arguments common to all scripts.} 
\end{table}


\begin{table}
\centering
\label{Tab:mt_com_args}
\begin{tabular}{ |p{3cm}|p{3,5cm}|p{6cm}| } 
 \hline
 Argument & Values & Description \\ 
 \hline \hline
 beta\_2 & float & An argument for the optimizer. \\ \hline
 cle\_dim & int & Dimension of character-level embeddings.  \\ \hline
 cont & 0/1 & Evaluating on the test data every epoch.  \\ \hline
 exp & string & Name of logs files.  \\ \hline
 factors & "Lemmas,Tags" & Factors to be predicted -- Lemmas, Tags or both. \\ \hline
word\_dropout & float & Probability of masking a word in the sentence during training.  \\ \hline

\hline

\end{tabular}
\caption{A list of arguments common to both scripts for tagging and lemmatization.} 
\end{table}


\begin{table}
\centering
\label{Tab:mt2_args}
\begin{tabular}{ |p{3cm}|p{3,5cm}|p{6cm}| } 
 \hline
 Argument & Values & Description \\ 
 \hline \hline
 data & string &  Input data directory. Data are supposed to be divided into train, dev and test \texttt{.txt} files. \\ \hline
 char\_dropout &  float &  Dropout for characters. \\ \hline
embeddings & string & Path to pre-comuputed embeddings for use. \\ \hline
factor\_layers & int & Number of dense-and-dropout blocks for each of factors.  \\ \hline
lemma\_re\_strip &  string & Regular expression for suffix to be stripped from lemma. \\ \hline
lemma\_rule\_min & int & Minimal occurences to keep a lemma.  \\ \hline
predict & string & Script returns only a prediction with model from the path given in this argument.  \\ \hline
rnn\_cell & "LSTM"/"GRU" & Type of rnn cell to use. \\ \hline
rnn\_cell\_dim & int & Dimension for rnn cells.  \\ \hline
rnn\_layers& int & Number of recurrent cell layers.  \\ \hline
we\_dim & int & Dimension of word embeddings.  \\ \hline  


bert\_model & string & Trained checkpoint for loading. Training will continue from this checkpoint. \\ \hline

test\_only & string & Path to the model, which will be load and weights will be printed.  \\ \hline
 \hline
\end{tabular}
\caption{A list of arguments specific to morpho\_tagger\_2.py, with detailed description.} 
\end{table}

\textbf{Sentiment analysis} experiments are runnable from \texttt{sentiment\_analysis.py} with arguments as described in tables \ref{Tab:com_args} and \ref{Tab:sent_args}.

\begin{table}
\centering
\label{Tab:sent_args}
\begin{tabular}{ |p{2cm}|p{4,5cm}|p{6cm}| } 
 \hline
 Argument & Values & Description \\ 
 \hline \hline
 datasets & \{mall,csfd,facebook\} &  Names of the input czech datasets, separated by comma. \\ \hline
 english & float & A percentage of result training data which should be formed by english IMDB dataset.\\ \hline
 
 freeze & {0,1} & 1 means, that bert layers will not be trained. \\ \hline
 seed & int & Inicialization of random seed. \\ \hline
 kfold & "k:i" & Data will be splitted into k folds and i-th fold will be used for test. It serves for running k-fold cross-validation in parallel runs. \\ \hline
 
 \hline
\end{tabular}
\caption{Arguments for \texttt{sentiment\_analysis.py} script.} 
\end{table}


\section{User guide}






