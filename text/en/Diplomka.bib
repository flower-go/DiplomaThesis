Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Straka2018,
abstract = {UDPipe is a trainable pipeline which performs sentence segmentation, tokeniza-tion, POS tagging, lemmatization and dependency parsing (Straka et al., 2016). We present a prototype for UDPipe 2.0 and evaluate it in the CoNLL 2018 UD Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, which employs three metrics for submission ranking. Out of 26 participants, the prototype placed first in the MLAS ranking, third in the LAS ranking and third in the BLEX ranking. In extrinsic parser evaluation EPE 2018, the system ranked first in the overall score. The prototype utilizes an artificial neu-ral network with a single joint model for POS tagging, lemmatization and dependency parsing, and is trained only using the CoNLL-U training data and pretrained word embeddings, contrary to both systems surpassing the prototype in the LAS and BLEX ranking in the shared task. The open-source code of the prototype is available at http://github.com/ CoNLL-UD-2018/UDPipe-Future. After the shared task, we slightly refined the model architecture, resulting in better performance both in the intrinsic evaluation (corresponding to first, second and second rank in MLAS, LAS and BLEX shared task metrics) and the extrinsic evaluation. The improved models will be available shortly in UDPipe at},
author = {Straka, Milan},
doi = {10.18653/v1/K18-2020},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Straka - Unknown - UDPipe 2.0 Prototype at CoNLL 2018 UD Shared Task.pdf:pdf},
journal = {Proc. CoNLL 2018 Shar. Task Multiling. Parsing from Raw Text to Univers. Depend.},
pages = {197--207},
title = {{UDPipe 2.0 Prototype at CoNLL 2018 UD Shared Task}},
url = {http://ufal.mff.cuni.cz/udpipe.},
year = {2018}
}
@techreport{Sun,
abstract = {Language model pre-training has proven to be useful in learning universal language representations. As a state-of-the-art language model pre-training model, BERT (Bidirectional En-coder Representations from Transformers) has achieved amazing results in many language understanding tasks. In this paper, we conduct exhaustive experiments to investigate different fine-tuning methods of BERT on text classification task and provide a general solution for BERT fine-tuning. Finally, the proposed solution obtains new state-of-the-art results on eight widely-studied text classification datasets. 1},
archivePrefix = {arXiv},
arxivId = {1905.05583v3},
author = {Sun, Chi and Qiu, Xipeng and Xu, Yige and Huang, Xuanjing},
eprint = {1905.05583v3},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Sun et al. - Unknown - How to Fine-Tune BERT for Text Classification.pdf:pdf},
title = {{How to Fine-Tune BERT for Text Classification?}},
url = {https://github.}
}
@article{Straka2019a,
abstract = {We present an extensive evaluation of three recently proposed methods for contextualized embeddings on 89 corpora in 54 languages of the Universal Dependencies 2.3 in three tasks: POS tagging, lemmatization, and dependency parsing. Employing the BERT, Flair and ELMo as pretrained embedding inputs in a strong baseline of UDPipe 2.0, one of the best-performing systems of the CoNLL 2018 Shared Task and an overall winner of the EPE 2018, we present a one-to-one comparison of the three contextualized word embedding methods, as well as a comparison with word2vec-like pretrained embeddings and with end-to-end character-level word embeddings. We report state-of-the-art results in all three tasks as compared to results on UD 2.2 in the CoNLL 2018 Shared Task.},
archivePrefix = {arXiv},
arxivId = {1908.07448},
author = {Straka, Milan and Strakov{\'{a}}, Jana and Haji{\v{c}}, Jan},
eprint = {1908.07448},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Straka, Strakov{\'{a}}, Haji{\v{c}} - 2019 - Evaluating Contextualized Embeddings on 54 Languages in POS Tagging, Lemmatization and Dependency Par.pdf:pdf},
month = {aug},
title = {{Evaluating Contextualized Embeddings on 54 Languages in POS Tagging, Lemmatization and Dependency Parsing}},
url = {http://arxiv.org/abs/1908.07448},
year = {2019}
}
@article{Minsky2017,
author = {Minsky, M and Papert, SA},
title = {{Perceptrons: An introduction to computational geometry}},
url = {https://www.google.com/books?hl=cs{\&}lr={\&}id=PLQ5DwAAQBAJ{\&}oi=fnd{\&}pg=PR5{\&}dq=perceptrons+an+introduction+to+computational+geometry{\&}ots=zzCzAMspY0{\&}sig=x0HLubdLNN3Irw4cZUlmdyHK8BM},
year = {2017}
}
@article{Feijo2020,
abstract = {BERT (Bidirectional Encoder Representations from Transformers) and ALBERT (A Lite BERT) are methods for pre-training language models which can later be fine-tuned for a variety of Natural Language Understanding tasks. These methods have been applied to a number of such tasks (mostly in English), achieving results that outperform the state-of-the-art. In this paper, our contribution is twofold. First, we make available our trained BERT and Albert model for Portuguese. Second, we compare our monolingual and the standard multilingual models using experiments in semantic textual similarity, recognizing textual entailment, textual category classification, sentiment analysis, offensive comment detection, and fake news detection, to assess the effectiveness of the generated language representations. The results suggest that both monolingual and multilingual models are able to achieve state-of-the-art and the advantage of training a single language model, if any, is small.},
archivePrefix = {arXiv},
arxivId = {2007.09757},
author = {Feijo, Diego de Vargas and Moreira, Viviane Pereira},
eprint = {2007.09757},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Feijo, Moreira - 2020 - Mono vs Multilingual Transformer-based Models a Comparison across Several Language Tasks.pdf:pdf},
month = {jul},
title = {{Mono vs Multilingual Transformer-based Models: a Comparison across Several Language Tasks}},
url = {http://arxiv.org/abs/2007.09757},
year = {2020}
}
@article{Kittask2020,
abstract = {Recently, large pre-trained language models, such as BERT, have reached state-of-the-art performance in many natural language processing tasks, but for many languages, including Estonian, BERT models are not yet available. However, there exist several multilingual BERT models that can handle multiple languages simultaneously and that have been trained also on Estonian data. In this paper, we evaluate four multilingual models---multilingual BERT, multilingual distilled BERT, XLM and XLM-RoBERTa---on several NLP tasks including POS and morphological tagging, NER and text classification. Our aim is to establish a comparison between these multilingual BERT models and the existing baseline neural models for these tasks. Our results show that multilingual BERT models can generalise well on different Estonian NLP tasks outperforming all baselines models for POS and morphological tagging and text classification, and reaching the comparable level with the best baseline for NER, with XLM-RoBERTa achieving the highest results compared with other multilingual models.},
archivePrefix = {arXiv},
arxivId = {2010.00454},
author = {Kittask, Claudia and Milintsevich, Kirill and Sirts, Kairit},
eprint = {2010.00454},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Kittask, Milintsevich, Sirts - 2020 - Evaluating Multilingual BERT for Estonian.pdf:pdf},
keywords = {Estonian,NER,POS tagging,multilingual BERT,text classification},
month = {oct},
title = {{Evaluating Multilingual BERT for Estonian}},
url = {http://arxiv.org/abs/2010.00454},
year = {2020}
}
@article{Rosenblatt1958,
abstract = {To answer the questions of how information about the physical world is sensed, in what form is information remembered, and how does information retained in memory influence recognition and behavior, a theory is developed for a hypothetical nervous system called a perceptron. The theory serves as a bridge between biophysics and psychology. It is possible to predict learning curves from neurological variables and vice versa. The quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems. 18 references. (PsycINFO Database Record (c) 2006 APA, all rights reserved). {\textcopyright} 1958 American Psychological Association.},
author = {Rosenblatt, F.},
doi = {10.1037/h0042519},
issn = {0033295X},
journal = {Psychol. Rev.},
keywords = {PERCEPTION, AS INFORMATION STORAGE MODEL INFORMATI},
month = {nov},
number = {6},
pages = {386--408},
pmid = {13602029},
title = {{The perceptron: A probabilistic model for information storage and organization in the brain}},
volume = {65},
year = {1958}
}
@article{Cano2019,
abstract = {In the area of online communication, commerce and transactions, analyzing sentiment polarity of texts written in various natural languages has become crucial. While there have been a lot of contributions in resources and studies for the English language, "smaller" languages like Czech have not received much attention. In this survey, we explore the effectiveness of many existing machine learning algorithms for sentiment analysis of Czech Facebook posts and product reviews. We report the sets of optimal parameter values for each algorithm and the scores in both datasets. We finally observe that support vector machines are the best classifier and efforts to increase performance even more with bagging, boosting or voting ensemble schemes fail to do so.},
archivePrefix = {arXiv},
arxivId = {1901.02780},
author = {{\c{C}}ano, Erion and Bojar, Ond≈ôej},
doi = {10.5220/0007695709730979},
eprint = {1901.02780},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/{\c{C}}ano, Bojar - 2019 - Sentiment Analysis of Czech Texts An Algorithmic Survey.pdf:pdf},
journal = {ICAART 2019 - Proc. 11th Int. Conf. Agents Artif. Intell.},
keywords = {Algorithmic Survey,Czech Text Datasets,Sentiment Analysis,Supervised Learning},
month = {jan},
pages = {973--979},
publisher = {SciTePress},
title = {{Sentiment Analysis of Czech Texts: An Algorithmic Survey}},
url = {http://arxiv.org/abs/1901.02780 http://dx.doi.org/10.5220/0007695709730979},
volume = {2},
year = {2019}
}
@techreport{McCulloch,
author = {McCulloch, WS and bulletin of mathematical Biophysics, W Pitts - The and undefined 1943},
booktitle = {Springer},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/McCulloch, biophysics, 1943 - Unknown - A logical calculus of the ideas immanent in nervous activity.pdf:pdf},
title = {{A logical calculus of the ideas immanent in nervous activity}},
url = {https://link.springer.com/article/10.1007{\%}252FBF02478259}
}
@techreport{Putra,
abstract = {Compared to English, the amount of labeled data for Indonesian text classification tasks is very small. Recently developed multilingual language models have shown its ability to create multilingual representations effectively. This paper investigates the effect of combining English and Indonesian data on building Indonesian text classification (e.g., sentiment analysis and hate speech) using multilingual language models. Using the feature-based approach, we observe its performance on various data sizes and total added English data. The experiment showed that the addition of English data, especially if the amount of Indonesian data is small, improves performance. Using the fine-tuning approach, we further showed its effectiveness in utilizing the English language to build Indonesian text classification models.},
author = {Putra, Ilham Firdausi and Purwarianti, Ayu and Ai-Vlb, U-Coe},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Putra, Purwarianti, Ai-Vlb - Unknown - Improving Indonesian Text Classification Using Multilingual Language Model.pdf:pdf},
keywords = {Indonesian text,hate speech classification,multilingual language model,sentiment analysis,text classification},
title = {{Improving Indonesian Text Classification Using Multilingual Language Model}},
url = {https://www.yelp.com/dataset}
}
@article{Wu2020,
abstract = {Multilingual BERT (mBERT) trained on 104 languages has shown surprisingly good cross-lingual performance on several NLP tasks, even without explicit cross-lingual signals. However, these evaluations have focused on cross-lingual transfer with high-resource languages, covering only a third of the languages covered by mBERT. We explore how mBERT performs on a much wider set of languages, focusing on the quality of representation for low-resource languages, measured by within-language performance. We consider three tasks: Named Entity Recognition (99 languages), Part-of-speech Tagging, and Dependency Parsing (54 languages each). mBERT does better than or comparable to baselines on high resource languages but does much worse for low resource languages. Furthermore, monolingual BERT models for these languages do even worse. Paired with similar languages, the performance gap between monolingual BERT and mBERT can be narrowed. We find that better models for low resource languages require more efficient pretraining techniques or more data.},
archivePrefix = {arXiv},
arxivId = {2005.09093},
author = {Wu, Shijie and Dredze, Mark},
eprint = {2005.09093},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Wu, Dredze - 2020 - Are All Languages Created Equal in Multilingual BERT.pdf:pdf},
month = {may},
pages = {120--130},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Are All Languages Created Equal in Multilingual BERT?}},
url = {http://arxiv.org/abs/2005.09093},
year = {2020}
}
@article{Virtanen2019,
abstract = {Deep learning-based language models pretrained on large unannotated text corpora have been demonstrated to allow efficient transfer learning for natural language processing, with recent approaches such as the transformer-based BERT model advancing the state of the art across a variety of tasks. While most work on these models has focused on high-resource languages, in particular English, a number of recent efforts have introduced multilingual models that can be fine-tuned to address tasks in a large number of different languages. However, we still lack a thorough understanding of the capabilities of these models, in particular for lower-resourced languages. In this paper, we focus on Finnish and thoroughly evaluate the multilingual BERT model on a range of tasks, comparing it with a new Finnish BERT model trained from scratch. The new language-specific model is shown to systematically and clearly outperform the multilingual. While the multilingual model largely fails to reach the performance of previously proposed methods, the custom Finnish BERT model establishes new state-of-the-art results on all corpora for all reference tasks: part-of-speech tagging, named entity recognition, and dependency parsing. We release the model and all related resources created for this study with open licenses at https://turkunlp.org/finbert .},
archivePrefix = {arXiv},
arxivId = {1912.07076},
author = {Virtanen, Antti and Kanerva, Jenna and Ilo, Rami and Luoma, Jouni and Luotolahti, Juhani and Salakoski, Tapio and Ginter, Filip and Pyysalo, Sampo},
eprint = {1912.07076},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Virtanen et al. - 2019 - Multilingual is not enough BERT for Finnish.pdf:pdf},
month = {dec},
title = {{Multilingual is not enough: BERT for Finnish}},
url = {http://arxiv.org/abs/1912.07076},
year = {2019}
}
@article{Straka2019,
abstract = {Contextualized embeddings, which capture appropriate word meaning depending on context, have recently been proposed. We evaluate two meth ods for precomputing such embeddings, BERT and Flair, on four Czech text processing tasks: part-of-speech (POS) tagging, lemmatization, dependency pars ing and named entity recognition (NER). The first three tasks, POS tagging, lemmatization and dependency parsing, are evaluated on two corpora: the Prague Dependency Treebank 3.5 and the Universal Dependencies 2.3. The named entity recognition (NER) is evaluated on the Czech Named Entity Corpus 1.1 and 2.0. We report state-of-the-art results for the above mentioned tasks and corpora.},
archivePrefix = {arXiv},
arxivId = {1909.03544},
author = {Straka, Milan and Strakov{\'{a}}, Jana and Haji{\v{c}}, Jan},
eprint = {1909.03544},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Straka, Strakov{\'{a}}, Haji{\v{c}} - 2019 - Czech Text Processing with Contextual Embeddings POS Tagging, Lemmatization, Parsing and NER.pdf:pdf},
journal = {Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics)},
keywords = {BERT,Contextualized embeddings,Czech,Dependency parsing,Flair,Lemmatization,Named entity recognition,POS tagging},
month = {sep},
pages = {137--150},
publisher = {Springer Verlag},
title = {{Czech Text Processing with Contextual Embeddings: POS Tagging, Lemmatization, Parsing and NER}},
url = {http://arxiv.org/abs/1909.03544},
volume = {11697 LNAI},
year = {2019}
}
