Automatically generated by Mendeley Desktop 1.19.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@phdthesis{Klouda,
abstract = {Instructions Sentiment analysis is an approach that aims to extract the polarity of a given text. Such polarity may, for example, correspond to a positive or negative review of some product. The aim of this work is to review and apply state of the art methods of sentiment analysis on product reviews in the Czech language. 1) Review and theoretically describe state of the art approaches for sentiment analysis. Focus on the various representations of words/documents like tf-idf or vector representations of words. 2) Use or implement at least two of the reviewed methods and experimentally compare their performance on reviews in the Czech language. Avoid implementing anew those methods that can be easily taken over from available implementations. 3) Propose a direction for further improvement of selected approaches. References Will be provided by the supervisor.},
author = {Klouda, Ing Karel and Langr, Luk{\'{a}}{\v{s}} and {Daniel Va{\v{s}}ata}, Ing},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Klouda, Langr, Daniel Va{\v{s}}ata - Unknown - Title Product review sentiment analysis in the Czech language Student.pdf:pdf},
school = {Czech Technical University in Prague},
title = {{Title: Product review sentiment analysis in the Czech language Student}},
type = {Bachelor's thesis},
year = {2019}
}
@techreport{Huh,
abstract = {The tremendous success of ImageNet-trained deep features on a wide range of transfer tasks raises the question: what is it about the ImageNet dataset that makes the learnt features as good as they are? This work provides an empirical investigation into the various facets of this question, such as, looking at the importance of the amount of examples , number of classes, balance between images-per-class and classes, and the role of fine and coarse grained recognition. We pre-train CNN features on various subsets of the ImageNet dataset and evaluate transfer performance on a variety of standard vision tasks. Our overall findings suggest that most changes in the choice of pre-training data long thought to be critical, do not significantly affect transfer performance.},
archivePrefix = {arXiv},
arxivId = {1608.08614v2},
author = {Huh, Minyoung and Agrawal, Pulkit and Efros, Alexei A},
eprint = {1608.08614v2},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Huh, Agrawal, Efros - Unknown - What makes ImageNet good for transfer learning.pdf:pdf},
title = {{What makes ImageNet good for transfer learning?}},
year = {2016}
}
@misc{Google,
author = {Google},
keywords = {necitovane},
mendeley-tags = {necitovane},
title = {{Google AI Blog: A Neural Network for Machine Translation, at Production Scale}},
url = {https://ai.googleblog.com/2016/09/a-neural-network-for-machine.html},
urldate = {2020-10-25}
}
@article{Liu2020,
abstract = {Contextual embeddings, such as ELMo and BERT, move beyond global word representations like Word2Vec and achieve ground-breaking performance on a wide range of natural language processing tasks. Contextual embeddings assign each word a representation based on its context, thereby capturing uses of words across varied contexts and encoding knowledge that transfers across languages. In this survey, we review existing contextual embedding models, cross-lingual polyglot pre-training, the application of contextual embeddings in downstream tasks, model compression, and model analyses.},
archivePrefix = {arXiv},
arxivId = {2003.07278},
author = {Liu, Qi and Kusner, Matt J. and Blunsom, Phil},
eprint = {2003.07278},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Liu, Kusner, Blunsom - 2020 - A Survey on Contextual Embeddings.pdf:pdf},
journal = {arXiv},
month = {mar},
publisher = {arXiv},
title = {{A Survey on Contextual Embeddings}},
url = {http://arxiv.org/abs/2003.07278},
year = {2020}
}
@article{Yang2019a,
abstract = {With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.},
archivePrefix = {arXiv},
arxivId = {1906.08237},
author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V.},
eprint = {1906.08237},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Yang et al. - 2019 - XLNet Generalized Autoregressive Pretraining for Language Understanding(2).pdf:pdf},
journal = {arXiv},
month = {jun},
publisher = {arXiv},
title = {{XLNet: Generalized Autoregressive Pretraining for Language Understanding}},
url = {http://arxiv.org/abs/1906.08237},
year = {2019}
}
@techreport{Mikolov2013,
abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
booktitle = {papers.nips.cc},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Mikolov et al. - Unknown - Distributed Representations of Words and Phrases and their Compositionality.pdf:pdf},
title = {{Distributed Representations of Words and Phrases and their Compositionality}},
url = {http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and},
year = {2013}
}
@article{Santos2016a,
abstract = {In this work, we propose Attentive Pooling (AP), a two-way attention mechanism for discriminative model training. In the context of pair-wise ranking or classification with neural networks, AP enables the pooling layer to be aware of the current input pair, in a way that information from the two input items can directly influence the computation of each other's representations. Along with such representations of the paired inputs, AP jointly learns a similarity measure over projected segments (e.g. trigrams) of the pair, and subsequently, derives the corresponding attention vector for each input to guide the pooling. Our two-way attention mechanism is a general framework independent of the underlying representation learning, and it has been applied to both convolutional neural networks (CNNs) and recurrent neural networks (RNNs) in our studies. The empirical results, from three very different benchmark tasks of question answering/answer selection, demonstrate that our proposed models outperform a variety of strong baselines and achieve state-of-the-art performance in all the benchmarks.},
archivePrefix = {arXiv},
arxivId = {1602.03609},
author = {dos Santos, Cicero and Tan, Ming and Xiang, Bing and Zhou, Bowen},
eprint = {1602.03609},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Santos et al. - 2016 - Attentive Pooling Networks(2).pdf:pdf},
keywords = {necitovane},
mendeley-tags = {necitovane},
month = {feb},
title = {{Attentive Pooling Networks}},
url = {http://arxiv.org/abs/1602.03609},
year = {2016}
}
@article{Radfort2018,
abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9{\%} on commonsense reasoning (Stories Cloze Test), 5.7{\%} on question answering (RACE), and 1.5{\%} on textual entailment (MultiNLI). 1},
author = {Radfort, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
file = {:Users/petravysusilova/Downloads/radford2018improving.pdf:pdf},
journal = {OpenAI},
title = {{Improving Language Understanding by Generative Pre-Training}},
year = {2018}
}
@misc{kysely,
author = {Kysel{\'{y}}, Radek},
title = {{kysely/sentiment-analysis-czech: Conducting and publishing sentiment analysis experiments in the Czech language}},
url = {https://github.com/kysely/sentiment-analysis-czech},
urldate = {2020-11-17},
year = {2017}
}
@techreport{Lenc2016,
author = {Lenc, Ladislav and Hercig, Tom{\'{a}}{\v{s}}},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Lenc, Hercig - 2016 - Neural Networks for Sentiment Analysis in Czech.pdf:pdf},
title = {{Neural Networks for Sentiment Analysis in Czech}},
year = {2016}
}
@techreport{Plank,
abstract = {Bidirectional long short-term memory (bi-LSTM) networks have recently proven successful for various NLP sequence mod-eling tasks, but little is known about their reliance to input representations, target languages, data set size, and label noise. We address these issues and evaluate bi-LSTMs with word, character, and unicode byte embeddings for POS tagging. We compare bi-LSTMs to traditional POS taggers across languages and data sizes. We also present a novel bi-LSTM model, which combines the POS tagging loss function with an auxiliary loss function that accounts for rare words. The model obtains state-of-the-art performance across 22 languages, and works especially well for morphologically complex languages. Our analysis suggests that bi-LSTMs are less sensitive to training data size and label corruptions (at small noise levels) than previously assumed.},
archivePrefix = {arXiv},
arxivId = {1604.05529v3},
author = {Plank, Barbara and S{\o}gaard, Anders and Goldberg, Yoav},
eprint = {1604.05529v3},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Plank, S{\o}gaard, Goldberg - Unknown - Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary.pdf:pdf},
keywords = {necitovane},
mendeley-tags = {necitovane},
title = {{Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss}},
url = {https://github.com/clab/cnn},
year = {2016}
}
@incollection{Sun,
abstract = {Language model pre-training has proven to be useful in learning universal language representations. As a state-of-the-art language model pre-training model, BERT (Bidirectional En-coder Representations from Transformers) has achieved amazing results in many language understanding tasks. In this paper, we conduct exhaustive experiments to investigate different fine-tuning methods of BERT on text classification task and provide a general solution for BERT fine-tuning. Finally, the proposed solution obtains new state-of-the-art results on eight widely-studied text classification datasets. 1},
archivePrefix = {arXiv},
arxivId = {1905.05583v3},
author = {Sun, Chi and Qiu, Xipeng and Xu, Yige and Huang, Xuanjing},
booktitle = {Chinese Comput. Linguist.},
eprint = {1905.05583v3},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Sun et al. - Unknown - How to Fine-Tune BERT for Text Classification.pdf:pdf},
keywords = {bertology,practise},
mendeley-tags = {bertology,practise},
pages = {194--206},
publisher = {Springer International Publishing},
title = {{How to Fine-Tune BERT for Text Classification?}},
url = {https://github.},
year = {2019}
}
@article{Collobert2011,
abstract = {We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including: part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.},
archivePrefix = {arXiv},
arxivId = {1103.0398},
author = {Collobert, Ronan and Weston, Jason and Bottou, Leon and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel},
eprint = {1103.0398},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Collobert et al. - 2011 - Natural Language Processing (almost) from Scratch.pdf:pdf},
journal = {J. Mach. Learn. Res.},
keywords = {Natural language processing,Neural networks},
month = {mar},
pages = {2493--2537},
title = {{Natural Language Processing (almost) from Scratch}},
url = {http://arxiv.org/abs/1103.0398},
volume = {12},
year = {2011}
}
@book{Veselovska,
author = {Veselovsk{\'{a}}, Kateřina},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Veselovsk{\'{a}} - Unknown - SENTIMENT ANALYSIS IN CZECH.pdf:pdf},
isbn = {9788088132035},
title = {{SENTIMENT ANALYSIS IN CZECH}},
year = {2017}
}
@article{Minsky2017,
author = {Minsky, M and Papert, SA},
title = {{Perceptrons: An introduction to computational geometry}},
url = {https://www.google.com/books?hl=cs{\&}lr={\&}id=PLQ5DwAAQBAJ{\&}oi=fnd{\&}pg=PR5{\&}dq=perceptrons+an+introduction+to+computational+geometry{\&}ots=zzCzAMspY0{\&}sig=x0HLubdLNN3Irw4cZUlmdyHK8BM},
year = {2017}
}
@article{Yang2019b,
abstract = {Transformer-based pre-trained language models have proven to be effective for learning contextualized language representation. However, current approaches only take advantage of the output of the encoder's final layer when fine-tuning the downstream tasks. We argue that only taking single layer's output restricts the power of pre-trained representation. Thus we deepen the representation learned by the model by fusing the hidden representation in terms of an explicit HIdden Representation Extractor (HIRE), which automatically absorbs the complementary representation with respect to the output from the final layer. Utilizing RoBERTa as the backbone encoder, our proposed improvement over the pre-trained models is shown effective on multiple natural language understanding tasks and help our model rival with the state-of-the-art models on the GLUE benchmark.},
archivePrefix = {arXiv},
arxivId = {1911.01940},
author = {Yang, Junjie and Zhao, Hai},
eprint = {1911.01940},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Yang, Zhao - 2019 - Deepening Hidden Representations from Pre-trained Language Models.pdf:pdf},
journal = {arXiv},
month = {nov},
title = {{Deepening Hidden Representations from Pre-trained Language Models}},
url = {http://arxiv.org/abs/1911.01940},
year = {2019}
}
@article{Clark2020,
abstract = {Masked language modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK] and then train a model to reconstruct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the original identities of the corrupted tokens, we train a discriminative model that predicts whether each token in the corrupted input was replaced by a generator sample or not. Thorough experiments demonstrate this new pre-training task is more efficient than MLM because the task is defined over all input tokens rather than just the small subset that was masked out. As a result, the contextual representations learned by our approach substantially outperform the ones learned by BERT given the same model size, data, and compute. The gains are particularly strong for small models; for example, we train a model on one GPU for 4 days that outperforms GPT (trained using 30x more compute) on the GLUE natural language understanding benchmark. Our approach also works well at scale, where it performs comparably to RoBERTa and XLNet while using less than 1/4 of their compute and outperforms them when using the same amount of compute.},
archivePrefix = {arXiv},
arxivId = {2003.10555},
author = {Clark, Kevin and Luong, Minh-Thang and Le, Quoc V. and Manning, Christopher D.},
eprint = {2003.10555},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Clark et al. - 2020 - ELECTRA Pre-training Text Encoders as Discriminators Rather Than Generators.pdf:pdf},
journal = {arXiv Prepr.},
keywords = {necitovane},
mendeley-tags = {necitovane},
month = {mar},
title = {{ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators}},
url = {http://arxiv.org/abs/2003.10555},
year = {2020}
}
@article{Tenney2019a,
abstract = {Contextualized representation models such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018) have recently achieved state-of-the-art results on a diverse array of downstream NLP tasks. Building on recent token-level probing work, we introduce a novel edge probing task design and construct a broad suite of sub-sentence tasks derived from the traditional structured NLP pipeline. We probe word-level contextual representations from four recent models and investigate how they encode sentence structure across a range of syntactic, semantic, local, and long-range phenomena. We find that existing models trained on language modeling and translation produce strong representations for syntactic phenomena, but only offer comparably small improvements on semantic tasks over a non-contextual baseline.},
archivePrefix = {arXiv},
arxivId = {1905.06316},
author = {Tenney, Ian and Xia, Patrick and Chen, Berlin and Wang, Alex and Poliak, Adam and McCoy, R Thomas and Kim, Najoung and {Van Durme}, Benjamin and Bowman, Samuel R. and Das, Dipanjan and Pavlick, Ellie},
eprint = {1905.06316},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Tenney et al. - 2019 - What do you learn from context Probing for sentence structure in contextualized word representations.pdf:pdf},
journal = {arXiv},
month = {may},
publisher = {arXiv},
title = {{What do you learn from context? Probing for sentence structure in contextualized word representations}},
url = {http://arxiv.org/abs/1905.06316},
year = {2019}
}
@techreport{Hana2005,
author = {Hana, Jiř{\'{i}} and Zeman, Daniel},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Hana, Zeman - 2005 - Manual for Morphological Annotation Revision for the Prague Dependency Treebank 2.0 ´ UFAL.pdf:pdf},
title = {{Manual for Morphological Annotation Revision for the Prague Dependency Treebank 2.0 UFAL}},
year = {2005}
}
@article{Raffel2019,
abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
archivePrefix = {arXiv},
arxivId = {1910.10683},
author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
eprint = {1910.10683},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Raffel et al. - 2019 - Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.pdf:pdf},
journal = {arXiv},
keywords = {attention-based models,deep learning,multi-task learning,natural language processing,transfer learning},
month = {oct},
pages = {1--67},
publisher = {arXiv},
title = {{Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer}},
url = {http://arxiv.org/abs/1910.10683},
volume = {21},
year = {2019}
}
@article{Wu2020,
abstract = {Multilingual BERT (mBERT) trained on 104 languages has shown surprisingly good cross-lingual performance on several NLP tasks, even without explicit cross-lingual signals. However, these evaluations have focused on cross-lingual transfer with high-resource languages, covering only a third of the languages covered by mBERT. We explore how mBERT performs on a much wider set of languages, focusing on the quality of representation for low-resource languages, measured by within-language performance. We consider three tasks: Named Entity Recognition (99 languages), Part-of-speech Tagging, and Dependency Parsing (54 languages each). mBERT does better than or comparable to baselines on high resource languages but does much worse for low resource languages. Furthermore, monolingual BERT models for these languages do even worse. Paired with similar languages, the performance gap between monolingual BERT and mBERT can be narrowed. We find that better models for low resource languages require more efficient pretraining techniques or more data.},
archivePrefix = {arXiv},
arxivId = {2005.09093},
author = {Wu, Shijie and Dredze, Mark},
eprint = {2005.09093},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Wu, Dredze - 2020 - Are All Languages Created Equal in Multilingual BERT.pdf:pdf},
keywords = {bertology,necitovane},
mendeley-tags = {bertology,necitovane},
month = {may},
pages = {120--130},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Are All Languages Created Equal in Multilingual BERT?}},
url = {http://arxiv.org/abs/2005.09093},
year = {2020}
}
@article{Rogers2020,
abstract = {Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model. We review the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue and approaches to compression. We then outline directions for future research.},
archivePrefix = {arXiv},
arxivId = {2002.12327},
author = {Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
eprint = {2002.12327},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Rogers, Kovaleva, Rumshisky - 2020 - A Primer in BERTology What we know about how BERT works(2).pdf:pdf},
journal = {arXiv},
month = {feb},
publisher = {arXiv},
title = {{A Primer in BERTology: What we know about how BERT works}},
url = {http://arxiv.org/abs/2002.12327},
year = {2020}
}
@article{Marcus1993,
author = {Marcus, Mitchell and Santorini, Beatrice and Marcinkiewicz, Mary Ann},
journal = {Tech. Reports},
month = {oct},
title = {{Building a Large Annotated Corpus of English: The Penn Treebank}},
url = {https://repository.upenn.edu/cis{\_}reports/237},
year = {1993}
}
@misc{Brown2020,
abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art finetuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
booktitle = {arXiv},
issn = {23318422},
title = {{Language models are few-shot learners}},
year = {2020}
}
@article{Lan2019,
abstract = {Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and $\backslash$squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.},
archivePrefix = {arXiv},
arxivId = {1909.11942},
author = {Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
eprint = {1909.11942},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Lan et al. - 2019 - ALBERT A Lite BERT for Self-supervised Learning of Language Representations.pdf:pdf},
journal = {arXiv},
month = {sep},
publisher = {arXiv},
title = {{ALBERT: A Lite BERT for Self-supervised Learning of Language Representations}},
url = {http://arxiv.org/abs/1909.11942},
year = {2019}
}
@techreport{Hladka,
author = {Hladk{\'{a}}},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - Part of Speech Tags for Automatic Tagging and Syntactic Structures 1.pdf:pdf},
title = {{Part of Speech Tags for Automatic Tagging and Syntactic Structures 1}}
}
@techreport{Cheng,
abstract = {In this paper we address the question of how to render sequence-level networks better at handling structured input. We propose a machine reading simulator which processes text incrementally from left to right and performs shallow reasoning with memory and attention. The reader extends the Long Short-Term Memory architecture with a memory network in place of a single memory cell. This enables adaptive memory usage during recurrence with neural attention, offering a way to weakly induce relations among tokens. The system is initially designed to process a single sequence but we also demonstrate how to integrate it with an encoder-decoder architecture. Experiments on language modeling, sentiment analysis, and natural language inference show that our model matches or outperforms the state of the art.},
archivePrefix = {arXiv},
arxivId = {1601.06733v7},
author = {Cheng, Jianpeng and Dong, Li and Lapata, Mirella},
booktitle = {arxiv.org},
eprint = {1601.06733v7},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Cheng, Dong, Lapata - Unknown - Long Short-Term Memory-Networks for Machine Reading.pdf:pdf},
title = {{Long Short-Term Memory-Networks for Machine Reading}},
url = {https://arxiv.org/abs/1601.06733},
year = {2016}
}
@techreport{Russell1995,
author = {Russell, Stuart J and Norvig, Peter and Canny, John F and Malik, Jitendra M and Edwards, Douglas D},
isbn = {0131038052},
pages = {529},
title = {{Artificial Intelligence A Modern Approach}},
year = {1995}
}
@article{Wang2015,
abstract = {Bidirectional Long Short-Term Memory Recurrent Neural Network (BLSTM-RNN) has been shown to be very effective for tagging sequential data, e.g. speech utterances or handwritten documents. While word embedding has been demoed as a powerful representation for characterizing the statistical properties of natural language. In this study, we propose to use BLSTM-RNN with word embedding for part-of-speech (POS) tagging task. When tested on Penn Treebank WSJ test set, a state-of-the-art performance of 97.40 tagging accuracy is achieved. Without using morphological features, this approach can also achieve a good performance comparable with the Stanford POS tagger.},
archivePrefix = {arXiv},
arxivId = {1510.06168},
author = {Wang, Peilu and Qian, Yao and Soong, Frank K. and He, Lei and Zhao, Hai},
eprint = {1510.06168},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Wang et al. - 2015 - Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Recurrent Neural Network.pdf:pdf},
keywords = {(),necitovane},
mendeley-tags = {necitovane},
month = {oct},
title = {{Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Recurrent Neural Network}},
url = {http://arxiv.org/abs/1510.06168},
year = {2015}
}
@techreport{Turian2010,
abstract = {If we take an existing supervised NLP system , a simple and general way to improve accuracy is to use unsupervised word representations as extra word features. We evaluate Brown clusters, Collobert and Weston (2008) embeddings, and HLBL (Mnih {\&} Hinton, 2009) embeddings of words on both NER and chunking. We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accuracy of these baselines. We find further improvements by combining different word representations. You can download our word features, for off-the-shelf use in existing NLP systems, as well as our code, here: http://metaoptimize. com/projects/wordreprs/},
author = {Turian, Joseph and Ratinov, Lev and Bengio, Yoshua},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Turian, Ratinov, Bengio - 2010 - Word representations A simple and general method for semi-supervised learning.pdf:pdf},
pages = {11--16},
publisher = {Association for Computational Linguistics},
title = {{Word representations: A simple and general method for semi-supervised learning}},
url = {http://metaoptimize.},
year = {2010}
}
@article{Kitaev2018,
abstract = {We show that constituency parsing benefits from unsupervised pre-training across a variety of languages and a range of pre-training conditions. We first compare the benefits of no pre-training, fastText, ELMo, and BERT for English and find that BERT outperforms ELMo, in large part due to increased model capacity, whereas ELMo in turn outperforms the non-contextual fastText embeddings. We also find that pre-training is beneficial across all 11 languages tested; however, large model sizes (more than 100 million parameters) make it computationally expensive to train separate models for each language. To address this shortcoming, we show that joint multilingual pre-training and fine-tuning allows sharing all but a small number of parameters between ten languages in the final model. The 10x reduction in model size compared to fine-tuning one model per language causes only a 3.2{\%} relative error increase in aggregate. We further explore the idea of joint fine-tuning and show that it gives low-resource languages a way to benefit from the larger datasets of other languages. Finally, we demonstrate new state-of-the-art results for 11 languages, including English (95.8 F1) and Chinese (91.8 F1).},
archivePrefix = {arXiv},
arxivId = {1812.11760},
author = {Kitaev, Nikita and Cao, Steven and Klein, Dan},
eprint = {1812.11760},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Kitaev, Cao, Klein - 2018 - Multilingual Constituency Parsing with Self-Attention and Pre-Training.pdf:pdf},
journal = {ACL 2019 - 57th Annu. Meet. Assoc. Comput. Linguist. Proc. Conf.},
month = {dec},
pages = {3499--3505},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Multilingual Constituency Parsing with Self-Attention and Pre-Training}},
url = {http://arxiv.org/abs/1812.11760},
year = {2018}
}
@article{Cano2019,
abstract = {In the area of online communication, commerce and transactions, analyzing sentiment polarity of texts written in various natural languages has become crucial. While there have been a lot of contributions in resources and studies for the English language, "smaller" languages like Czech have not received much attention. In this survey, we explore the effectiveness of many existing machine learning algorithms for sentiment analysis of Czech Facebook posts and product reviews. We report the sets of optimal parameter values for each algorithm and the scores in both datasets. We finally observe that support vector machines are the best classifier and efforts to increase performance even more with bagging, boosting or voting ensemble schemes fail to do so.},
archivePrefix = {arXiv},
arxivId = {1901.02780},
author = {{\c{C}}ano, Erion and Bojar, Ondřej},
doi = {10.5220/0007695709730979},
eprint = {1901.02780},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/{\c{C}}ano, Bojar - 2019 - Sentiment Analysis of Czech Texts An Algorithmic Survey.pdf:pdf},
journal = {ICAART 2019 - Proc. 11th Int. Conf. Agents Artif. Intell.},
keywords = {Algorithmic Survey,Czech Text Datasets,Sentiment Analysis,Supervised Learning,necitovane,text},
mendeley-tags = {necitovane,text},
month = {jan},
pages = {973--979},
publisher = {SciTePress},
title = {{Sentiment Analysis of Czech Texts: An Algorithmic Survey}},
url = {http://arxiv.org/abs/1901.02780 http://dx.doi.org/10.5220/0007695709730979},
volume = {2},
year = {2019}
}
@techreport{Hajicova2000,
author = {Haji{\v{c}}ov{\'{a}}, Eva and Panevov{\'{a}}, Jarmila and Sgall, Petr and Ceplov{\'{a}}, M and {Řezn{\'{i}}{\v{c}}kov{\'{a}} Translated by Kirschner}, V Z and Haji{\v{c}}ov{\'{a}}, E and Sgall, P},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Haji{\v{c}}ov{\'{a}} et al. - 2000 - A MANUAL FOR TECTOGRAMMATICAL TAGGING OF THE PRAGUE DEPENDENCY TREEBANK In cooperation with A.pdf:pdf},
keywords = {necitovane},
mendeley-tags = {necitovane},
title = {{A MANUAL FOR TECTOGRAMMATICAL TAGGING OF THE PRAGUE DEPENDENCY TREEBANK In cooperation with A}},
year = {2000}
}
@article{Kittask2020,
abstract = {Recently, large pre-trained language models, such as BERT, have reached state-of-the-art performance in many natural language processing tasks, but for many languages, including Estonian, BERT models are not yet available. However, there exist several multilingual BERT models that can handle multiple languages simultaneously and that have been trained also on Estonian data. In this paper, we evaluate four multilingual models---multilingual BERT, multilingual distilled BERT, XLM and XLM-RoBERTa---on several NLP tasks including POS and morphological tagging, NER and text classification. Our aim is to establish a comparison between these multilingual BERT models and the existing baseline neural models for these tasks. Our results show that multilingual BERT models can generalise well on different Estonian NLP tasks outperforming all baselines models for POS and morphological tagging and text classification, and reaching the comparable level with the best baseline for NER, with XLM-RoBERTa achieving the highest results compared with other multilingual models.},
archivePrefix = {arXiv},
arxivId = {2010.00454},
author = {Kittask, Claudia and Milintsevich, Kirill and Sirts, Kairit},
eprint = {2010.00454},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Kittask, Milintsevich, Sirts - 2020 - Evaluating Multilingual BERT for Estonian.pdf:pdf},
journal = {arXiv},
keywords = {Estonian,NER,POS tagging,multilingual BERT,necitovane,text classification},
mendeley-tags = {necitovane},
month = {oct},
title = {{Evaluating Multilingual BERT for Estonian}},
url = {http://arxiv.org/abs/2010.00454},
year = {2020}
}
@article{Michel2019,
abstract = {Attention is a powerful and ubiquitous mechanism for allowing neural models to focus on particular salient pieces of information by taking their weighted average when making predictions. In particular, multi-headed attention is a driving force behind many recent state-of-the-art NLP models such as Transformer-based MT models and BERT. These models apply multiple attention mechanisms in parallel, with each attention "head" potentially focusing on different parts of the input, which makes it possible to express sophisticated functions beyond the simple weighted average. In this paper we make the surprising observation that even if models have been trained using multiple heads, in practice, a large percentage of attention heads can be removed at test time without significantly impacting performance. In fact, some layers can even be reduced to a single head. We further examine greedy algorithms for pruning down models, and the potential speed, memory efficiency, and accuracy improvements obtainable therefrom. Finally, we analyze the results with respect to which parts of the model are more reliant on having multiple heads, and provide precursory evidence that training dynamics play a role in the gains provided by multi-head attention.},
archivePrefix = {arXiv},
arxivId = {1905.10650},
author = {Michel, Paul and Levy, Omer and Neubig, Graham},
eprint = {1905.10650},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Michel, Levy, Neubig - 2019 - Are Sixteen Heads Really Better than One.pdf:pdf},
journal = {arXiv},
month = {may},
publisher = {arXiv},
title = {{Are Sixteen Heads Really Better than One?}},
url = {http://arxiv.org/abs/1905.10650},
year = {2019}
}
@inproceedings{Peters2017,
abstract = {Pre-trained word embeddings learned from unlabeled text have become a standard component of neural network architectures for NLP tasks. However, in most cases, the recurrent network that operates on word-level representations to produce context sensitive representations is trained on relatively little labeled data. In this paper, we demonstrate a general semi-supervised approach for adding pre-trained context embeddings from bidirectional language models to NLP systems and apply it to sequence labeling tasks. We evaluate our model on two standard datasets for named entity recognition (NER) and chunking, and in both cases achieve state of the art results, surpassing previous systems that use other forms of transfer or joint learning with additional labeled data and task specific gazetteers.},
author = {Peters, Matthew E. and Ammar, Waleed and Bhagavatula, Chandra and Power, Russell},
booktitle = {ACL 2017 - 55th Annu. Meet. Assoc. Comput. Linguist. Proc. Conf. (Long Pap.},
doi = {10.18653/v1/P17-1161},
file = {:Users/petravysusilova/Downloads/peters.pdf:pdf},
title = {{Semi-supervised sequence tagging with bidirectional language models}},
volume = {1},
year = {2017}
}
@article{Dong2019,
abstract = {This paper presents a new Unified pre-trained Language Model (UniLM) that can be fine-tuned for both natural language understanding and generation tasks. The model is pre-trained using three types of language modeling tasks: unidirectional, bidirectional, and sequence-to-sequence prediction. The unified modeling is achieved by employing a shared Transformer network and utilizing specific self-attention masks to control what context the prediction conditions on. UniLM compares favorably with BERT on the GLUE benchmark, and the SQuAD 2.0 and CoQA question answering tasks. Moreover, UniLM achieves new state-of-the-art results on five natural language generation datasets, including improving the CNN/DailyMail abstractive summarization ROUGE-L to 40.51 (2.04 absolute improvement), the Gigaword abstractive summarization ROUGE-L to 35.75 (0.86 absolute improvement), the CoQA generative question answering F1 score to 82.5 (37.1 absolute improvement), the SQuAD question generation BLEU-4 to 22.12 (3.75 absolute improvement), and the DSTC7 document-grounded dialog response generation NIST-4 to 2.67 (human performance is 2.65). The code and pre-trained models are available at https://github.com/microsoft/unilm.},
archivePrefix = {arXiv},
arxivId = {1905.03197},
author = {Dong, Li and Yang, Nan and Wang, Wenhui and Wei, Furu and Liu, Xiaodong and Wang, Yu and Gao, Jianfeng and Zhou, Ming and Hon, Hsiao-Wuen},
eprint = {1905.03197},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Dong et al. - 2019 - Unified Language Model Pre-training for Natural Language Understanding and Generation.pdf:pdf},
journal = {arXiv},
month = {may},
publisher = {arXiv},
title = {{Unified Language Model Pre-training for Natural Language Understanding and Generation}},
url = {http://arxiv.org/abs/1905.03197},
year = {2019}
}
@inproceedings{Arkhipov2019,
abstract = {Our paper addresses the problem of multilingual named entity recognition on the material of 4 languages: Russian, Bulgarian, Czech and Polish. We solve this task using the BERT model. We use a hundred languages multilingual model as base for transfer to the mentioned Slavic languages. Unsupervised pre-training of the BERT model on these 4 languages allows to significantly outperform baseline neural approaches and multilingual BERT. Additional improvement is achieved by extending BERT with a word-level CRF layer. Our system was submitted to BSNLP 2019 Shared Task on Multilingual Named Entity Recognition and took the 1st place in 3 competition metrics out of 4 we participated in. We open-sourced NER models and BERT model pre-trained on the four Slavic languages.},
author = {Arkhipov, Mikhail and Trofimova, Maria and Kuratov, Yuri and Sorokin, Alexey},
doi = {10.18653/v1/w19-3712},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Arkhipov et al. - 2019 - Tuning Multilingual Transformers for Language-Specific Named Entity Recognition.pdf:pdf},
keywords = {necitovane},
mendeley-tags = {necitovane},
month = {sep},
pages = {89--93},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Tuning Multilingual Transformers for Language-Specific Named Entity Recognition}},
url = {https://github.com/google-research/},
year = {2019}
}
@article{Chronopoulou2019,
abstract = {A growing number of state-of-the-art transfer learning methods employ language models pretrained on large generic corpora. In this paper we present a conceptually simple and effective transfer learning approach that addresses the problem of catastrophic forgetting. Specifically, we combine the task-specific optimization function with an auxiliary language model objective, which is adjusted during the training process. This preserves language regularities captured by language models, while enabling sufficient adaptation for solving the target task. Our method does not require pretraining or finetuning separate components of the network and we train our models end-to-end in a single step. We present results on a variety of challenging affective and text classification tasks, surpassing well established transfer learning methods with greater level of complexity.},
archivePrefix = {arXiv},
arxivId = {1902.10547},
author = {Chronopoulou, Alexandra and Baziotis, Christos and Potamianos, Alexandros},
eprint = {1902.10547},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Chronopoulou, Baziotis, Potamianos - 2019 - An Embarrassingly Simple Approach for Transfer Learning from Pretrained Language Models.pdf:pdf},
journal = {NAACL HLT 2019 - 2019 Conf. North Am. Chapter Assoc. Comput. Linguist. Hum. Lang. Technol. - Proc. Conf.},
keywords = {moznosti,necitovane},
mendeley-tags = {moznosti,necitovane},
month = {feb},
pages = {2089--2095},
publisher = {Association for Computational Linguistics (ACL)},
title = {{An Embarrassingly Simple Approach for Transfer Learning from Pretrained Language Models}},
url = {http://arxiv.org/abs/1902.10547},
volume = {1},
year = {2019}
}
@inproceedings{Yildiz2019,
abstract = {In this study, we present Morpheus, a joint contextual lemmatizer and morphological tagger. Morpheus is based on a neural sequential architecture where inputs are the characters of the surface words in a sentence and the outputs are the minimum edit operations between surface words and their lemmata as well as the morphological tags assigned to the words. The experiments on the datasets in nearly 100 languages provided by SigMorphon 2019 Shared Task 2 organizers show that the performance of Morpheus is comparable to the state-of-the-art system in terms of lemmatization. In morphological tagging, on the other hand, Morpheus significantly outperforms the SigMorphon baseline. In our experiments, we also show that the neural encoder-decoder architecture trained to predict the minimum edit operations can produce considerably better results than the architecture trained to predict the characters in lemmata directly as in previous studies. According to the SigMorphon 2019 Shared Task 2 results, Morpheus has placed 3rd in lemmatization and reached the 9th place in morphological tagging among all participant teams.},
author = {Yildiz, Eray and Tantuğ, A. C{\"{u}}neyd},
doi = {10.18653/v1/w19-4205},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Yildiz, Tantuğ - 2019 - Morpheus A Neural Network for Jointly Learning Contextual Lemmatization and Morphological Tagging.pdf:pdf},
title = {{Morpheus: A Neural Network for Jointly Learning Contextual Lemmatization and Morphological Tagging}},
year = {2019}
}
@techreport{Rumelhart,
author = {Rumelhart, DE and Hinton, GE and Nature, RJ Williams - and 1986, Undefined},
booktitle = {nature.com},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Rumelhart et al. - Unknown - Learning representations by back-propagating errors.pdf:pdf},
title = {{Learning representations by back-propagating errors}},
url = {https://www.nature.com/articles/323533a0},
year = {1986}
}
@techreport{Bengio2003,
abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.},
author = {Bengio, Yoshua and Ducharme, R{\'{e}}jean and Vincent, Pascal and Jauvin, Christian and Ca, Jauvinc@iro Umontreal and Kandola, Jaz and Hofmann, Thomas and Poggio, Tomaso and Shawe-Taylor, John},
booktitle = {J. Mach. Learn. Res.},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Bengio et al. - 2003 - A Neural Probabilistic Language Model.pdf:pdf},
keywords = {Statistical language modeling,artificial neural networks,curse of dimensionality,distributed representation,embeddings},
mendeley-tags = {embeddings},
pages = {1137--1155},
title = {{A Neural Probabilistic Language Model}},
volume = {3},
year = {2003}
}
@techreport{Hochreiter1997,
abstract = {Learning to store information over extended time intervals via recurrent backpropagation takes a very long time, mostly due to insuucient, decaying error back ow. We brieey review Hochreiter's 1991 analysis of this problem, then address it by introducing a novel, eecient, gradient-based method called $\backslash$Long Short-Term Memory" (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete time steps by enforcing constant error ow through $\backslash$constant error carrousels" within special units. Multiplicative gate units learn to open and close access to the constant error ow. LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artiicial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with RTRL, BPTT, Recurrent Cascade-Correlation, Elman nets, and Neural Sequence Chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artiicial long time lag tasks that have never been solved by previous recurrent network algorithms.},
author = {Hochreiter, Sepp and {Urgen Schmidhuber}, J J},
booktitle = {Mem. Neural Comput.},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Hochreiter, Urgen Schmidhuber - 1997 - Long short-term memory.pdf:pdf},
number = {8},
pages = {1735--1780},
title = {{Long short-term memory}},
url = {http://www7.informatik.tu-muenchen.de/{~}hochreithttp://www.idsia.ch/{~}juergen},
volume = {9},
year = {1997}
}
@inproceedings{Plisson,
abstract = {Lemmatization is the process of finding the normalized form of a word. It is the same as looking for a transformation to apply on a word to get its normalized form. The approach presented in this paper focuses on word endings: what word suffix should be removed and/or added to get the normalized form. This paper compares the results of two word lemmatization algorithms, one based on if-then rules and the other based on ripple down rules induction algorithms. It presents the problem of lemmatization of words from Slovene free text and explains why the Ripple Down Rules (RDR) approach is very well suited for the task. When learning from a corpus of lemmatized Slovene words the RDR approach results in easy to understand rules of improved classification accuracy compared to the results of rule learning achieved in previous work.},
author = {Plisson, Jo{\"{e}}l and Lavrac, Nada and Mladenic, Dunja},
booktitle = {Proc. 7th Int. Multiconference Inf. Soc.},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Plisson, Lavrac, Mladenic - Unknown - A Rule based Approach to Word Lemmatization.pdf:pdf},
keywords = {lemmatization,necitovane},
mendeley-tags = {lemmatization,necitovane},
pages = {83--86},
title = {{A Rule based Approach to Word Lemmatization}},
year = {2004}
}
@inproceedings{Toutanova2003,
abstract = {We present a new part-of-speech tagger that demonstrates the following ideas: (i) explicit use of both preceding and following tag contexts via a dependency network representation, (ii) broad use of lexical features, including jointly conditioning on multiple consecutive words, (iii) effective use of priors in conditional loglinear models, and (iv) ﬁne-grained modeling of unknown word features. Using these ideas together, the resulting tagger gives a 97.24{\%} accuracy on the Penn Treebank WSJ, an error reduction of 4.4{\%} on the best previous single automatically learned tagging result},
address = {Morristown, NJ, USA},
author = {Toutanova, Kristina and Klein, Dan and Manning, Christopher D. and Singer, Yoram},
booktitle = {Proc. 2003 Conf. North Am. Chapter Assoc. Comput. Linguist. Hum. Lang. Technol.  - NAACL '03},
doi = {10.3115/1073445.1073478},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Toutanova et al. - 2003 - Feature-rich part-of-speech tagging with a cyclic dependency network.pdf:pdf},
keywords = {necitovane},
mendeley-tags = {necitovane},
pages = {173--180},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Feature-rich part-of-speech tagging with a cyclic dependency network}},
url = {http://portal.acm.org/citation.cfm?doid=1073445.1073478},
volume = {1},
year = {2003}
}
@article{Feijo2020,
abstract = {BERT (Bidirectional Encoder Representations from Transformers) and ALBERT (A Lite BERT) are methods for pre-training language models which can later be fine-tuned for a variety of Natural Language Understanding tasks. These methods have been applied to a number of such tasks (mostly in English), achieving results that outperform the state-of-the-art. In this paper, our contribution is twofold. First, we make available our trained BERT and Albert model for Portuguese. Second, we compare our monolingual and the standard multilingual models using experiments in semantic textual similarity, recognizing textual entailment, textual category classification, sentiment analysis, offensive comment detection, and fake news detection, to assess the effectiveness of the generated language representations. The results suggest that both monolingual and multilingual models are able to achieve state-of-the-art and the advantage of training a single language model, if any, is small.},
archivePrefix = {arXiv},
arxivId = {2007.09757},
author = {Feijo, Diego de Vargas and Moreira, Viviane Pereira},
eprint = {2007.09757},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Feijo, Moreira - 2020 - Mono vs Multilingual Transformer-based Models a Comparison across Several Language Tasks.pdf:pdf},
journal = {arXiv},
month = {jul},
title = {{Mono vs Multilingual Transformer-based Models: a Comparison across Several Language Tasks}},
url = {http://arxiv.org/abs/2007.09757},
year = {2020}
}
@inproceedings{Montoyo2012a,
abstract = {In this introduction, we present an overview of the current state of research in the Natural Language Processing tasks of subjectivity and sentiment analysis, as well as their application domains and closely-related research field of emotion detection. Although many definitions exist for these tasks and the research done within their frame spans over approaches with different objectives, we consider subjectivity analysis to deal with the detection of "private states" (opinions, emotions, sentiments, beliefs, speculations) and sentiment analysis as the task of detecting, extracting and classifying opinions and sentiments concerning different topics, as expressed in textual input. After describing the key concepts and research directions in these tasks, we present the main achievements obtained so far and the issues that remain to be tackled. Subsequently, we introduce each of the papers in this volume and present their contribution to the research areas of subjectivity and sentiment analysis. Finally, we conclude on the present state of work in these fields and reflect on the possible future developments. {\textcopyright} 2012 Elsevier B.V. All rights reserved.},
author = {Montoyo, Andr{\'{e}}s and Mart{\'{i}}nez-Barco, Patricio and Balahur, Alexandra},
booktitle = {Decis. Support Syst.},
doi = {10.1016/j.dss.2012.05.022},
issn = {01679236},
keywords = {Emotion detection,Opinion mining,Sentiment analysis,Social media mining,Social network mining,Subjectivity analysis,Text mining},
month = {nov},
number = {4},
pages = {675--679},
publisher = {North-Holland},
title = {{Subjectivity and sentiment analysis: An overview of the current state of the area and envisaged developments}},
volume = {53},
year = {2012}
}
@article{Krizhevsky2017,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%}, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used nonsaturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
doi = {10.1145/3065386},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Krizhevsky, Sutskever, Hinton - 2017 - ImageNet classification with deep convolutional neural networks.pdf:pdf},
issn = {15577317},
journal = {Commun. ACM},
month = {jun},
number = {6},
pages = {84--90},
publisher = {Association for Computing Machinery},
title = {{ImageNet classification with deep convolutional neural networks}},
url = {https://dl.acm.org/doi/10.1145/3065386},
volume = {60},
year = {2017}
}
@article{Ustun2019,
abstract = {This paper describes our submission to SIG-MORPHON 2019 Task 2: Morphological analysis and lemmatization in context. Our model is a multi-task sequence to sequence neural network, which jointly learns morphological tagging and lemmatization. On the encoding side, we exploit character-level as well as contextual information. We introduce a multi-attention decoder to selectively focus on different parts of character and word sequences. To further improve the model, we train on multiple datasets simultaneously and use external embeddings for initialization. Our final model reaches an average morphological tagging F1 score of 94.54 and a lemma accuracy of 93.91 on the test data, ranking respectively 3rd and 6th out of 13 teams in the SIG-MORPHON 2019 shared task.},
author = {{\"{U}}st{\"{u}}n, A and van der Goot, R and {\ldots}, G Bouma - Proceedings of the 16th and undefined 2019},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/{\"{U}}st{\"{u}}n et al. - 2019 - Multi-Team A Multi-attention, Multi-decoder Approach to Morphological Analysis.pdf:pdf},
journal = {aclweb.org},
pages = {35--49},
title = {{Multi-Team: A Multi-attention, Multi-decoder Approach to Morphological Analysis.}},
url = {https://www.aclweb.org/anthology/W19-4206.pdf},
year = {2019}
}
@article{McCulloch,
author = {McCulloch, WS and bulletin of mathematical Biophysics, W Pitts - The and 1943, Undefined},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/McCulloch, biophysics, 1943 - Unknown - A logical calculus of the ideas immanent in nervous activity.pdf:pdf},
journal = {Bull. Math. Biophys. 5, 115–133},
keywords = {perceptron},
mendeley-tags = {perceptron},
title = {{A logical calculus of the ideas immanent in nervous activity}},
url = {https://link.springer.com/article/10.1007{\%}252FBF02478259},
year = {1943}
}
@techreport{Dai2015,
abstract = {We present two approaches that use unlabeled data to improve sequence learning with recurrent networks. The first approach is to predict what comes next in a sequence, which is a conventional language model in natural language processing. The second approach is to use a sequence autoencoder, which reads the input sequence into a vector and predicts the input sequence again. These two algorithms can be used as a "pretraining" step for a later supervised sequence learning algorithm. In other words, the parameters obtained from the unsupervised step can be used as a starting point for other supervised training models. In our experiments, we find that long short term memory recurrent networks after being pretrained with the two approaches are more stable and generalize better. With pretraining, we are able to train long short term memory recurrent networks up to a few hundred timesteps, thereby achieving strong performance in many text classification tasks, such as IMDB, DBpedia and 20 Newsgroups.},
archivePrefix = {arXiv},
arxivId = {1511.01432v1},
author = {Dai, Andrew M and Le, Quoc V},
eprint = {1511.01432v1},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Dai, Le - 2015 - Semi-supervised Sequence Learning.pdf:pdf},
keywords = {()},
title = {{Semi-supervised Sequence Learning}},
url = {http://ai.stanford.edu/amaas/data/sentiment/index.html},
year = {2015}
}
@article{Ganesh2020,
abstract = {Transformer-based models pre-trained on large-scale corpora achieve state-of-the-art accuracy for natural language processing tasks, but are too resource-hungry and compute-intensive to suit low-capability devices or applications with strict latency requirements. One potential remedy is model compression, which has attracted extensive attention. This paper summarizes the branches of research on compressing Transformers, focusing on the especially popular BERT model. BERT's complex architecture means that a compression technique that is highly effective on one part of the model, e.g., attention layers, may be less successful on another part, e.g., fully connected layers. In this systematic study, we identify the state of the art in compression for each part of BERT, clarify current best practices for compressing large-scale Transformer models, and provide insights into the inner workings of various methods. Our categorization and analysis also shed light on promising future research directions for achieving a lightweight, accurate, and generic natural language processing model.},
archivePrefix = {arXiv},
arxivId = {2002.11985},
author = {Ganesh, Prakhar and Chen, Yao and Lou, Xin and Khan, Mohammad Ali and Yang, Yin and Chen, Deming and Winslett, Marianne and Sajjad, Hassan and Nakov, Preslav},
eprint = {2002.11985},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Ganesh et al. - 2020 - Compressing Large-Scale Transformer-Based Models A Case Study on BERT.pdf:pdf},
journal = {arXiv},
month = {feb},
publisher = {arXiv},
title = {{Compressing Large-Scale Transformer-Based Models: A Case Study on BERT}},
url = {http://arxiv.org/abs/2002.11985},
year = {2020}
}
@techreport{Pruksachatkun,
abstract = {While pretrained models such as BERT have shown large gains across natural language understanding tasks, their performance can be improved by further training the model on a data-rich intermediate task, before fine-tuning it on a target task. However, it is still poorly understood when and why intermediate-task training is beneficial for a given target task. To investigate this, we perform a large-scale study on the pretrained RoBERTa model with 110 intermediate-target task combinations. We further evaluate all trained models with 25 probing tasks meant to reveal the specific skills that drive transfer. We observe that intermediate tasks requiring high-level inference and reasoning abilities tend to work best. We also observe that target task performance is strongly correlated with higher-level abilities such as coreference resolution. However , we fail to observe more granular correlations between probing and target task performance , highlighting the need for further work on broad-coverage probing benchmarks. We also observe evidence that the forgetting of knowledge learned during pretraining may limit our analysis, highlighting the need for further work on transfer learning methods in these settings.},
archivePrefix = {arXiv},
arxivId = {2005.00628v2},
author = {Pruksachatkun, Yada and Phang, Jason and Liu, Haokun and {Mon Htut}, Phu and Zhang, Xiaoyi and Pang, Richard Yuanzhe and Vania, Clara and Kann, Katharina and Bowman, Samuel R},
booktitle = {arxiv.org},
eprint = {2005.00628v2},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Pruksachatkun et al. - Unknown - Intermediate-Task Transfer Learning with Pretrained Models for Natural Language Understanding When and.pdf:pdf},
title = {{Intermediate-Task Transfer Learning with Pretrained Models for Natural Language Understanding: When and Why Does It Work?}},
url = {http://data.quora.com/First-Quora-DatasetRelease-},
year = {2020}
}
@techreport{Vaswani2017,
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
author = {Vaswani, Ashish and Brain, Google and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Vaswani et al. - Unknown - Attention Is All You Need.pdf:pdf},
title = {{Attention Is All You Need}},
year = {2017}
}
@article{Lewis2019,
abstract = {We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most influence end-task performance.},
archivePrefix = {arXiv},
arxivId = {1910.13461},
author = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
eprint = {1910.13461},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Lewis et al. - 2019 - BART Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension.pdf:pdf},
journal = {arXiv},
month = {oct},
publisher = {arXiv},
title = {{BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension}},
url = {http://arxiv.org/abs/1910.13461},
year = {2019}
}
@article{Brown,
abstract = {In this paper, we present a statistical approach to machine translation. We describe the application of our approach to translation from French to English and give preliminary results.},
author = {Brown, Peter F and Cocke, John and {Della Pietra}, Stephen A and {Della Pietra}, Vincent J and Jelinek, Fredrick and Lafferty, John D and Mercer, Robert L and Roossin, Paul S},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Brown et al. - Unknown - A STATISTICAL APPROACH TO MACHINE TRANSLATION.pdf:pdf},
journal = {Comput. linquistics},
pages = {79--85},
title = {{A STATISTICAL APPROACH TO MACHINE TRANSLATION}},
volume = {16(2)},
year = {1990}
}
@inproceedings{Cho2014,
abstract = {In this paper, we propose a novel neural network model called RNN Encoder- Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
archivePrefix = {arXiv},
arxivId = {1406.1078},
author = {Cho, Kyunghyun and {Van Merri{\"{e}}nboer}, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
booktitle = {EMNLP 2014 - 2014 Conf. Empir. Methods Nat. Lang. Process. Proc. Conf.},
doi = {10.3115/v1/d14-1179},
eprint = {1406.1078},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Cho et al. - 2014 - Learning phrase representations using RNN encoder-decoder for statistical machine translation.pdf:pdf},
isbn = {9781937284961},
keywords = {gru,transformers},
mendeley-tags = {gru,transformers},
month = {jun},
pages = {1724--1734},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Learning phrase representations using RNN encoder-decoder for statistical machine translation}},
url = {https://arxiv.org/abs/1406.1078v3},
year = {2014}
}
@techreport{Pennington,
abstract = {Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic , but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global log-bilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word co-occurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful sub-structure, as evidenced by its performance of 75{\%} on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.},
author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Pennington, Socher, Manning - Unknown - GloVe Global Vectors for Word Representation.pdf:pdf},
title = {{GloVe: Global Vectors for Word Representation}},
url = {http://nlp.},
year = {2014}
}
@article{Hercig2018,
abstract = {Sentiment analysis is a wide area with great potential and many research directions. One direction is stance detection, which is somewhat similar to sentiment analysis. We supplement stance detection dataset with sentiment annotation and explore the similarities of these tasks. We show that stance detection and sentiment analysis can be mutually beneficial by using gold label for one task as features for the other task. We analysed the presence of target entities for stance detection in the dataset. We outperform the state-of-the-art results for stance detection in Czech and set new state-of-the-art results for the newly created sentiment analysis part of the extended dataset.},
author = {Hercig, Tom{\'{a}}{\v{s}} and Krejzl, Peter and Kr{\'{a}}l, Pavel},
doi = {10.13053/CyS-22-3-3014},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Hercig, Krejzl, Kr{\'{a}}l - 2018 - Stance and sentiment in Czech.pdf:pdf},
issn = {20079737},
journal = {Comput. y Sist.},
keywords = {Czech,Natural language processing,Sentiment analysis,Stance detection,necitovane},
mendeley-tags = {necitovane},
number = {3},
pages = {787--794},
publisher = {Instituto Politecnico Nacional},
title = {{Stance and sentiment in Czech}},
url = {http://nlp.kiv.zcu.cz/research/sentiment{\#}stance.},
volume = {22},
year = {2018}
}
@inproceedings{Ruder2018,
abstract = {Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24{\%} on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100× more data. We open-source our pretrained models and code1},
author = {Howard, Jeremy and Ruder, Sebastian},
booktitle = {ACL 2018 - 56th Annu. Meet. Assoc. Comput. Linguist. Proc. Conf. (Long Pap.},
doi = {10.18653/v1/p18-1031},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Howard, Ruder - 2018 - Universal language model fine-tuning for text classification(2).pdf:pdf},
title = {{Universal language model fine-tuning for text classification}},
volume = {1},
year = {2018}
}
@inproceedings{Libovicky,
abstract = {In this work, we focus on three different NLP tasks: image captioning, machine translation, and sentiment analysis. We reimplement successful approaches of other authors and adapt them to the Czech language. We provide end-to-end architectures that achieve state-of-the-art or nearly state-of-the-art results on all of the tasks within a single sequence learning toolkit. The trained models are available both for download as well as in an online demo. 1 End-to-End Training Traditionally, solving tasks such as machine translation or sentiment analysis required complex processing pipelines consisting of tools which transformed one explicit representation of the data into another, with the structure of the internal representations defined by the system designer. In machine translation [24, 6], we would devise explicit word alignment links, extract phrase tables, train a language model, etc.; in sentiment analysis [32, 43], we could label the data with part-of-speech tags, decode their syntactic structure, and/or assign them with semantic labels. All of these more-or-less linguistically motivated internal representations are not inherently required to produce the desired output, but have been devised as clever and useful ways to break down the large and hard task into smaller and manageable substeps. With the advent of end-to-end training of deep neural networks (DNN) [26, 14, 28], the need for most of this has been eliminated. In the end-to-end learning paradigm, there is only one model, directly trained to produce the desired outputs from the inputs, without any explicit intermediate representations. The system designer now only has to design a rather generic architecture of the system. It mostly does not enforce any complex explicit representations and processing steps, but rather offers opportunities for the DNN to devise its own notion of intermediate representations and processing steps through training. This also means that similar architectures can be used to solve very different tasks. Rather than by the nature of the task itself, the structure of the DNN to use is mostly determined by the structure of the input and output -e.g. image inputs are processed by two-dimensional convolutions [27], while text inputs are processed by one-dimensional convolutions, recurrent units [38], and/or attentions [3], typically applied to word or subword embed-dings [5, 10, 33]; classification can produce its output in one step, while text generation is better done iteratively using recurrent decoders; etc. Thanks to that, a single general framework can be used to solve many different tasks. One just needs to transform the inputs and outputs into a suitable format, define an adequate network structure, and let the system train for a few days or weeks. Sadly, the burden of hyperparameter tuning has not been alleviated by DNNs, but rather made worse by the computational costliness of the training. However, with a bit of experience, one is often able to propose a suitable architecture and hyperparameter values at the first attempt, already achieving very competitive results even without any further tuning.},
author = {Libovick{\'{y}}, Jindřich and Rosa, Rudolf and Helcl, Jindřich and Popel, Martin},
booktitle = {Proc. 18th Conf. ITAT 2018 Slov. NLP Work. (SloNLP 2018)},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Libovick{\'{y}} et al. - Unknown - Solving Three Czech NLP Tasks End-to-End with Neural Models.pdf:pdf},
isbn = {11234/12839},
keywords = {necitovane},
mendeley-tags = {necitovane},
pages = {138--143},
publisher = {CreateSpace Independent Publishing Platform, Ko{\v{s}}ice},
title = {{Solving Three Czech NLP Tasks End-to-End with Neural Models}},
url = {https://www.yelp.com/dataset/},
year = {2018}
}
@inproceedings{Stickland2019,
abstract = {Multi-task learning shares information between related tasks, sometimes reducing the number of parameters required. State-of-the-art results across multiple natural language understanding tasks in the GLUE benchmark have previously used transfer from a single large task: unsuper-vised pre-training with BERT, where a separate BERT model was fine-tuned for each task. We explore multi-task approaches that share a single BERT model with a small number of additional task-specific parameters. Using new adaptation modules, PALs or 'projected attention layers', we match the performance of separately fine-tuned models on the GLUE benchmark with {\~{}}7 times fewer parameters, and obtain state-of-the-art results on the Recognizing Textual Entailment dataset.},
author = {Stickland, Asa Cooper and Murray, Iain},
booktitle = {36th Int. Conf. Mach. Learn. ICML 2019},
file = {:Users/petravysusilova/Downloads/stickland19a.pdf:pdf},
title = {{BERT and PALs: Projected attention layers for efficient adaptation in multi-task learning}},
volume = {2019-June},
year = {2019}
}
@techreport{Allen19,
abstract = {Recent developments in neural algorithms provide a new approach to natural language processing. Two sets of brief studies show how networks may be developed for processing simple demonstratives and analogies. Two longer studies consider pronoun reference and natural language translation. Taken together, the studies provide additional support for the applicability of these algorithms to natural language processing.},
author = {Allen, Robert B},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Allen - Unknown - Several Studies on Natural Language {\textperiodcentered} and Back-Propagation.pdf:pdf},
keywords = {transformers},
mendeley-tags = {transformers},
title = {{Several Studies on Natural Language {\textperiodcentered} and Back-Propagation}},
year = {1987}
}
@article{Sido2021,
abstract = {This paper describes the training process of the first Czech monolingual
language representation models based on BERT and ALBERT architectures. We
pre-train our models on more than 340K of sentences, which is 50 times more
than multilingual models that include Czech data. We outperform the
multilingual models on 9 out of 11 datasets. In addition, we establish the new
state-of-the-art results on nine datasets. At the end, we discuss properties of
monolingual and multilingual models based upon our results. We publish all the
pre-trained and fine-tuned models freely for the research community.},
archivePrefix = {arXiv},
arxivId = {2103.13031},
author = {Sido, Jakub and Pra{\v{z}}{\'{a}}k, Ondřej and Přib{\'{a}}ň, Pavel and Pa{\v{s}}ek, Jan and Sej{\'{a}}k, Michal and Konop{\'{i}}k, Miloslav},
eprint = {2103.13031},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Sido et al. - 2021 - Czert -- Czech BERT-like Model for Language Representation.pdf:pdf},
month = {mar},
title = {{Czert -- Czech BERT-like Model for Language Representation}},
url = {https://arxiv.org/abs/2103.13031v2},
year = {2021}
}
@misc{Hutchins,
author = {Hutchins, John},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Hutchins - Unknown - Two precursors of machine translation Artsrouni and Trojanskij.pdf:pdf},
title = {{Two precursors of machine translation: Artsrouni and Trojanskij}},
url = {https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.14.2564{\&}rep=rep1{\&}type=pdf},
urldate = {2020-10-23}
}
@article{Conneau2019,
abstract = {This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6{\%} average accuracy on XNLI, +13{\%} average F1 score on MLQA, and +2.4{\%} F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7{\%} in XNLI accuracy for Swahili and 11.4{\%} for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code, data and models publicly available.},
archivePrefix = {arXiv},
arxivId = {1911.02116},
author = {Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzm{\'{a}}n, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin},
eprint = {1911.02116},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Conneau et al. - 2019 - Unsupervised Cross-lingual Representation Learning at Scale.pdf:pdf},
journal = {arXiv},
month = {nov},
title = {{Unsupervised Cross-lingual Representation Learning at Scale}},
url = {http://arxiv.org/abs/1911.02116},
year = {2019}
}
@inproceedings{Zizka,
abstract = {The paper investigates a problem connected with automatic analysis of sentiment (opinion) in textual natural-language documents. The initial situation works on the assumption that a user has many documents centered around a certain topic with different opinions of it. The user wants to pick out only relevant documents that represent a certain sentiment-for example, only positive reviews of a certain subject. Having not too many typical patterns of the desired document type, the user needs a tool that can collect documents which are similar to the patterns. The suggested procedure is based on computing the similarity degree between patterns and unlabeled documents, which are then ranked according to their similarity to the patterns. The similarity is calculated as a distance between patterns and unlabeled items. The results are shown for publicly accessible down-loaded real-world data in two languages, English and Czech.},
author = {{\v{Z}}i{\v{z}}ka, Jan and Dařena, Franti{\v{s}}ek},
booktitle = {Int. Conf. Text, Speech Dialogue},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/{\v{Z}}i{\v{z}}ka, Dařena - Unknown - Automatic Sentiment Analysis Using the Textual Pattern Content Similarity in Natural Language.pdf:pdf},
keywords = {natural language,sentiment/opinion analysis,similarity ranking,textual document similarity,textual patterns},
publisher = {Springer},
title = {{Automatic Sentiment Analysis Using the Textual Pattern Content Similarity in Natural Language}},
year = {2010}
}
@article{Straka2019,
abstract = {Contextualized embeddings, which capture appropriate word meaning depending on context, have recently been proposed. We evaluate two meth ods for precomputing such embeddings, BERT and Flair, on four Czech text processing tasks: part-of-speech (POS) tagging, lemmatization, dependency pars ing and named entity recognition (NER). The first three tasks, POS tagging, lemmatization and dependency parsing, are evaluated on two corpora: the Prague Dependency Treebank 3.5 and the Universal Dependencies 2.3. The named entity recognition (NER) is evaluated on the Czech Named Entity Corpus 1.1 and 2.0. We report state-of-the-art results for the above mentioned tasks and corpora.},
archivePrefix = {arXiv},
arxivId = {1909.03544},
author = {Straka, Milan and Strakov{\'{a}}, Jana and Haji{\v{c}}, Jan},
eprint = {1909.03544},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Straka, Strakov{\'{a}}, Haji{\v{c}} - 2019 - Czech Text Processing with Contextual Embeddings POS Tagging, Lemmatization, Parsing and NER.pdf:pdf},
journal = {Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics)},
keywords = {BERT,Contextualized embeddings,Czech,Dependency parsing,Flair,Lemmatization,Named entity recognition,POS tagging},
month = {sep},
pages = {137--150},
publisher = {Springer Verlag},
title = {{Czech Text Processing with Contextual Embeddings: POS Tagging, Lemmatization, Parsing and NER}},
url = {http://arxiv.org/abs/1909.03544},
volume = {11697 LNAI},
year = {2019}
}
@book{Mitchell1997,
author = {Mitchell, Tom M.},
edition = {McGraw-Hil},
pages = {99},
publisher = {New York : McGraw-Hill},
title = {{Machine Learning}},
year = {1997}
}
@article{Li,
abstract = {In this paper, we investigate the modeling power of contextualized embeddings from pre-trained language models, e.g. BERT, on the E2E-ABSA task. Specifically, we build a series of simple yet insightful neural base-lines to deal with E2E-ABSA. The experimental results show that even with a simple linear classification layer, our BERT-based architecture can outperform state-of-the-art works. Besides, we also standardize the comparative study by consistently utilizing a hold-out development dataset for model selection, which is largely ignored by previous works. Therefore , our work can serve as a BERT-based benchmark for E2E-ABSA. 1},
author = {Li, Xin and Bing, Lidong and Zhang, Wenxuan and Lam, Wai},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Li et al. - Unknown - Exploiting BERT for End-to-End Aspect-based Sentiment Analysis.pdf:pdf},
journal = {CoRR},
keywords = {necitovane},
mendeley-tags = {necitovane},
title = {{Exploiting BERT for End-to-End Aspect-based Sentiment Analysis *}},
url = {http://arxiv.org/abs/1910.00883},
volume = {abs/1910.0},
year = {2019}
}
@article{Yosinski2014,
abstract = {Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.},
archivePrefix = {arXiv},
arxivId = {1411.1792},
author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
eprint = {1411.1792},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Yosinski et al. - 2014 - How transferable are features in deep neural networks.pdf:pdf},
journal = {Adv. Neural Inf. Process. Syst.},
month = {nov},
number = {January},
pages = {3320--3328},
publisher = {Neural information processing systems foundation},
title = {{How transferable are features in deep neural networks?}},
url = {http://arxiv.org/abs/1411.1792},
volume = {4},
year = {2014}
}
@article{Zhang2019,
abstract = {Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding. We argue that informative entities in KGs can enhance language representation with external knowledge. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks. The source code of this paper can be obtained from https://github.com/thunlp/ERNIE.},
archivePrefix = {arXiv},
arxivId = {1905.07129},
author = {Zhang, Zhengyan and Han, Xu and Liu, Zhiyuan and Jiang, Xin and Sun, Maosong and Liu, Qun},
eprint = {1905.07129},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Zhang et al. - 2019 - ERNIE Enhanced Language Representation with Informative Entities(2).pdf:pdf},
journal = {ACL 2019 - 57th Annu. Meet. Assoc. Comput. Linguist. Proc. Conf.},
month = {may},
pages = {1441--1451},
publisher = {Association for Computational Linguistics (ACL)},
title = {{ERNIE: Enhanced Language Representation with Informative Entities}},
url = {http://arxiv.org/abs/1905.07129},
year = {2019}
}
@article{Rosa2019,
abstract = {We use the English model of BERT and explore how a deletion of one word in a sentence changes representations of other words. Our hypothesis is that removing a reducible word (e.g. an adjective) does not affect the representation of other words so much as removing e.g. the main verb, which makes the sentence ungrammatical and of "high surprise" for the language model. We estimate reducibilities of individual words and also of longer continuous phrases (word n-grams), study their syntax-related properties, and then also use them to induce full dependency trees.},
archivePrefix = {arXiv},
arxivId = {1906.11511},
author = {Rosa, Rudolf and Mare{\v{c}}ek, David},
eprint = {1906.11511},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Rosa, Mare{\v{c}}ek - 2019 - Inducing Syntactic Trees from BERT Representations.pdf:pdf},
journal = {arXiv},
month = {jun},
publisher = {arXiv},
title = {{Inducing Syntactic Trees from BERT Representations}},
url = {http://arxiv.org/abs/1906.11511},
year = {2019}
}
@article{Taylor1953,
abstract = {{\textless}p{\textgreater}Here is the first comprehensive statement of a research method and its theory which were introduced briefly during a workshop at the 1953 AEJ convention. Included are findings from three pilot studies and two experiments in which “cloze procedure” results are compared with those of two readability formulas.{\textless}/p{\textgreater}},
author = {Taylor, Wilson L.},
doi = {10.1177/107769905303000401},
issn = {0022-5533},
journal = {Journal. Q.},
month = {sep},
number = {4},
pages = {415--433},
publisher = {SAGE Publications},
title = {{“Cloze Procedure”: A New Tool for Measuring Readability}},
url = {http://journals.sagepub.com/doi/10.1177/107769905303000401},
volume = {30},
year = {1953}
}
@article{Raffel2019a,
abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
archivePrefix = {arXiv},
arxivId = {1910.10683},
author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
eprint = {1910.10683},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Raffel et al. - 2019 - Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer(3).pdf:pdf},
journal = {arXiv},
keywords = {T5,attention-based models,deep learning,multi-task learning,natural language processing,transfer learning},
mendeley-tags = {T5},
month = {oct},
pages = {1--67},
publisher = {arXiv},
title = {{Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer}},
url = {http://arxiv.org/abs/1910.10683},
volume = {21},
year = {2019}
}
@article{Tenney2019,
abstract = {Pre-trained text encoders have rapidly advanced the state of the art on many NLP tasks. We focus on one such model, BERT, and aim to quantify where linguistic information is captured within the network. We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lower-level decisions on the basis of disambiguating information from higher-level representations.},
archivePrefix = {arXiv},
arxivId = {1905.05950},
author = {Tenney, Ian and Das, Dipanjan and Pavlick, Ellie},
eprint = {1905.05950},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Tenney, Das, Pavlick - 2019 - BERT Rediscovers the Classical NLP Pipeline.pdf:pdf},
journal = {ACL 2019 - 57th Annu. Meet. Assoc. Comput. Linguist. Proc. Conf.},
keywords = {necitovane},
mendeley-tags = {necitovane},
month = {may},
pages = {4593--4601},
publisher = {Association for Computational Linguistics (ACL)},
title = {{BERT Rediscovers the Classical NLP Pipeline}},
url = {http://arxiv.org/abs/1905.05950},
year = {2019}
}
@techreport{Bahdanau,
abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neu-ral machine translation often belong to a family of encoder-decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture , and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
archivePrefix = {arXiv},
arxivId = {1409.0473v7},
author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
eprint = {1409.0473v7},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Bahdanau, Cho, Bengio - Unknown - NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE.pdf:pdf},
title = {{NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE}},
year = {2014}
}
@inproceedings{Brill1998,
abstract = {One of the most exciting recent directions in machine learning is the discovery that the combination of multiple classifiers often results in significantly better performance than what can be achieved with a single classifier. In this paper, we first show that the errors made from three different state of the art part of speech taggers are strongly complementary. Next, we show how this complementary behavior can be used to our advantage. By using contextual cues to guide tagger combination, we are able to derive a new tagger that achieves performance significantly greater than any of the individual taggers. Introduction Part of speech tagging has been a central problem in natural language processing for many years. Since the advent of manually tagged corpora such as the Brown Corpus and the Penn Treebank (Francis(1982), Marcus(1993)), the efficacy of machine learning for training a tagger has been demonstrated using a wide array of techniques, including: Markov models, decision trees...},
author = {Brill, Eric and Wu, Jun},
doi = {10.3115/980845.980876},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Brill, Wu - 1998 - Classifier combination for improved lexical disambiguation.pdf:pdf},
keywords = {necitovane},
mendeley-tags = {necitovane},
pages = {191},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Classifier combination for improved lexical disambiguation}},
url = {http://www.cis.upenn.edu/-adwait},
year = {1998}
}
@article{Hoerl1970,
abstract = {In multiple regression it is shown that parameter estimates based on minimum residual sum of squares have a high probability of being unsatisfactory, if not incorrect, if the prediction vectors are not orthogonal. Proposed is an estimation procedure based on adding small positive quantities to the diagonal of X′X. Introduced is the ridge trace, a method for showing in two dimensions the effects of nonorthogonality. It is then shown how to augment X′X to obtain biased estimates with smaller mean square error. {\textcopyright} 1970 Taylor and Francis Group, LLC.},
author = {Hoerl, Arthur E. and Kennard, Robert W.},
doi = {10.1080/00401706.1970.10488634},
issn = {15372723},
journal = {Technometrics},
number = {1},
pages = {55--67},
title = {{Ridge Regression: Biased Estimation for Nonorthogonal Problems}},
volume = {12},
year = {1970}
}
@inproceedings{Montoyo2012,
abstract = {In this introduction, we present an overview of the current state of research in the Natural Language Processing tasks of subjectivity and sentiment analysis, as well as their application domains and closely-related research field of emotion detection. Although many definitions exist for these tasks and the research done within their frame spans over approaches with different objectives, we consider subjectivity analysis to deal with the detection of "private states" (opinions, emotions, sentiments, beliefs, speculations) and sentiment analysis as the task of detecting, extracting and classifying opinions and sentiments concerning different topics, as expressed in textual input. After describing the key concepts and research directions in these tasks, we present the main achievements obtained so far and the issues that remain to be tackled. Subsequently, we introduce each of the papers in this volume and present their contribution to the research areas of subjectivity and sentiment analysis. Finally, we conclude on the present state of work in these fields and reflect on the possible future developments. {\textcopyright} 2012 Elsevier B.V. All rights reserved.},
author = {Montoyo, Andr{\'{e}}s and Mart{\'{i}}nez-Barco, Patricio and Balahur, Alexandra},
booktitle = {Decis. Support Syst.},
doi = {10.1016/j.dss.2012.05.022},
file = {:Users/petravysusilova/Downloads/subjecti.pdf:pdf},
issn = {01679236},
keywords = {Emotion detection,Opinion mining,Sentiment analysis,Social media mining,Social network mining,Subjectivity analysis,Text mining},
month = {nov},
number = {4},
pages = {675--679},
title = {{Subjectivity and sentiment analysis: An overview of the current state of the area and envisaged developments}},
volume = {53},
year = {2012}
}
@article{Russakovsky2015,
abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.},
archivePrefix = {arXiv},
arxivId = {1409.0575},
author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
doi = {10.1007/s11263-015-0816-y},
eprint = {1409.0575},
issn = {15731405},
journal = {Int. J. Comput. Vis.},
keywords = {Benchmark,Dataset,Large-scale,Object detection,Object recognition},
month = {dec},
number = {3},
pages = {211--252},
publisher = {Springer New York LLC},
title = {{ImageNet Large Scale Visual Recognition Challenge}},
url = {https://link.springer.com/article/10.1007/s11263-015-0816-y},
volume = {115},
year = {2015}
}
@article{Putra,
abstract = {Compared to English, the amount of labeled data for Indonesian text classification tasks is very small. Recently developed multilingual language models have shown its ability to create multilingual representations effectively. This paper investigates the effect of combining English and Indonesian data on building Indonesian text classification (e.g., sentiment analysis and hate speech) using multilingual language models. Using the feature-based approach, we observe its performance on various data sizes and total added English data. The experiment showed that the addition of English data, especially if the amount of Indonesian data is small, improves performance. Using the fine-tuning approach, we further showed its effectiveness in utilizing the English language to build Indonesian text classification models.},
author = {Putra, Ilham Firdausi and Purwarianti, Ayu and Ai-Vlb, U-Coe},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Putra, Purwarianti, Ai-Vlb - Unknown - Improving Indonesian Text Classification Using Multilingual Language Model.pdf:pdf},
journal = {Int. Conf. Adv. Informatics Concept, Theory Appl.},
keywords = {Indonesian text,hate speech classification,multilingual language model,necitovane,practise,sentiment analysis,text classification},
mendeley-tags = {necitovane,practise},
title = {{Improving Indonesian Text Classification Using Multilingual Language Model}},
url = {https://www.yelp.com/dataset},
year = {2020}
}
@article{Virtanen2019,
abstract = {Deep learning-based language models pretrained on large unannotated text corpora have been demonstrated to allow efficient transfer learning for natural language processing, with recent approaches such as the transformer-based BERT model advancing the state of the art across a variety of tasks. While most work on these models has focused on high-resource languages, in particular English, a number of recent efforts have introduced multilingual models that can be fine-tuned to address tasks in a large number of different languages. However, we still lack a thorough understanding of the capabilities of these models, in particular for lower-resourced languages. In this paper, we focus on Finnish and thoroughly evaluate the multilingual BERT model on a range of tasks, comparing it with a new Finnish BERT model trained from scratch. The new language-specific model is shown to systematically and clearly outperform the multilingual. While the multilingual model largely fails to reach the performance of previously proposed methods, the custom Finnish BERT model establishes new state-of-the-art results on all corpora for all reference tasks: part-of-speech tagging, named entity recognition, and dependency parsing. We release the model and all related resources created for this study with open licenses at https://turkunlp.org/finbert .},
archivePrefix = {arXiv},
arxivId = {1912.07076},
author = {Virtanen, Antti and Kanerva, Jenna and Ilo, Rami and Luoma, Jouni and Luotolahti, Juhani and Salakoski, Tapio and Ginter, Filip and Pyysalo, Sampo},
eprint = {1912.07076},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Virtanen et al. - 2019 - Multilingual is not enough BERT for Finnish.pdf:pdf},
journal = {arXiv},
keywords = {bertology,necitovane,practise},
mendeley-tags = {bertology,necitovane,practise},
month = {dec},
title = {{Multilingual is not enough: BERT for Finnish}},
url = {http://arxiv.org/abs/1912.07076},
year = {2019}
}
@inproceedings{Kingma2015,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P. and Ba, Jimmy Lei},
booktitle = {3rd Int. Conf. Learn. Represent. ICLR 2015 - Conf. Track Proc.},
eprint = {1412.6980},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Kingma, Ba - 2015 - Adam A method for stochastic optimization.pdf:pdf},
month = {dec},
publisher = {International Conference on Learning Representations, ICLR},
title = {{Adam: A method for stochastic optimization}},
url = {https://arxiv.org/abs/1412.6980v9},
year = {2015}
}
@article{Clark2019,
abstract = {Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data. Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classifiers). Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to BERT. BERT's attention heads exhibit patterns such as attending to delimiter tokens, specific positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors. We further show that certain attention heads correspond well to linguistic notions of syntax and coreference. For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy. Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in BERT's attention.},
archivePrefix = {arXiv},
arxivId = {1906.04341},
author = {Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D.},
eprint = {1906.04341},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Clark et al. - 2019 - What Does BERT Look At An Analysis of BERT's Attention.pdf:pdf},
journal = {arXiv},
month = {jun},
publisher = {arXiv},
title = {{What Does BERT Look At? An Analysis of BERT's Attention}},
url = {http://arxiv.org/abs/1906.04341},
year = {2019}
}
@inproceedings{Ramachandran2017,
abstract = {This work presents a general unsupervised learning method to improve the accuracy of sequence to sequence (seq2seq) models. In our method, the weights of the encoder and decoder of a seq2seq model are initialized with the pretrained weights of two language models and then fine-tuned with labeled data. We apply this method to challenging benchmarks in machine translation and abstractive summarization and find that it significantly improves the subsequent supervised models. Our main result is that pretraining improves the generalization of seq2seq models. We achieve state-of-the-art results on the WMT English→German task, surpassing a range of methods using both phrase-based machine translation and neural machine translation. Our method achieves a significant improvement of 1.3 BLEU from the previous best models on both WMT'14 and WMT'15 English→German. We also conduct human evaluations on abstractive summarization and find that our method outperforms a purely supervised learning baseline in a statistically significant manner.},
archivePrefix = {arXiv},
arxivId = {1611.02683},
author = {Ramachandran, Prajit and Liu, Peter J. and Le, Quoc V.},
booktitle = {EMNLP 2017 - Conf. Empir. Methods Nat. Lang. Process. Proc.},
doi = {10.18653/v1/d17-1039},
eprint = {1611.02683},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Ramachandran, Liu, Le - 2017 - Unsupervised pretraining for sequence to sequence learning.pdf:pdf},
isbn = {9781945626838},
pages = {383--391},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Unsupervised pretraining for sequence to sequence learning}},
year = {2017}
}
@article{Sutskever2014,
abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
archivePrefix = {arXiv},
arxivId = {1409.3215},
author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
eprint = {1409.3215},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Sutskever, Vinyals, Le - 2014 - Sequence to Sequence Learning with Neural Networks.pdf:pdf},
journal = {Adv. Neural Inf. Process. Syst.},
keywords = {transformers},
mendeley-tags = {transformers},
month = {sep},
number = {January},
pages = {3104--3112},
publisher = {Neural information processing systems foundation},
title = {{Sequence to Sequence Learning with Neural Networks}},
url = {http://arxiv.org/abs/1409.3215},
volume = {4},
year = {2014}
}
@inproceedings{Devlin2019,
abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5{\%} (7.7{\%} point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
archivePrefix = {arXiv},
arxivId = {1810.04805},
author = {Devlin, Jacob and Chang, Ming Wei and Lee, Kenton and Toutanova, Kristina},
booktitle = {NAACL HLT 2019 - 2019 Conf. North Am. Chapter Assoc. Comput. Linguist. Hum. Lang. Technol. - Proc. Conf.},
eprint = {1810.04805},
file = {:Users/petravysusilova/Downloads/1810.04805.pdf:pdf},
isbn = {9781950737130},
pages = {4171--4186},
publisher = {Association for Computational Linguistics (ACL)},
title = {{BERT: Pre-training of deep bidirectional transformers for language understanding}},
volume = {1},
year = {2019}
}
@techreport{Goldberg,
author = {Goldberg, Yoav},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Goldberg - Unknown - A Primer on Neural Network Models for Natural Language Processing.pdf:pdf},
keywords = {modeling,necitovane},
mendeley-tags = {modeling,necitovane},
title = {{A Primer on Neural Network Models for Natural Language Processing}},
url = {http://www.cs.biu.},
year = {2015}
}
@article{RadfordAlec2019,
abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension , and summarization, are typically approached with supervised learning on task-specific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset-matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
author = {{Radford Alec} and {Wu Jeffrey} and {Child Rewon} and {Luan David} and {Amodei Dario} and {Sutskever Ilya}},
file = {:Users/petravysusilova/Downloads/radford2019language.pdf:pdf},
journal = {OpenAI Blog},
number = {8},
title = {{Language Models are Unsupervised Multitask Learners | Enhanced Reader}},
volume = {1},
year = {2019}
}
@article{Liu2019,
abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
archivePrefix = {arXiv},
arxivId = {1907.11692},
author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
eprint = {1907.11692},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Liu et al. - 2019 - RoBERTa A Robustly Optimized BERT Pretraining Approach(2).pdf:pdf},
journal = {arXiv},
keywords = {necitovane},
mendeley-tags = {necitovane},
month = {jul},
publisher = {arXiv},
title = {{RoBERTa: A Robustly Optimized BERT Pretraining Approach}},
url = {http://arxiv.org/abs/1907.11692},
year = {2019}
}
@techreport{Szegedy2015,
abstract = {We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the Im-ageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular in-carnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
booktitle = {cv-foundation.org},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Szegedy et al. - Unknown - Going Deeper with Convolutions.pdf:pdf},
keywords = {labelsmoothing},
mendeley-tags = {labelsmoothing},
title = {{Going Deeper with Convolutions}},
url = {https://www.cv-foundation.org/openaccess/content{\_}cvpr{\_}2015/html/Szegedy{\_}Going{\_}Deeper{\_}With{\_}2015{\_}CVPR{\_}paper.html},
year = {2015}
}
@misc{Du2019,
abstract = {Word Sense Disambiguation (WSD), which aims to identify the correct sense of a given polyseme, is a long-standing problem in NLP. In this paper, we propose to use BERT to extract better polyseme representations for WSD and explore several ways of combining BERT and the classifier. We also utilize sense definitions to train a unified classifier for all words, which enables the model to disambiguate unseen polysemes. Experiments show that our model achieves the state-of-the-art results on the standard English All-word WSD evaluation.},
author = {Du, Jiaju and Qi, Fanchao and Sun, Maosong},
booktitle = {arXiv},
issn = {23318422},
title = {{Using BERT for word sense disambiguation}},
year = {2019}
}
@misc{Brychcin2013,
abstract = {Current approaches to document-level sentiment analysis rely on local information , e.g., the words within the given document. We try to achieve better performance by incorporating global context of the sentiment target (e.g., a movie or a product). We assume that sentiment labels of reviews about the same target are often consistent in some way. We model this consistency by Dirichlet distribution over sentiment labels and use it together with Maximum entropy classifier to gain significant improvement. This unsuper-vised extension increases the classification F-measure by almost 3{\%} absolute on both Czech and English movie review datasets and outperforms the current state of the art.},
author = {Brychc{\'{i}}n, Tom{\'{a}}{\v{s}} and Habernal, Ivan},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Brychc{\'{i}}n, Habernal - 2013 - Unsupervised Improving of Sentiment Analysis Using Global Target Context(2).pdf:pdf},
pages = {122--128},
title = {{Unsupervised Improving of Sentiment Analysis Using Global Target Context}},
url = {https://aclanthology.org/R13-1016},
year = {2013}
}
@article{Pan2009,
abstract = {A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classification, regression and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multi-task learning and sample selection bias, as well as co-variate shift. We also explore some potential future issues in transfer learning research.},
author = {Pan, Sinno Jialin and Yang, Qiang},
doi = {10.1109/TKDE.2009.191},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Pan, Yang - 2009 - A Survey on Transfer Learning.pdf:pdf},
journal = {IEEE Trans. Knowl. Data Eng.},
keywords = {Data Mining,Index Terms-Transfer Learning,Machine Learning,Survey},
number = {10},
pages = {1345--1359},
title = {{A Survey on Transfer Learning}},
url = {http://socrates.acadiau.ca/courses/comp/dsilver/NIPS95},
volume = {22},
year = {2010}
}
@techreport{Straka2019b,
abstract = {We present our contribution to the SIGMOR-PHON 2019 Shared Task: Crosslinguality and Context in Morphology, Task 2: contextual morphological analysis and lemmatization. We submitted a modification of the UDPipe 2.0, one of best-performing systems of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies and an overall winner of the The 2018 Shared Task on Extrinsic Parser Evaluation. As our first improvement, we use the pre-trained contextualized embeddings (BERT) as additional inputs to the network; secondly, we use individual morphological features as reg-ularization; and finally, we merge the selected corpora of the same language. In the lemmatization task, our system exceeds all the submitted systems by a wide margin with lemmatization accuracy 95.78 (second best was 95.00, third 94.46). In the morphological analysis, our system placed tightly second: our morphological analysis accuracy was 93.19, the winning system's 93.23.},
author = {Straka, Milan and Strakov{\'{a}}, Jana and Haji{\v{c}}, Jan},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Straka, Strakov{\'{a}}, Haji{\v{c}} - 2019 - Regularization with Morphological Categories, Corpora Merging.pdf:pdf},
keywords = {necitovane},
mendeley-tags = {necitovane},
pages = {95--103},
title = {{Regularization with Morphological Categories, Corpora Merging}},
url = {https://github.com/google-research/},
year = {2019}
}
@article{Rosenblatt1958,
abstract = {To answer the questions of how information about the physical world is sensed, in what form is information remembered, and how does information retained in memory influence recognition and behavior, a theory is developed for a hypothetical nervous system called a perceptron. The theory serves as a bridge between biophysics and psychology. It is possible to predict learning curves from neurological variables and vice versa. The quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems. 18 references. (PsycINFO Database Record (c) 2006 APA, all rights reserved). {\textcopyright} 1958 American Psychological Association.},
author = {Rosenblatt, F.},
doi = {10.1037/h0042519},
issn = {0033295X},
journal = {Psychol. Rev.},
keywords = {PERCEPTION, AS INFORMATION STORAGE MODEL INFORMATI},
month = {nov},
number = {6},
pages = {386--408},
pmid = {13602029},
title = {{The perceptron: A probabilistic model for information storage and organization in the brain}},
volume = {65},
year = {1958}
}
@article{Kondratyuk2019,
abstract = {We present UDify, a multilingual multi-task model capable of accurately predicting universal part-of-speech, morphological features, lemmas, and dependency trees simultaneously for all 124 Universal Dependencies treebanks across 75 languages. By leveraging a multilingual BERT self-attention model pretrained on 104 languages, we found that fine-tuning it on all datasets concatenated together with simple softmax classifiers for each UD task can result in state-of-the-art UPOS, UFeats, Lemmas, UAS, and LAS scores, without requiring any recurrent or language-specific components. We evaluate UDify for multilingual learning, showing that low-resource languages benefit the most from cross-linguistic annotations. We also evaluate for zero-shot learning, with results suggesting that multilingual training provides strong UD predictions even for languages that neither UDify nor BERT have ever been trained on. Code for UDify is available at https://github.com/hyperparticle/udify.},
archivePrefix = {arXiv},
arxivId = {1904.02099},
author = {Kondratyuk, Dan and Straka, Milan},
eprint = {1904.02099},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Kondratyuk, Straka - 2019 - 75 Languages, 1 Model Parsing Universal Dependencies Universally.pdf:pdf},
journal = {EMNLP-IJCNLP 2019 - 2019 Conf. Empir. Methods Nat. Lang. Process. 9th Int. Jt. Conf. Nat. Lang. Process. Proc. Conf.},
month = {apr},
pages = {2779--2795},
publisher = {Association for Computational Linguistics},
title = {{75 Languages, 1 Model: Parsing Universal Dependencies Universally}},
url = {http://arxiv.org/abs/1904.02099},
year = {2019}
}
@article{Ettinger2019,
abstract = {Pre-training by language modeling has become a popular and successful approach to NLP tasks, but we have yet to understand exactly what linguistic capacities these pre-training processes confer upon models. In this paper we introduce a suite of diagnostics drawn from human language experiments, which allow us to ask targeted questions about the information used by language models for generating predictions in context. As a case study, we apply these diagnostics to the popular BERT model, finding that it can generally distinguish good from bad completions involving shared category or role reversal, albeit with less sensitivity than humans, and it robustly retrieves noun hypernyms, but it struggles with challenging inferences and role-based event prediction -- and in particular, it shows clear insensitivity to the contextual impacts of negation.},
archivePrefix = {arXiv},
arxivId = {1907.13528},
author = {Ettinger, Allyson},
eprint = {1907.13528},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Ettinger - 2019 - What BERT is not Lessons from a new suite of psycholinguistic diagnostics for language models.pdf:pdf},
journal = {arXiv},
month = {jul},
publisher = {arXiv},
title = {{What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models}},
url = {http://arxiv.org/abs/1907.13528},
year = {2019}
}
@article{Dai2019,
abstract = {Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80{\%} longer than RNNs and 450{\%} longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.},
archivePrefix = {arXiv},
arxivId = {1901.02860},
author = {Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V. and Salakhutdinov, Ruslan},
eprint = {1901.02860},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Dai et al. - 2019 - Transformer-XL Attentive Language Models Beyond a Fixed-Length Context.pdf:pdf},
journal = {ACL 2019 - 57th Annu. Meet. Assoc. Comput. Linguist. Proc. Conf.},
month = {jan},
pages = {2978--2988},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context}},
url = {http://arxiv.org/abs/1901.02860},
year = {2019}
}
@article{Yang2019,
abstract = {With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.},
archivePrefix = {arXiv},
arxivId = {1906.08237},
author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V.},
eprint = {1906.08237},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Yang et al. - 2019 - XLNet Generalized Autoregressive Pretraining for Language Understanding.pdf:pdf},
journal = {arXiv},
month = {jun},
publisher = {arXiv},
title = {{XLNet: Generalized Autoregressive Pretraining for Language Understanding}},
url = {http://arxiv.org/abs/1906.08237},
year = {2019}
}
@techreport{Schwenk2006,
abstract = {Statistical machine translation systems are based on one or more translation models and a language model of the target language. While many different translation models and phrase extraction algorithms have been proposed, a standard word n-gram back-off language model is used in most systems. In this work, we propose to use a new statistical language model that is based on a continuous representation of the words in the vocabulary. A neural network is used to perform the projection and the probability estimation. We consider the translation of European Parliament Speeches. This task is part of an international evaluation organized by the TC-STAR project in 2006. The proposed method achieves consistent improvements in the BLEU score on the development and test data. We also present algorithms to improve the estimation of the language model probabilities when splitting long sentences into shorter chunks.},
author = {Schwenk, Holger and Dchelotte, Daniel and Gauvain, Jean-Luc},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Schwenk, Dchelotte, Gauvain - 2006 - Continuous Space Language Models for Statistical Machine Translation.pdf:pdf},
keywords = {modeling},
mendeley-tags = {modeling},
pages = {723--730},
title = {{Continuous Space Language Models for Statistical Machine Translation}},
year = {2006}
}
@techreport{Hajic,
abstract = {Statistical modeling is now the prevailing method using in automatic procedures of analysis of a natural language. Such an analysis can be performed at various levels, from phonetics to semantics. Two levels of representation are described: a morphological one and a syntactic one that is further subdivided into a surface syntax and deep syntax (tectogrammatics). The role of linguistically annotated corpora will be stressed as a necessary prerequisite for any supervised machine learning algorithms, showing examples from the Prague Dependency Treebank (PDT) being developed at Charles University, Prague. A possible application of some of the tools created during (and thanks to) the development of the PDT will be shown, namely, a machine translation system translating from Czech to Slovak.},
author = {Haji{\v{c}}, Jan},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Haji{\v{c}} - Unknown - Statistick{\'{e}} modelov{\'{a}}n{\'{i}} a automatick{\'{a}} anal{\'{y}}za přirozen{\'{e}}ho jazyka (morfologie, syntax, překlad).pdf:pdf},
keywords = {necitovane},
mendeley-tags = {necitovane},
title = {{Statistick{\'{e}} modelov{\'{a}}n{\'{i}} a automatick{\'{a}} anal{\'{y}}za přirozen{\'{e}}ho jazyka (morfologie, syntax, překlad)}}
}
@article{Huang2015,
abstract = {In this paper, we propose a variety of Long Short-Term Memory (LSTM) based models for sequence tagging. These models include LSTM networks, bidirectional LSTM (BI-LSTM) networks, LSTM with a Conditional Random Field (CRF) layer (LSTM-CRF) and bidirectional LSTM with a CRF layer (BI-LSTM-CRF). Our work is the first to apply a bidirectional LSTM CRF (denoted as BI-LSTM-CRF) model to NLP benchmark sequence tagging data sets. We show that the BI-LSTM-CRF model can efficiently use both past and future input features thanks to a bidirectional LSTM component. It can also use sentence level tag information thanks to a CRF layer. The BI-LSTM-CRF model can produce state of the art (or close to) accuracy on POS, chunking and NER data sets. In addition, it is robust and has less dependence on word embedding as compared to previous observations.},
archivePrefix = {arXiv},
arxivId = {1508.01991},
author = {Huang, Zhiheng and Xu, Wei and Yu, Kai},
eprint = {1508.01991},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Huang, Xu, Yu - 2015 - Bidirectional LSTM-CRF Models for Sequence Tagging.pdf:pdf},
month = {aug},
title = {{Bidirectional LSTM-CRF Models for Sequence Tagging}},
url = {http://arxiv.org/abs/1508.01991},
year = {2015}
}
@inproceedings{McCann2017,
abstract = {Computer vision has benefited from initializing multiple deep layers with weights pretrained on large supervised training sets like ImageNet. Natural language processing (NLP) typically sees initialization of only the lowest layer of deep models with pretrained word vectors. In this paper, we use a deep LSTM encoder from an attentional sequence-to-sequence model trained for machine translation (MT) to contextualize word vectors. We show that adding these context vectors (CoVe) improves performance over using only unsupervised word and character vectors on a wide variety of common NLP tasks: sentiment analysis (SST, IMDb), question classification (TREC), entailment (SNLI), and question answering (SQuAD). For fine-grained sentiment analysis and entailment, CoVe improves performance of our baseline models to the state of the art.},
author = {McCann, Bryan and Bradbury, James and Xiong, Caiming and Socher, Richard},
booktitle = {Adv. Neural Inf. Process. Syst.},
file = {:Users/petravysusilova/Downloads/1708.00107.pdf:pdf},
issn = {10495258},
title = {{Learned in translation: Contextualized word vectors}},
volume = {2017-Decem},
year = {2017}
}
@article{Tibshirani1996,
abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
author = {Tibshirani, Robert},
doi = {10.1111/j.2517-6161.1996.tb02080.x},
issn = {0035-9246},
journal = {J. R. Stat. Soc. Ser. B},
keywords = {quadratic programming,regression,shrinkage,subset selection},
month = {jan},
number = {1},
pages = {267--288},
publisher = {Wiley},
title = {{Regression Shrinkage and Selection Via the Lasso}},
url = {https://rss.onlinelibrary.wiley.com/doi/full/10.1111/j.2517-6161.1996.tb02080.x https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1996.tb02080.x https://rss.onlinelibrary.wiley.com/doi/10.1111/j.2517-6161.1996.tb02080.x},
volume = {58},
year = {1996}
}
@inproceedings{Forcada1997,
abstract = {This paper presents a modification of Pollack's RAAM (Recursive Auto-Associative Memory), called a Recursive Hetero-Associative Memory (RHAM), and shows that it is capable of learning simple translation tasks, by building a state-space representation of each input string and unfolding it to obtain the corresponding output string. RHAM-based translators are computationally more powerful and easier to train than their corresponding double-RAAM counterparts in the literature.},
author = {Forcada, Mikel L. and {\~{N}}eco, Ram{\'{o}}n P.},
booktitle = {Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics)},
doi = {10.1007/bfb0032504},
isbn = {3540630473},
issn = {16113349},
keywords = {transformers},
mendeley-tags = {transformers},
pages = {453--462},
publisher = {Springer Verlag},
title = {{Recursive hetero-Associative memories for translation}},
url = {https://link.springer.com/chapter/10.1007/BFb0032504},
volume = {1240 LNCS},
year = {1997}
}
@techreport{Hutchins1996,
author = {Hutchins, John},
booktitle = {books.google.com},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Hutchins - 1996 - ALPAC the (in)famous report.pdf:pdf},
pages = {131--135},
publisher = {The MIT Press},
title = {{ALPAC: the (in)famous report}},
url = {https://books.google.com/books?hl=cs{\&}lr={\&}id=yx3lEVJMBmMC{\&}oi=fnd{\&}pg=PA131{\&}dq=alpac+report{\&}ots=se2vhONMHp{\&}sig=ByL2IgJLxRwF3f6n9bqOPFx88r4},
volume = {14},
year = {1996}
}
@inproceedings{Hewitt2020,
abstract = {Probes, supervised models trained to predict properties (like parts-of-speech) from representations (like ELMo), have achieved high accuracy on a range of linguistic tasks. But does this mean that the representations encode linguistic structure or just that the probe has learned the linguistic task? In this paper, we propose control tasks, which associate word types with random outputs, to complement linguistic tasks. By construction, these tasks can only be learned by the probe itself. So a good probe, (one that reflects the representation), should be selective, achieving high linguistic task accuracy and low control task accuracy. The selectivity of a probe puts linguistic task accuracy in context with the probe's capacity to memorize from word types. We construct control tasks for English part-of-speech tagging and dependency edge prediction, and show that popular probes on ELMo representations are not selective. We also find that dropout, commonly used to control probe complexity, is ineffective for improving selectivity of MLPs, but that other forms of regularization are effective. Finally, we find that while probes on the first layer of ELMo yield slightly better part-of-speech tagging accuracy than the second, probes on the second layer are substantially more selective, which raises the question of which layer better represents parts-of-speech.},
archivePrefix = {arXiv},
arxivId = {1909.03368},
author = {Hewitt, John and Liang, Percy},
booktitle = {EMNLP-IJCNLP 2019 - 2019 Conf. Empir. Methods Nat. Lang. Process. 9th Int. Jt. Conf. Nat. Lang. Process. Proc. Conf.},
doi = {10.18653/v1/d19-1275},
eprint = {1909.03368},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Hewitt, Liang - 2020 - Designing and interpreting probes with control tasks.pdf:pdf},
isbn = {9781950737901},
pages = {2733--2743},
publisher = {Association for Computational Linguistics},
title = {{Designing and interpreting probes with control tasks}},
year = {2020}
}
@article{Straka2018,
abstract = {UDPipe is a trainable pipeline which performs sentence segmentation, tokeniza-tion, POS tagging, lemmatization and dependency parsing (Straka et al., 2016). We present a prototype for UDPipe 2.0 and evaluate it in the CoNLL 2018 UD Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, which employs three metrics for submission ranking. Out of 26 participants, the prototype placed first in the MLAS ranking, third in the LAS ranking and third in the BLEX ranking. In extrinsic parser evaluation EPE 2018, the system ranked first in the overall score. The prototype utilizes an artificial neu-ral network with a single joint model for POS tagging, lemmatization and dependency parsing, and is trained only using the CoNLL-U training data and pretrained word embeddings, contrary to both systems surpassing the prototype in the LAS and BLEX ranking in the shared task. The open-source code of the prototype is available at http://github.com/ CoNLL-UD-2018/UDPipe-Future. After the shared task, we slightly refined the model architecture, resulting in better performance both in the intrinsic evaluation (corresponding to first, second and second rank in MLAS, LAS and BLEX shared task metrics) and the extrinsic evaluation. The improved models will be available shortly in UDPipe at},
author = {Straka, Milan},
doi = {10.18653/v1/K18-2020},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Straka - Unknown - UDPipe 2.0 Prototype at CoNLL 2018 UD Shared Task.pdf:pdf},
journal = {Proc. CoNLL 2018 Shar. Task Multiling. Parsing from Raw Text to Univers. Depend.},
keywords = {necitovane},
mendeley-tags = {necitovane},
pages = {197--207},
title = {{UDPipe 2.0 Prototype at CoNLL 2018 UD Shared Task}},
url = {http://ufal.mff.cuni.cz/udpipe.},
year = {2018}
}
@article{Straka2021,
abstract = {We present RobeCzech, a monolingual RoBERTa language representation model trained on Czech data. RoBERTa is a robustly optimized Transformer-based pretraining approach. We show that RobeCzech considerably outperforms equally-sized multilingual and Czech-trained contextualized language representation models, surpasses current state of the art in all five evaluated NLP tasks and reaches state-of-theart results in four of them. The RobeCzech model is released publicly at https://hdl.handle.net/11234/1-3691 and https://huggingface.co/ufal/robeczech-base.},
archivePrefix = {arXiv},
arxivId = {2105.11314},
author = {Straka, Milan and N{\'{a}}plava, Jakub and Strakov{\'{a}}, Jana and Samuel, David},
eprint = {2105.11314},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Straka et al. - 2021 - RobeCzech Czech RoBERTa, a monolingual contextualized language representation model.pdf:pdf},
keywords = {Czech,Czech {\textperiodcentered},RoBER,Robe,Ta,Ta {\textperiodcentered}},
month = {may},
title = {{RobeCzech: Czech RoBERTa, a monolingual contextualized language representation model}},
url = {http://arxiv.org/abs/2105.11314},
year = {2021}
}
@article{Ling,
abstract = {We introduce a model for constructing vector representations of words by composing characters using bidirectional LSTMs. Relative to traditional word representation models that have independent vectors for each word type, our model requires only a single vector per character type and a fixed set of parameters for the compositional model. Despite the compactness of this model and, more importantly, the arbitrary nature of the form-function relationship in language , our "composed" word representations yield state-of-the-art results in language modeling and part-of-speech tagging. Benefits over traditional baselines are particularly pronounced in morphologically rich languages (e.g., Turkish).},
archivePrefix = {arXiv},
arxivId = {1508.02096v2},
author = {Ling, Wang and Lu{\'{i}}s, Tiago and Marujo, Lu{\'{i}}s and Fernandez, Ram{\'{o}}n and Amir, Astudillo Silvio and Dyer, Chris and Black, Alan W and Trancoso, Isabel},
eprint = {1508.02096v2},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Ling et al. - Unknown - Finding Function in Form Compositional Character Models for Open Vocabulary Word Representation(2).pdf:pdf},
journal = {arXiv},
keywords = {embeddings},
mendeley-tags = {embeddings},
title = {{Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation}},
year = {2016}
}
@article{Straka2019a,
abstract = {We present an extensive evaluation of three recently proposed methods for contextualized embeddings on 89 corpora in 54 languages of the Universal Dependencies 2.3 in three tasks: POS tagging, lemmatization, and dependency parsing. Employing the BERT, Flair and ELMo as pretrained embedding inputs in a strong baseline of UDPipe 2.0, one of the best-performing systems of the CoNLL 2018 Shared Task and an overall winner of the EPE 2018, we present a one-to-one comparison of the three contextualized word embedding methods, as well as a comparison with word2vec-like pretrained embeddings and with end-to-end character-level word embeddings. We report state-of-the-art results in all three tasks as compared to results on UD 2.2 in the CoNLL 2018 Shared Task.},
archivePrefix = {arXiv},
arxivId = {1908.07448},
author = {Straka, Milan and Strakov{\'{a}}, Jana and Haji{\v{c}}, Jan},
eprint = {1908.07448},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Straka, Strakov{\'{a}}, Haji{\v{c}} - 2019 - Evaluating Contextualized Embeddings on 54 Languages in POS Tagging, Lemmatization and Dependency Par.pdf:pdf},
journal = {arXiv},
keywords = {necitovane},
mendeley-tags = {necitovane},
month = {aug},
title = {{Evaluating Contextualized Embeddings on 54 Languages in POS Tagging, Lemmatization and Dependency Parsing}},
url = {http://arxiv.org/abs/1908.07448},
year = {2019}
}
@article{Sun2019,
abstract = {Pre-trained language models such as BERT have proven to be highly effective for natural language processing (NLP) tasks. However, the high demand for computing resources in training such models hinders their application in practice. In order to alleviate this resource hunger in large-scale model training, we propose a Patient Knowledge Distillation approach to compress an original large model (teacher) into an equally-effective lightweight shallow network (student). Different from previous knowledge distillation methods, which only use the output from the last layer of the teacher network for distillation, our student model patiently learns from multiple intermediate layers of the teacher model for incremental knowledge extraction, following two strategies: ({\$}i{\$}) PKD-Last: learning from the last {\$}k{\$} layers; and ({\$}ii{\$}) PKD-Skip: learning from every {\$}k{\$} layers. These two patient distillation schemes enable the exploitation of rich information in the teacher's hidden layers, and encourage the student model to patiently learn from and imitate the teacher through a multi-layer distillation process. Empirically, this translates into improved results on multiple NLP tasks with significant gain in training efficiency, without sacrificing model accuracy.},
archivePrefix = {arXiv},
arxivId = {1908.09355},
author = {Sun, Siqi and Cheng, Yu and Gan, Zhe and Liu, Jingjing},
eprint = {1908.09355},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Sun et al. - 2019 - Patient Knowledge Distillation for BERT Model Compression.pdf:pdf},
journal = {EMNLP-IJCNLP 2019 - 2019 Conf. Empir. Methods Nat. Lang. Process. 9th Int. Jt. Conf. Nat. Lang. Process. Proc. Conf.},
month = {aug},
pages = {4323--4332},
publisher = {Association for Computational Linguistics},
title = {{Patient Knowledge Distillation for BERT Model Compression}},
url = {http://arxiv.org/abs/1908.09355},
year = {2019}
}
@article{Yang2020,
abstract = {Pre-trained language models, such as BERT, have achieved significant accuracy gain in many natural language processing tasks. Despite its effectiveness, the huge number of parameters makes training a BERT model computationally very challenging. In this paper, we propose an efficient multi-stage layerwise training (MSLT) approach to reduce the training time of BERT. We decompose the whole training process into several stages. The training is started from a small model with only a few encoder layers and we gradually increase the depth of the model by adding new encoder layers. At each stage, we only train the top (near the output layer) few encoder layers which are newly added. The parameters of the other layers which have been trained in the previous stages will not be updated in the current stage. In BERT training, the backward computation is much more time-consuming than the forward computation, especially in the distributed training setting in which the backward computation time further includes the communication time for gradient synchronization. In the proposed training strategy, only top few layers participate in backward computation, while most layers only participate in forward computation. Hence both the computation and communication efficiencies are greatly improved. Experimental results show that the proposed method can achieve more than 110{\%} training speedup without significant performance degradation.},
archivePrefix = {arXiv},
arxivId = {2011.13635},
author = {Yang, Cheng and Wang, Shengnan and Yang, Chao and Li, Yuechuan and He, Ru and Zhang, Jingqiao},
eprint = {2011.13635},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Yang et al. - 2020 - Progressively Stacking 2.0 A Multi-stage Layerwise Training Method for BERT Training Speedup.pdf:pdf},
journal = {arXiv},
month = {nov},
publisher = {arXiv},
title = {{Progressively Stacking 2.0: A Multi-stage Layerwise Training Method for BERT Training Speedup}},
url = {http://arxiv.org/abs/2011.13635},
year = {2020}
}
@techreport{Hajic-BarboraHladka,
author = {{Haji{\v{c}} -Barbora Hladk{\'{a}}}, Jan},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Haji{\v{c}} -Barbora Hladk{\'{a}} - Unknown - Morfologick{\'{e}} zna{\v{c}}kov{\'{a}}n{\'{i}} korpusu {\v{c}}esk{\'{y}}ch textů stochastickou metodou.pdf:pdf},
keywords = {necitovane},
mendeley-tags = {necitovane},
title = {{Morfologick{\'{e}} zna{\v{c}}kov{\'{a}}n{\'{i}} korpusu {\v{c}}esk{\'{y}}ch textů stochastickou metodou}}
}
@inproceedings{Peters2018,
abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-Trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
archivePrefix = {arXiv},
arxivId = {1802.05365},
author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
booktitle = {NAACL HLT 2018 - 2018 Conf. North Am. Chapter Assoc. Comput. Linguist. Hum. Lang. Technol. - Proc. Conf.},
doi = {10.18653/v1/n18-1202},
eprint = {1802.05365},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Peters et al. - 2018 - Deep contextualized word representations.pdf:pdf},
isbn = {9781948087278},
month = {feb},
pages = {2227--2237},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Deep contextualized word representations}},
url = {http://allennlp.org/elmo},
volume = {1},
year = {2018}
}
@inproceedings{Ruder2019,
author = {Ruder, Sebastian and Peters, Matthew E and Swayamdipta, Swabha and Wolf, Thomas},
booktitle = {Proc. 2019 Conf. North Am. Chapter Assoc. Comput. Linguist. Tutorials},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Ruder, Breslin, Ghaffari - 2019 - Neural Transfer Learning for Natural Language Processing.pdf:pdf},
pages = {15--18},
title = {{Neural Transfer Learning for Natural Language Processing}},
year = {2019}
}
@article{Lin2017,
abstract = {This paper proposes a new model for extracting an interpretable sentence
embedding by introducing self-attention. Instead of using a vector, we use a
2-D matrix to represent the embedding, with each row of the matrix attending on
a different part of the sentence. We also propose a self-attention mechanism
and a special regularization term for the model. As a side effect, the
embedding comes with an easy way of visualizing what specific parts of the
sentence are encoded into the embedding. We evaluate our model on 3 different
tasks: author profiling, sentiment classification, and textual entailment.
Results show that our model yields a significant performance gain compared to
other sentence embedding methods in all of the 3 tasks.},
archivePrefix = {arXiv},
arxivId = {1703.03130},
author = {Lin, Zhouhan and Feng, Minwei and dos Santos, Cicero Nogueira and Yu, Mo and Xiang, Bing and Zhou, Bowen and Bengio, Yoshua},
eprint = {1703.03130},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Lin et al. - 2017 - A Structured Self-attentive Sentence Embedding.pdf:pdf},
journal = {5th Int. Conf. Learn. Represent. ICLR 2017 - Conf. Track Proc.},
month = {mar},
publisher = {International Conference on Learning Representations, ICLR},
title = {{A Structured Self-attentive Sentence Embedding}},
url = {https://arxiv.org/abs/1703.03130v1},
year = {2017}
}
@article{Wu2016,
abstract = {Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units ("wordpieces") for both input and output. This method provides a good balance between the flexibility of "character"-delimited models and the efficiency of "word"-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60{\%} compared to Google's phrase-based production system.},
archivePrefix = {arXiv},
arxivId = {1609.08144},
author = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V. and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and Klingner, Jeff and Shah, Apurva and Johnson, Melvin and Liu, Xiaobing and Kaiser, {\L}ukasz and Gouws, Stephan and Kato, Yoshikiyo and Kudo, Taku and Kazawa, Hideto and Stevens, Keith and Kurian, George and Patil, Nishant and Wang, Wei and Young, Cliff and Smith, Jason and Riesa, Jason and Rudnick, Alex and Vinyals, Oriol and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
eprint = {1609.08144},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Wu et al. - 2016 - Google's Neural Machine Translation System Bridging the Gap between Human and Machine Translation.pdf:pdf},
journal = {arXiv},
keywords = {transformers},
mendeley-tags = {transformers},
month = {sep},
title = {{Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation}},
url = {http://arxiv.org/abs/1609.08144},
year = {2016}
}
@article{Huan2021,
author = {Huan, Liu and Zhixiong, Zhang and Yufei, Wang and Huan, Liu and Zhixiong, Zhang and Yufei, Wang},
doi = {10.11925/INFOTECH.2096-3467.2020.0965},
issn = {2096-3467},
journal = {Data Anal. Knowl. Discov.},
keywords = {BERT,Knowledge Integration,Model Compression,Pre-Training},
month = {feb},
number = {1},
pages = {3--15},
title = {{A Review on Main Optimization Methods of BERT}},
url = {http://manu44.magtech.com.cn/Jwk{\_}infotech{\_}wk3/EN/abstract/abstract4997.shtml},
volume = {5},
year = {2021}
}
@article{Akbik2018,
abstract = {Recent advances in language modeling using recurrent neural networks have made it viable to model language as distributions over characters. By learning to predict the next character on the basis of previous characters, such models have been shown to automatically internalize linguistic concepts such as words, sentences, subclauses and even sentiment. In this paper, we propose to leverage the internal states of a trained character language model to produce a novel type of word embedding which we refer to as contextual string embeddings. Our proposed embeddings have the distinct properties that they (a) are trained without any explicit notion of words and thus fundamentally model words as sequences of characters, and (b) are contextualized by their surrounding text, meaning that the same word will have different embeddings depending on its contextual use. We conduct a comparative evaluation against previous embeddings and find that our embeddings are highly useful for downstream tasks: across four classic sequence labeling tasks we consistently outperform the previous state-of-the-art. In particular, we significantly outperform previous work on English and German named entity recognition (NER), allowing us to report new state-of-the-art F1-scores on the CONLL03 shared task. We release all code and pre-trained language models in a simple-to-use framework to the research community, to enable reproduction of these experiments and application of our proposed embeddings to other tasks: https://github.com/zalandoresearch/flair},
author = {Akbik, Alan and Blythe, Duncan and Vollgraf, Roland},
file = {:Users/petravysusilova/Downloads/C18-1139.pdf:pdf},
journal = {Proc. 27th Int. Conf. Comput. Linguist.},
title = {{Contextual String Embeddings for Sequence Labeling}},
year = {2018}
}
@techreport{Santos2016,
abstract = {In this work, we propose Attentive Pooling (AP), a two-way attention mechanism for discriminative model training. In the context of pair-wise ranking or classification with neural networks, AP enables the pooling layer to be aware of the current input pair, in a way that information from the two input items can directly influence the computation of each other's representations. Along with such representations of the paired inputs, AP jointly learns a similarity measure over projected segments (e.g. trigrams) of the pair, and subsequently, derives the corresponding attention vector for each input to guide the pooling. Our two-way attention mechanism is a general framework independent of the underlying representation learning, and it has been applied to both convolutional neural networks (CNNs) and recurrent neural networks (RNNs) in our studies. The empirical results, from three very different benchmark tasks of question answering/answer selection, demonstrate that our proposed models outperform a variety of strong baselines and achieve state-of-the-art performance in all the benchmarks.},
archivePrefix = {arXiv},
arxivId = {1602.03609},
author = {dos Santos, Cicero and Tan, Ming and Xiang, Bing and Zhou, Bowen},
eprint = {1602.03609},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Santos et al. - 2016 - Attentive Pooling Networks.pdf:pdf},
month = {feb},
title = {{Attentive Pooling Networks}},
url = {http://arxiv.org/abs/1602.03609},
year = {2016}
}
@inproceedings{Horsmann,
abstract = {A recent study by Plank et al. (2016) found that LSTM-based PoS taggers considerably improve over the current state-of-the-art when evaluated on the corpora of the Universal Dependencies project that use a coarse-grained tagset. We replicate this study using a fresh collection of 27 corpora of 21 languages that are annotated with fine-grained tagsets of varying size. Our replication confirms the result in general , and we additionally find that the advantage of LSTMs is even bigger for larger tagsets. However, we also find that for the very large tagsets of morphologically rich languages, hand-crafted morphological lexicons are still necessary to reach state-of-the-art performance.},
author = {Horsmann, Tobias and Zesch, Torsten},
booktitle = {EMNLP 2017 - Conf. Empir. Methods Nat. Lang. Process. Proc.},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Horsmann, Zesch - Unknown - Do LSTMs really work so well for PoS tagging-A replication study.pdf:pdf},
keywords = {necitovane},
mendeley-tags = {necitovane},
pages = {727--736},
title = {{Do LSTMs really work so well for PoS tagging?-A replication study}},
year = {2017}
}
@techreport{Wilks,
abstract = {The article surveys fifty years of work in computational language processing and machine translation, and suggests that a great number of the important ideas were present in the earliest days and hampered only back lack of computational power. Sections review the influence of linguistics proper on the computational area, as well as the influence of artificial intelligence and concerns from logic and knowledge representation. Later, corpora and machine readable dictionaries were made available, which in turn made possible the recent statistically-based empirical emphasis in the subject, a trend that began in machine translation under the influence of success in automatic speech processing. Finally, it is suggested that, despite these many influences on the field from outside, there is nonetheless a distinctive process-based computational linguistics and examples are suggested. Keywords: machine translation information retrieval information extraction parsing thesaurus beliefs syntactic structures semantic representations logic statistics question answering summarization psychology word-sense part-of-speech-tagging sense and reference performance case grammar agents computational semantics},
author = {Wilks, Yorick},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Wilks - Unknown - The History of Natural Language Processing and Machine Translation(2).pdf:pdf},
title = {{The History of Natural Language Processing and Machine Translation}},
year = {2005}
}
@techreport{Strakova,
abstract = {We present two recently released open-source taggers: NameTag is a free software for named entity recognition (NER) which achieves state-of-the-art performance on Czech; MorphoDiTa (Morpho-logical Dictionary and Tagger) performs morphological analysis (with lemmatiza-tion), morphological generation, tagging and tokenization with state-of-the-art results for Czech and a throughput around 10-200K words per second. The taggers can be trained for any language for which annotated data exist, but they are specifically designed to be efficient for inflective languages, Both tools are free software under LGPL license and are distributed along with trained linguistic models which are free for non-commercial use under the CC BY-NC-SA license. The releases include standalone tools, C++ libraries with Java, Python and Perl bindings and web services.},
author = {Strakov{\'{a}}, Jana and Straka, Milan and Haji{\v{c}}, Jan},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Strakov{\'{a}}, Straka, Haji{\v{c}} - Unknown - Open-Source Tools for Morphology, Lemmatization, POS Tagging and Named Entity Recognition.pdf:pdf},
pages = {13--18},
title = {{Open-Source Tools for Morphology, Lemmatization, POS Tagging and Named Entity Recognition}},
url = {http://ufal.mff.cuni.cz/morce/index.php}
}
