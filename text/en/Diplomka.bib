Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Marcus1993,
author = {Marcus, Mitchell and Santorini, Beatrice and Marcinkiewicz, Mary Ann},
journal = {Tech. Reports},
month = {oct},
title = {{Building a Large Annotated Corpus of English: The Penn Treebank}},
url = {https://repository.upenn.edu/cis{\_}reports/237},
year = {1993}
}
@techreport{Schwenk2006,
abstract = {Statistical machine translation systems are based on one or more translation models and a language model of the target language. While many different translation models and phrase extraction algorithms have been proposed, a standard word n-gram back-off language model is used in most systems. In this work, we propose to use a new statistical language model that is based on a continuous representation of the words in the vocabulary. A neural network is used to perform the projection and the probability estimation. We consider the translation of European Parliament Speeches. This task is part of an international evaluation organized by the TC-STAR project in 2006. The proposed method achieves consistent improvements in the BLEU score on the development and test data. We also present algorithms to improve the estimation of the language model probabilities when splitting long sentences into shorter chunks.},
author = {Schwenk, Holger and Dchelotte, Daniel and Gauvain, Jean-Luc},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Schwenk, Dchelotte, Gauvain - 2006 - Continuous Space Language Models for Statistical Machine Translation.pdf:pdf},
keywords = {modeling},
mendeley-tags = {modeling},
pages = {723--730},
title = {{Continuous Space Language Models for Statistical Machine Translation}},
year = {2006}
}
@techreport{Cheng,
abstract = {In this paper we address the question of how to render sequence-level networks better at handling structured input. We propose a machine reading simulator which processes text incrementally from left to right and performs shallow reasoning with memory and attention. The reader extends the Long Short-Term Memory architecture with a memory network in place of a single memory cell. This enables adaptive memory usage during recurrence with neural attention, offering a way to weakly induce relations among tokens. The system is initially designed to process a single sequence but we also demonstrate how to integrate it with an encoder-decoder architecture. Experiments on language modeling, sentiment analysis, and natural language inference show that our model matches or outperforms the state of the art.},
archivePrefix = {arXiv},
arxivId = {1601.06733v7},
author = {Cheng, Jianpeng and Dong, Li and Lapata, Mirella},
booktitle = {arxiv.org},
eprint = {1601.06733v7},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Cheng, Dong, Lapata - Unknown - Long Short-Term Memory-Networks for Machine Reading.pdf:pdf},
title = {{Long Short-Term Memory-Networks for Machine Reading}},
url = {https://arxiv.org/abs/1601.06733},
year = {2016}
}
@inproceedings{Ruder2019,
author = {Ruder, Sebastian and Peters, Matthew E and Swayamdipta, Swabha and Wolf, Thomas},
booktitle = {Proc. 2019 Conf. North Am. Chapter Assoc. Comput. Linguist. Tutorials},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Ruder, Breslin, Ghaffari - 2019 - Neural Transfer Learning for Natural Language Processing.pdf:pdf},
pages = {15--18},
title = {{Neural Transfer Learning for Natural Language Processing}},
year = {2019}
}
@techreport{Libovicky,
abstract = {In this work, we focus on three different NLP tasks: image captioning, machine translation, and sentiment analysis. We reimplement successful approaches of other authors and adapt them to the Czech language. We provide end-to-end architectures that achieve state-of-the-art or nearly state-of-the-art results on all of the tasks within a single sequence learning toolkit. The trained models are available both for download as well as in an online demo. 1 End-to-End Training Traditionally, solving tasks such as machine translation or sentiment analysis required complex processing pipelines consisting of tools which transformed one explicit representation of the data into another, with the structure of the internal representations defined by the system designer. In machine translation [24, 6], we would devise explicit word alignment links, extract phrase tables, train a language model, etc.; in sentiment analysis [32, 43], we could label the data with part-of-speech tags, decode their syntactic structure, and/or assign them with semantic labels. All of these more-or-less linguistically motivated internal representations are not inherently required to produce the desired output, but have been devised as clever and useful ways to break down the large and hard task into smaller and manageable substeps. With the advent of end-to-end training of deep neural networks (DNN) [26, 14, 28], the need for most of this has been eliminated. In the end-to-end learning paradigm, there is only one model, directly trained to produce the desired outputs from the inputs, without any explicit intermediate representations. The system designer now only has to design a rather generic architecture of the system. It mostly does not enforce any complex explicit representations and processing steps, but rather offers opportunities for the DNN to devise its own notion of intermediate representations and processing steps through training. This also means that similar architectures can be used to solve very different tasks. Rather than by the nature of the task itself, the structure of the DNN to use is mostly determined by the structure of the input and output -e.g. image inputs are processed by two-dimensional convolutions [27], while text inputs are processed by one-dimensional convolutions, recurrent units [38], and/or attentions [3], typically applied to word or subword embed-dings [5, 10, 33]; classification can produce its output in one step, while text generation is better done iteratively using recurrent decoders; etc. Thanks to that, a single general framework can be used to solve many different tasks. One just needs to transform the inputs and outputs into a suitable format, define an adequate network structure, and let the system train for a few days or weeks. Sadly, the burden of hyperparameter tuning has not been alleviated by DNNs, but rather made worse by the computational costliness of the training. However, with a bit of experience, one is often able to propose a suitable architecture and hyperparameter values at the first attempt, already achieving very competitive results even without any further tuning.},
author = {Libovick{\'{y}}, Jindřich and Rosa, Rudolf and Helcl, Jindřich and Popel, Martin},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Libovick{\'{y}} et al. - Unknown - Solving Three Czech NLP Tasks End-to-End with Neural Models.pdf:pdf},
isbn = {11234/12839},
keywords = {necitovane},
mendeley-tags = {necitovane},
title = {{Solving Three Czech NLP Tasks End-to-End with Neural Models}},
url = {https://www.yelp.com/dataset/}
}
@article{Liu2020,
abstract = {Contextual embeddings, such as ELMo and BERT, move beyond global word representations like Word2Vec and achieve ground-breaking performance on a wide range of natural language processing tasks. Contextual embeddings assign each word a representation based on its context, thereby capturing uses of words across varied contexts and encoding knowledge that transfers across languages. In this survey, we review existing contextual embedding models, cross-lingual polyglot pre-training, the application of contextual embeddings in downstream tasks, model compression, and model analyses.},
archivePrefix = {arXiv},
arxivId = {2003.07278},
author = {Liu, Qi and Kusner, Matt J. and Blunsom, Phil},
eprint = {2003.07278},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Liu, Kusner, Blunsom - 2020 - A Survey on Contextual Embeddings.pdf:pdf},
journal = {arXiv},
month = {mar},
publisher = {arXiv},
title = {{A Survey on Contextual Embeddings}},
url = {http://arxiv.org/abs/2003.07278},
year = {2020}
}
@article{Straka2018,
abstract = {UDPipe is a trainable pipeline which performs sentence segmentation, tokeniza-tion, POS tagging, lemmatization and dependency parsing (Straka et al., 2016). We present a prototype for UDPipe 2.0 and evaluate it in the CoNLL 2018 UD Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, which employs three metrics for submission ranking. Out of 26 participants, the prototype placed first in the MLAS ranking, third in the LAS ranking and third in the BLEX ranking. In extrinsic parser evaluation EPE 2018, the system ranked first in the overall score. The prototype utilizes an artificial neu-ral network with a single joint model for POS tagging, lemmatization and dependency parsing, and is trained only using the CoNLL-U training data and pretrained word embeddings, contrary to both systems surpassing the prototype in the LAS and BLEX ranking in the shared task. The open-source code of the prototype is available at http://github.com/ CoNLL-UD-2018/UDPipe-Future. After the shared task, we slightly refined the model architecture, resulting in better performance both in the intrinsic evaluation (corresponding to first, second and second rank in MLAS, LAS and BLEX shared task metrics) and the extrinsic evaluation. The improved models will be available shortly in UDPipe at},
author = {Straka, Milan},
doi = {10.18653/v1/K18-2020},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Straka - Unknown - UDPipe 2.0 Prototype at CoNLL 2018 UD Shared Task.pdf:pdf},
journal = {Proc. CoNLL 2018 Shar. Task Multiling. Parsing from Raw Text to Univers. Depend.},
keywords = {necitovane},
mendeley-tags = {necitovane},
pages = {197--207},
title = {{UDPipe 2.0 Prototype at CoNLL 2018 UD Shared Task}},
url = {http://ufal.mff.cuni.cz/udpipe.},
year = {2018}
}
@techreport{Hochreiter1997,
abstract = {Learning to store information over extended time intervals via recurrent backpropagation takes a very long time, mostly due to insuucient, decaying error back ow. We brieey review Hochreiter's 1991 analysis of this problem, then address it by introducing a novel, eecient, gradient-based method called $\backslash$Long Short-Term Memory" (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete time steps by enforcing constant error ow through $\backslash$constant error carrousels" within special units. Multiplicative gate units learn to open and close access to the constant error ow. LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artiicial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with RTRL, BPTT, Recurrent Cascade-Correlation, Elman nets, and Neural Sequence Chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artiicial long time lag tasks that have never been solved by previous recurrent network algorithms.},
author = {Hochreiter, Sepp and {Urgen Schmidhuber}, J J},
booktitle = {Mem. Neural Comput.},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Hochreiter, Urgen Schmidhuber - 1997 - Long short-term memory.pdf:pdf},
number = {8},
pages = {1735--1780},
title = {{Long short-term memory}},
url = {http://www7.informatik.tu-muenchen.de/{~}hochreithttp://www.idsia.ch/{~}juergen},
volume = {9},
year = {1997}
}
@article{Sutskever2014,
abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
archivePrefix = {arXiv},
arxivId = {1409.3215},
author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
eprint = {1409.3215},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Sutskever, Vinyals, Le - 2014 - Sequence to Sequence Learning with Neural Networks.pdf:pdf},
journal = {Adv. Neural Inf. Process. Syst.},
keywords = {transformers},
mendeley-tags = {transformers},
month = {sep},
number = {January},
pages = {3104--3112},
publisher = {Neural information processing systems foundation},
title = {{Sequence to Sequence Learning with Neural Networks}},
url = {http://arxiv.org/abs/1409.3215},
volume = {4},
year = {2014}
}
@article{Vaswani2017,
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
author = {Vaswani, Ashish and Brain, Google and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Vaswani et al. - Unknown - Attention Is All You Need.pdf:pdf},
title = {{Attention Is All You Need}},
year = {2017}
}
@techreport{Plisson,
abstract = {Lemmatization is the process of finding the normalized form of a word. It is the same as looking for a transformation to apply on a word to get its normalized form. The approach presented in this paper focuses on word endings: what word suffix should be removed and/or added to get the normalized form. This paper compares the results of two word lemmatization algorithms, one based on if-then rules and the other based on ripple down rules induction algorithms. It presents the problem of lemmatization of words from Slovene free text and explains why the Ripple Down Rules (RDR) approach is very well suited for the task. When learning from a corpus of lemmatized Slovene words the RDR approach results in easy to understand rules of improved classification accuracy compared to the results of rule learning achieved in previous work.},
author = {Plisson, Jo{\"{e}}l and Lavrac, Nada and Mladenic, Dunja},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Plisson, Lavrac, Mladenic - Unknown - A Rule based Approach to Word Lemmatization.pdf:pdf},
keywords = {lemmatization,necitovane},
mendeley-tags = {lemmatization,necitovane},
title = {{A Rule based Approach to Word Lemmatization}}
}
@techreport{Hladka,
author = {Hladk{\'{a}}},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - Part of Speech Tags for Automatic Tagging and Syntactic Structures 1.pdf:pdf},
title = {{Part of Speech Tags for Automatic Tagging and Syntactic Structures 1}}
}
@article{Wang2015,
abstract = {Bidirectional Long Short-Term Memory Recurrent Neural Network (BLSTM-RNN) has been shown to be very effective for tagging sequential data, e.g. speech utterances or handwritten documents. While word embedding has been demoed as a powerful representation for characterizing the statistical properties of natural language. In this study, we propose to use BLSTM-RNN with word embedding for part-of-speech (POS) tagging task. When tested on Penn Treebank WSJ test set, a state-of-the-art performance of 97.40 tagging accuracy is achieved. Without using morphological features, this approach can also achieve a good performance comparable with the Stanford POS tagger.},
archivePrefix = {arXiv},
arxivId = {1510.06168},
author = {Wang, Peilu and Qian, Yao and Soong, Frank K. and He, Lei and Zhao, Hai},
eprint = {1510.06168},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Wang et al. - 2015 - Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Recurrent Neural Network.pdf:pdf},
keywords = {(),necitovane},
mendeley-tags = {necitovane},
month = {oct},
title = {{Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Recurrent Neural Network}},
url = {http://arxiv.org/abs/1510.06168},
year = {2015}
}
@techreport{Rumelhart,
author = {Rumelhart, DE and Hinton, GE and Nature, RJ Williams - and 1986, Undefined},
booktitle = {nature.com},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Rumelhart et al. - Unknown - Learning representations by back-propagating errors.pdf:pdf},
title = {{Learning representations by back-propagating errors}},
url = {https://www.nature.com/articles/323533a0},
year = {1986}
}
@misc{Farber1969,
author = {Farber, Steven M.},
booktitle = {IEEE Trans. Inf. Theory},
doi = {10.1109/TIT.1969.1054388},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Farber - 1969 - Book Reviews.pdf:pdf},
issn = {15579654},
keywords = {necitovane},
mendeley-tags = {necitovane},
number = {6},
pages = {738--739},
title = {{Book Reviews}},
volume = {15},
year = {1969}
}
@techreport{Pennington,
abstract = {Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic , but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global log-bilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word co-occurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful sub-structure, as evidenced by its performance of 75{\%} on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.},
author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Pennington, Socher, Manning - Unknown - GloVe Global Vectors for Word Representation.pdf:pdf},
title = {{GloVe: Global Vectors for Word Representation}},
url = {http://nlp.},
year = {2014}
}
@techreport{Allen19,
abstract = {Recent developments in neural algorithms provide a new approach to natural language processing. Two sets of brief studies show how networks may be developed for processing simple demonstratives and analogies. Two longer studies consider pronoun reference and natural language translation. Taken together, the studies provide additional support for the applicability of these algorithms to natural language processing.},
author = {Allen, Robert B},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Allen - Unknown - Several Studies on Natural Language {\textperiodcentered} and Back-Propagation.pdf:pdf},
keywords = {transformers},
mendeley-tags = {transformers},
title = {{Several Studies on Natural Language {\textperiodcentered} and Back-Propagation}},
year = {1987}
}
@article{Yang2019,
abstract = {With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.},
archivePrefix = {arXiv},
arxivId = {1906.08237},
author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V.},
eprint = {1906.08237},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Yang et al. - 2019 - XLNet Generalized Autoregressive Pretraining for Language Understanding.pdf:pdf},
journal = {arXiv},
month = {jun},
publisher = {arXiv},
title = {{XLNet: Generalized Autoregressive Pretraining for Language Understanding}},
url = {http://arxiv.org/abs/1906.08237},
year = {2019}
}
@techreport{Horsmann,
abstract = {A recent study by Plank et al. (2016) found that LSTM-based PoS taggers considerably improve over the current state-of-the-art when evaluated on the corpora of the Universal Dependencies project that use a coarse-grained tagset. We replicate this study using a fresh collection of 27 corpora of 21 languages that are annotated with fine-grained tagsets of varying size. Our replication confirms the result in general , and we additionally find that the advantage of LSTMs is even bigger for larger tagsets. However, we also find that for the very large tagsets of morphologically rich languages, hand-crafted morphological lexicons are still necessary to reach state-of-the-art performance.},
author = {Horsmann, Tobias and Zesch, Torsten},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Horsmann, Zesch - Unknown - Do LSTMs really work so well for PoS tagging-A replication study.pdf:pdf},
keywords = {necitovane},
mendeley-tags = {necitovane},
pages = {727--736},
title = {{Do LSTMs really work so well for PoS tagging?-A replication study}}
}
@article{Clark2020,
abstract = {Masked language modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK] and then train a model to reconstruct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the original identities of the corrupted tokens, we train a discriminative model that predicts whether each token in the corrupted input was replaced by a generator sample or not. Thorough experiments demonstrate this new pre-training task is more efficient than MLM because the task is defined over all input tokens rather than just the small subset that was masked out. As a result, the contextual representations learned by our approach substantially outperform the ones learned by BERT given the same model size, data, and compute. The gains are particularly strong for small models; for example, we train a model on one GPU for 4 days that outperforms GPT (trained using 30x more compute) on the GLUE natural language understanding benchmark. Our approach also works well at scale, where it performs comparably to RoBERTa and XLNet while using less than 1/4 of their compute and outperforms them when using the same amount of compute.},
archivePrefix = {arXiv},
arxivId = {2003.10555},
author = {Clark, Kevin and Luong, Minh-Thang and Le, Quoc V. and Manning, Christopher D.},
eprint = {2003.10555},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Clark et al. - 2020 - ELECTRA Pre-training Text Encoders as Discriminators Rather Than Generators.pdf:pdf},
keywords = {necitovane},
mendeley-tags = {necitovane},
month = {mar},
title = {{ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators}},
url = {http://arxiv.org/abs/2003.10555},
year = {2020}
}
@article{Rosenblatt1958,
abstract = {To answer the questions of how information about the physical world is sensed, in what form is information remembered, and how does information retained in memory influence recognition and behavior, a theory is developed for a hypothetical nervous system called a perceptron. The theory serves as a bridge between biophysics and psychology. It is possible to predict learning curves from neurological variables and vice versa. The quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems. 18 references. (PsycINFO Database Record (c) 2006 APA, all rights reserved). {\textcopyright} 1958 American Psychological Association.},
author = {Rosenblatt, F.},
doi = {10.1037/h0042519},
issn = {0033295X},
journal = {Psychol. Rev.},
keywords = {PERCEPTION, AS INFORMATION STORAGE MODEL INFORMATI},
month = {nov},
number = {6},
pages = {386--408},
pmid = {13602029},
title = {{The perceptron: A probabilistic model for information storage and organization in the brain}},
volume = {65},
year = {1958}
}
@techreport{Hajicova2000,
author = {Haji{\v{c}}ov{\'{a}}, Eva and Panevov{\'{a}}, Jarmila and Sgall, Petr and Ceplov{\'{a}}, M and {Řezn{\'{i}}{\v{c}}kov{\'{a}} Translated by Kirschner}, V Z and Haji{\v{c}}ov{\'{a}}, E and Sgall, P},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Haji{\v{c}}ov{\'{a}} et al. - 2000 - A MANUAL FOR TECTOGRAMMATICAL TAGGING OF THE PRAGUE DEPENDENCY TREEBANK In cooperation with A.pdf:pdf},
keywords = {necitovane},
mendeley-tags = {necitovane},
title = {{A MANUAL FOR TECTOGRAMMATICAL TAGGING OF THE PRAGUE DEPENDENCY TREEBANK In cooperation with A}},
year = {2000}
}
@inproceedings{Devlin2019,
abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5{\%} (7.7{\%} point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
archivePrefix = {arXiv},
arxivId = {1810.04805},
author = {Devlin, Jacob and Chang, Ming Wei and Lee, Kenton and Toutanova, Kristina},
booktitle = {NAACL HLT 2019 - 2019 Conf. North Am. Chapter Assoc. Comput. Linguist. Hum. Lang. Technol. - Proc. Conf.},
eprint = {1810.04805},
file = {:Users/petravysusilova/Downloads/1810.04805.pdf:pdf},
isbn = {9781950737130},
pages = {4171--4186},
publisher = {Association for Computational Linguistics (ACL)},
title = {{BERT: Pre-training of deep bidirectional transformers for language understanding}},
volume = {1},
year = {2019}
}
@inproceedings{Arkhipov2019,
abstract = {Our paper addresses the problem of multilingual named entity recognition on the material of 4 languages: Russian, Bulgarian, Czech and Polish. We solve this task using the BERT model. We use a hundred languages multilingual model as base for transfer to the mentioned Slavic languages. Unsupervised pre-training of the BERT model on these 4 languages allows to significantly outperform baseline neural approaches and multilingual BERT. Additional improvement is achieved by extending BERT with a word-level CRF layer. Our system was submitted to BSNLP 2019 Shared Task on Multilingual Named Entity Recognition and took the 1st place in 3 competition metrics out of 4 we participated in. We open-sourced NER models and BERT model pre-trained on the four Slavic languages.},
author = {Arkhipov, Mikhail and Trofimova, Maria and Kuratov, Yuri and Sorokin, Alexey},
doi = {10.18653/v1/w19-3712},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Arkhipov et al. - 2019 - Tuning Multilingual Transformers for Language-Specific Named Entity Recognition.pdf:pdf},
keywords = {necitovane},
mendeley-tags = {necitovane},
month = {sep},
pages = {89--93},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Tuning Multilingual Transformers for Language-Specific Named Entity Recognition}},
url = {https://github.com/google-research/},
year = {2019}
}
@techreport{Putra,
abstract = {Compared to English, the amount of labeled data for Indonesian text classification tasks is very small. Recently developed multilingual language models have shown its ability to create multilingual representations effectively. This paper investigates the effect of combining English and Indonesian data on building Indonesian text classification (e.g., sentiment analysis and hate speech) using multilingual language models. Using the feature-based approach, we observe its performance on various data sizes and total added English data. The experiment showed that the addition of English data, especially if the amount of Indonesian data is small, improves performance. Using the fine-tuning approach, we further showed its effectiveness in utilizing the English language to build Indonesian text classification models.},
author = {Putra, Ilham Firdausi and Purwarianti, Ayu and Ai-Vlb, U-Coe},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Putra, Purwarianti, Ai-Vlb - Unknown - Improving Indonesian Text Classification Using Multilingual Language Model.pdf:pdf},
keywords = {Indonesian text,hate speech classification,multilingual language model,necitovane,practise,sentiment analysis,text classification},
mendeley-tags = {necitovane,practise},
title = {{Improving Indonesian Text Classification Using Multilingual Language Model}},
url = {https://www.yelp.com/dataset}
}
@techreport{Wilks,
abstract = {The article surveys fifty years of work in computational language processing and machine translation, and suggests that a great number of the important ideas were present in the earliest days and hampered only back lack of computational power. Sections review the influence of linguistics proper on the computational area, as well as the influence of artificial intelligence and concerns from logic and knowledge representation. Later, corpora and machine readable dictionaries were made available, which in turn made possible the recent statistically-based empirical emphasis in the subject, a trend that began in machine translation under the influence of success in automatic speech processing. Finally, it is suggested that, despite these many influences on the field from outside, there is nonetheless a distinctive process-based computational linguistics and examples are suggested. Keywords: machine translation information retrieval information extraction parsing thesaurus beliefs syntactic structures semantic representations logic statistics question answering summarization psychology word-sense part-of-speech-tagging sense and reference performance case grammar agents computational semantics},
author = {Wilks, Yorick},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Wilks - Unknown - The History of Natural Language Processing and Machine Translation(2).pdf:pdf},
title = {{The History of Natural Language Processing and Machine Translation}},
year = {2005}
}
@techreport{Brown,
abstract = {In this paper, we present a statistical approach to machine translation. We describe the application of our approach to translation from French to English and give preliminary results.},
author = {Brown, Peter F and Cocke, John and {Della Pietra}, Stephen A and {Della Pietra}, Vincent J and Jelinek, Fredrick and Lafferty, John D and Mercer, Robert L and Roossin, Paul S},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Brown et al. - Unknown - A STATISTICAL APPROACH TO MACHINE TRANSLATION.pdf:pdf},
title = {{A STATISTICAL APPROACH TO MACHINE TRANSLATION}},
year = {1990}
}
@article{Wu2020,
abstract = {Multilingual BERT (mBERT) trained on 104 languages has shown surprisingly good cross-lingual performance on several NLP tasks, even without explicit cross-lingual signals. However, these evaluations have focused on cross-lingual transfer with high-resource languages, covering only a third of the languages covered by mBERT. We explore how mBERT performs on a much wider set of languages, focusing on the quality of representation for low-resource languages, measured by within-language performance. We consider three tasks: Named Entity Recognition (99 languages), Part-of-speech Tagging, and Dependency Parsing (54 languages each). mBERT does better than or comparable to baselines on high resource languages but does much worse for low resource languages. Furthermore, monolingual BERT models for these languages do even worse. Paired with similar languages, the performance gap between monolingual BERT and mBERT can be narrowed. We find that better models for low resource languages require more efficient pretraining techniques or more data.},
archivePrefix = {arXiv},
arxivId = {2005.09093},
author = {Wu, Shijie and Dredze, Mark},
eprint = {2005.09093},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Wu, Dredze - 2020 - Are All Languages Created Equal in Multilingual BERT.pdf:pdf},
keywords = {bertology,necitovane},
mendeley-tags = {bertology,necitovane},
month = {may},
pages = {120--130},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Are All Languages Created Equal in Multilingual BERT?}},
url = {http://arxiv.org/abs/2005.09093},
year = {2020}
}
@article{Straka2019a,
abstract = {We present an extensive evaluation of three recently proposed methods for contextualized embeddings on 89 corpora in 54 languages of the Universal Dependencies 2.3 in three tasks: POS tagging, lemmatization, and dependency parsing. Employing the BERT, Flair and ELMo as pretrained embedding inputs in a strong baseline of UDPipe 2.0, one of the best-performing systems of the CoNLL 2018 Shared Task and an overall winner of the EPE 2018, we present a one-to-one comparison of the three contextualized word embedding methods, as well as a comparison with word2vec-like pretrained embeddings and with end-to-end character-level word embeddings. We report state-of-the-art results in all three tasks as compared to results on UD 2.2 in the CoNLL 2018 Shared Task.},
archivePrefix = {arXiv},
arxivId = {1908.07448},
author = {Straka, Milan and Strakov{\'{a}}, Jana and Haji{\v{c}}, Jan},
eprint = {1908.07448},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Straka, Strakov{\'{a}}, Haji{\v{c}} - 2019 - Evaluating Contextualized Embeddings on 54 Languages in POS Tagging, Lemmatization and Dependency Par.pdf:pdf},
keywords = {necitovane},
mendeley-tags = {necitovane},
month = {aug},
title = {{Evaluating Contextualized Embeddings on 54 Languages in POS Tagging, Lemmatization and Dependency Parsing}},
url = {http://arxiv.org/abs/1908.07448},
year = {2019}
}
@inproceedings{Montoyo2012,
abstract = {In this introduction, we present an overview of the current state of research in the Natural Language Processing tasks of subjectivity and sentiment analysis, as well as their application domains and closely-related research field of emotion detection. Although many definitions exist for these tasks and the research done within their frame spans over approaches with different objectives, we consider subjectivity analysis to deal with the detection of "private states" (opinions, emotions, sentiments, beliefs, speculations) and sentiment analysis as the task of detecting, extracting and classifying opinions and sentiments concerning different topics, as expressed in textual input. After describing the key concepts and research directions in these tasks, we present the main achievements obtained so far and the issues that remain to be tackled. Subsequently, we introduce each of the papers in this volume and present their contribution to the research areas of subjectivity and sentiment analysis. Finally, we conclude on the present state of work in these fields and reflect on the possible future developments. {\textcopyright} 2012 Elsevier B.V. All rights reserved.},
author = {Montoyo, Andr{\'{e}}s and Mart{\'{i}}nez-Barco, Patricio and Balahur, Alexandra},
booktitle = {Decis. Support Syst.},
doi = {10.1016/j.dss.2012.05.022},
issn = {01679236},
keywords = {Emotion detection,Opinion mining,Sentiment analysis,Social media mining,Social network mining,Subjectivity analysis,Text mining},
month = {nov},
number = {4},
pages = {675--679},
title = {{Subjectivity and sentiment analysis: An overview of the current state of the area and envisaged developments}},
volume = {53},
year = {2012}
}
@techreport{Szegedy2015,
abstract = {We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the Im-ageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular in-carnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
booktitle = {cv-foundation.org},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Szegedy et al. - Unknown - Going Deeper with Convolutions.pdf:pdf},
keywords = {labelsmoothing},
mendeley-tags = {labelsmoothing},
title = {{Going Deeper with Convolutions}},
url = {https://www.cv-foundation.org/openaccess/content{\_}cvpr{\_}2015/html/Szegedy{\_}Going{\_}Deeper{\_}With{\_}2015{\_}CVPR{\_}paper.html},
year = {2015}
}
@article{Huang2015,
abstract = {In this paper, we propose a variety of Long Short-Term Memory (LSTM) based models for sequence tagging. These models include LSTM networks, bidirectional LSTM (BI-LSTM) networks, LSTM with a Conditional Random Field (CRF) layer (LSTM-CRF) and bidirectional LSTM with a CRF layer (BI-LSTM-CRF). Our work is the first to apply a bidirectional LSTM CRF (denoted as BI-LSTM-CRF) model to NLP benchmark sequence tagging data sets. We show that the BI-LSTM-CRF model can efficiently use both past and future input features thanks to a bidirectional LSTM component. It can also use sentence level tag information thanks to a CRF layer. The BI-LSTM-CRF model can produce state of the art (or close to) accuracy on POS, chunking and NER data sets. In addition, it is robust and has less dependence on word embedding as compared to previous observations.},
archivePrefix = {arXiv},
arxivId = {1508.01991},
author = {Huang, Zhiheng and Xu, Wei and Yu, Kai},
eprint = {1508.01991},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Huang, Xu, Yu - 2015 - Bidirectional LSTM-CRF Models for Sequence Tagging.pdf:pdf},
month = {aug},
title = {{Bidirectional LSTM-CRF Models for Sequence Tagging}},
url = {http://arxiv.org/abs/1508.01991},
year = {2015}
}
@techreport{Hana2005,
author = {Hana, Jiř{\'{i}} and Zeman, Daniel},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Hana, Zeman - 2005 - Manual for Morphological Annotation Revision for the Prague Dependency Treebank 2.0 ´ UFAL.pdf:pdf},
title = {{Manual for Morphological Annotation Revision for the Prague Dependency Treebank 2.0 UFAL}},
year = {2005}
}
@inproceedings{Toutanova2003,
abstract = {We present a new part-of-speech tagger that demonstrates the following ideas: (i) explicit use of both preceding and following tag contexts via a dependency network representation, (ii) broad use of lexical features, including jointly conditioning on multiple consecutive words, (iii) effective use of priors in conditional loglinear models, and (iv) ﬁne-grained modeling of unknown word features. Using these ideas together, the resulting tagger gives a 97.24{\%} accuracy on the Penn Treebank WSJ, an error reduction of 4.4{\%} on the best previous single automatically learned tagging result},
address = {Morristown, NJ, USA},
author = {Toutanova, Kristina and Klein, Dan and Manning, Christopher D. and Singer, Yoram},
booktitle = {Proc. 2003 Conf. North Am. Chapter Assoc. Comput. Linguist. Hum. Lang. Technol.  - NAACL '03},
doi = {10.3115/1073445.1073478},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Toutanova et al. - 2003 - Feature-rich part-of-speech tagging with a cyclic dependency network.pdf:pdf},
keywords = {necitovane},
mendeley-tags = {necitovane},
pages = {173--180},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Feature-rich part-of-speech tagging with a cyclic dependency network}},
url = {http://portal.acm.org/citation.cfm?doid=1073445.1073478},
volume = {1},
year = {2003}
}
@article{Taylor1953,
abstract = {{\textless}p{\textgreater}Here is the first comprehensive statement of a research method and its theory which were introduced briefly during a workshop at the 1953 AEJ convention. Included are findings from three pilot studies and two experiments in which “cloze procedure” results are compared with those of two readability formulas.{\textless}/p{\textgreater}},
author = {Taylor, Wilson L.},
doi = {10.1177/107769905303000401},
issn = {0022-5533},
journal = {Journal. Q.},
month = {sep},
number = {4},
pages = {415--433},
publisher = {SAGE Publications},
title = {{“Cloze Procedure”: A New Tool for Measuring Readability}},
url = {http://journals.sagepub.com/doi/10.1177/107769905303000401},
volume = {30},
year = {1953}
}
@article{Cano2019,
abstract = {In the area of online communication, commerce and transactions, analyzing sentiment polarity of texts written in various natural languages has become crucial. While there have been a lot of contributions in resources and studies for the English language, "smaller" languages like Czech have not received much attention. In this survey, we explore the effectiveness of many existing machine learning algorithms for sentiment analysis of Czech Facebook posts and product reviews. We report the sets of optimal parameter values for each algorithm and the scores in both datasets. We finally observe that support vector machines are the best classifier and efforts to increase performance even more with bagging, boosting or voting ensemble schemes fail to do so.},
archivePrefix = {arXiv},
arxivId = {1901.02780},
author = {{\c{C}}ano, Erion and Bojar, Ondřej},
doi = {10.5220/0007695709730979},
eprint = {1901.02780},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/{\c{C}}ano, Bojar - 2019 - Sentiment Analysis of Czech Texts An Algorithmic Survey.pdf:pdf},
journal = {ICAART 2019 - Proc. 11th Int. Conf. Agents Artif. Intell.},
keywords = {Algorithmic Survey,Czech Text Datasets,Sentiment Analysis,Supervised Learning,necitovane,text},
mendeley-tags = {necitovane,text},
month = {jan},
pages = {973--979},
publisher = {SciTePress},
title = {{Sentiment Analysis of Czech Texts: An Algorithmic Survey}},
url = {http://arxiv.org/abs/1901.02780 http://dx.doi.org/10.5220/0007695709730979},
volume = {2},
year = {2019}
}
@book{Mitchell1997,
author = {Mitchell, Tom M.},
edition = {McGraw-Hil},
pages = {99},
publisher = {New York : McGraw-Hill},
title = {{Machine Learning}},
year = {1997}
}
@article{Kittask2020,
abstract = {Recently, large pre-trained language models, such as BERT, have reached state-of-the-art performance in many natural language processing tasks, but for many languages, including Estonian, BERT models are not yet available. However, there exist several multilingual BERT models that can handle multiple languages simultaneously and that have been trained also on Estonian data. In this paper, we evaluate four multilingual models---multilingual BERT, multilingual distilled BERT, XLM and XLM-RoBERTa---on several NLP tasks including POS and morphological tagging, NER and text classification. Our aim is to establish a comparison between these multilingual BERT models and the existing baseline neural models for these tasks. Our results show that multilingual BERT models can generalise well on different Estonian NLP tasks outperforming all baselines models for POS and morphological tagging and text classification, and reaching the comparable level with the best baseline for NER, with XLM-RoBERTa achieving the highest results compared with other multilingual models.},
archivePrefix = {arXiv},
arxivId = {2010.00454},
author = {Kittask, Claudia and Milintsevich, Kirill and Sirts, Kairit},
eprint = {2010.00454},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Kittask, Milintsevich, Sirts - 2020 - Evaluating Multilingual BERT for Estonian.pdf:pdf},
keywords = {Estonian,NER,POS tagging,multilingual BERT,necitovane,text classification},
mendeley-tags = {necitovane},
month = {oct},
title = {{Evaluating Multilingual BERT for Estonian}},
url = {http://arxiv.org/abs/2010.00454},
year = {2020}
}
@article{Tibshirani1996,
abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
author = {Tibshirani, Robert},
doi = {10.1111/j.2517-6161.1996.tb02080.x},
issn = {0035-9246},
journal = {J. R. Stat. Soc. Ser. B},
keywords = {quadratic programming,regression,shrinkage,subset selection},
month = {jan},
number = {1},
pages = {267--288},
publisher = {Wiley},
title = {{Regression Shrinkage and Selection Via the Lasso}},
url = {https://rss.onlinelibrary.wiley.com/doi/full/10.1111/j.2517-6161.1996.tb02080.x https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1996.tb02080.x https://rss.onlinelibrary.wiley.com/doi/10.1111/j.2517-6161.1996.tb02080.x},
volume = {58},
year = {1996}
}
@article{Tenney2019,
abstract = {Pre-trained text encoders have rapidly advanced the state of the art on many NLP tasks. We focus on one such model, BERT, and aim to quantify where linguistic information is captured within the network. We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lower-level decisions on the basis of disambiguating information from higher-level representations.},
archivePrefix = {arXiv},
arxivId = {1905.05950},
author = {Tenney, Ian and Das, Dipanjan and Pavlick, Ellie},
eprint = {1905.05950},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Tenney, Das, Pavlick - 2019 - BERT Rediscovers the Classical NLP Pipeline.pdf:pdf},
journal = {ACL 2019 - 57th Annu. Meet. Assoc. Comput. Linguist. Proc. Conf.},
keywords = {necitovane},
mendeley-tags = {necitovane},
month = {may},
pages = {4593--4601},
publisher = {Association for Computational Linguistics (ACL)},
title = {{BERT Rediscovers the Classical NLP Pipeline}},
url = {http://arxiv.org/abs/1905.05950},
year = {2019}
}
@techreport{Hutchins1996,
author = {Hutchins, John},
booktitle = {books.google.com},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Hutchins - 1996 - ALPAC the (in)famous report.pdf:pdf},
pages = {131--135},
publisher = {The MIT Press},
title = {{ALPAC: the (in)famous report}},
url = {https://books.google.com/books?hl=cs{\&}lr={\&}id=yx3lEVJMBmMC{\&}oi=fnd{\&}pg=PA131{\&}dq=alpac+report{\&}ots=se2vhONMHp{\&}sig=ByL2IgJLxRwF3f6n9bqOPFx88r4},
volume = {14},
year = {1996}
}
@article{Feijo2020,
abstract = {BERT (Bidirectional Encoder Representations from Transformers) and ALBERT (A Lite BERT) are methods for pre-training language models which can later be fine-tuned for a variety of Natural Language Understanding tasks. These methods have been applied to a number of such tasks (mostly in English), achieving results that outperform the state-of-the-art. In this paper, our contribution is twofold. First, we make available our trained BERT and Albert model for Portuguese. Second, we compare our monolingual and the standard multilingual models using experiments in semantic textual similarity, recognizing textual entailment, textual category classification, sentiment analysis, offensive comment detection, and fake news detection, to assess the effectiveness of the generated language representations. The results suggest that both monolingual and multilingual models are able to achieve state-of-the-art and the advantage of training a single language model, if any, is small.},
archivePrefix = {arXiv},
arxivId = {2007.09757},
author = {Feijo, Diego de Vargas and Moreira, Viviane Pereira},
eprint = {2007.09757},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Feijo, Moreira - 2020 - Mono vs Multilingual Transformer-based Models a Comparison across Several Language Tasks.pdf:pdf},
month = {jul},
title = {{Mono vs Multilingual Transformer-based Models: a Comparison across Several Language Tasks}},
url = {http://arxiv.org/abs/2007.09757},
year = {2020}
}
@techreport{McCulloch,
author = {McCulloch, WS and bulletin of mathematical Biophysics, W Pitts - The and 1943, Undefined},
booktitle = {Springer},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/McCulloch, biophysics, 1943 - Unknown - A logical calculus of the ideas immanent in nervous activity.pdf:pdf},
keywords = {perceptron},
mendeley-tags = {perceptron},
title = {{A logical calculus of the ideas immanent in nervous activity}},
url = {https://link.springer.com/article/10.1007{\%}252FBF02478259}
}
@techreport{Mikolov2013,
abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
booktitle = {papers.nips.cc},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Mikolov et al. - Unknown - Distributed Representations of Words and Phrases and their Compositionality.pdf:pdf},
title = {{Distributed Representations of Words and Phrases and their Compositionality}},
url = {http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and},
year = {2013}
}
@article{Collobert2011,
abstract = {We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including: part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.},
archivePrefix = {arXiv},
arxivId = {1103.0398},
author = {Collobert, Ronan and Weston, Jason and Bottou, Leon and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel},
eprint = {1103.0398},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Collobert et al. - 2011 - Natural Language Processing (almost) from Scratch.pdf:pdf},
journal = {J. Mach. Learn. Res.},
keywords = {Natural language processing,Neural networks},
month = {mar},
pages = {2493--2537},
title = {{Natural Language Processing (almost) from Scratch}},
url = {http://arxiv.org/abs/1103.0398},
volume = {12},
year = {2011}
}
@inproceedings{Peters2018,
abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-Trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
archivePrefix = {arXiv},
arxivId = {1802.05365},
author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
booktitle = {NAACL HLT 2018 - 2018 Conf. North Am. Chapter Assoc. Comput. Linguist. Hum. Lang. Technol. - Proc. Conf.},
doi = {10.18653/v1/n18-1202},
eprint = {1802.05365},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Peters et al. - 2018 - Deep contextualized word representations.pdf:pdf},
isbn = {9781948087278},
month = {feb},
pages = {2227--2237},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Deep contextualized word representations}},
url = {http://allennlp.org/elmo},
volume = {1},
year = {2018}
}
@inproceedings{Forcada1997,
abstract = {This paper presents a modification of Pollack's RAAM (Recursive Auto-Associative Memory), called a Recursive Hetero-Associative Memory (RHAM), and shows that it is capable of learning simple translation tasks, by building a state-space representation of each input string and unfolding it to obtain the corresponding output string. RHAM-based translators are computationally more powerful and easier to train than their corresponding double-RAAM counterparts in the literature.},
author = {Forcada, Mikel L. and {\~{N}}eco, Ram{\'{o}}n P.},
booktitle = {Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics)},
doi = {10.1007/bfb0032504},
isbn = {3540630473},
issn = {16113349},
keywords = {transformers},
mendeley-tags = {transformers},
pages = {453--462},
publisher = {Springer Verlag},
title = {{Recursive hetero-Associative memories for translation}},
url = {https://link.springer.com/chapter/10.1007/BFb0032504},
volume = {1240 LNCS},
year = {1997}
}
@misc{Google,
author = {Google},
keywords = {necitovane},
mendeley-tags = {necitovane},
title = {{Google AI Blog: A Neural Network for Machine Translation, at Production Scale}},
url = {https://ai.googleblog.com/2016/09/a-neural-network-for-machine.html},
urldate = {2020-10-25}
}
@techreport{Plank,
abstract = {Bidirectional long short-term memory (bi-LSTM) networks have recently proven successful for various NLP sequence mod-eling tasks, but little is known about their reliance to input representations, target languages, data set size, and label noise. We address these issues and evaluate bi-LSTMs with word, character, and unicode byte embeddings for POS tagging. We compare bi-LSTMs to traditional POS taggers across languages and data sizes. We also present a novel bi-LSTM model, which combines the POS tagging loss function with an auxiliary loss function that accounts for rare words. The model obtains state-of-the-art performance across 22 languages, and works especially well for morphologically complex languages. Our analysis suggests that bi-LSTMs are less sensitive to training data size and label corruptions (at small noise levels) than previously assumed.},
archivePrefix = {arXiv},
arxivId = {1604.05529v3},
author = {Plank, Barbara and S{\o}gaard, Anders and Goldberg, Yoav},
eprint = {1604.05529v3},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Plank, S{\o}gaard, Goldberg - Unknown - Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary.pdf:pdf},
keywords = {necitovane},
mendeley-tags = {necitovane},
title = {{Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss}},
url = {https://github.com/clab/cnn}
}
@techreport{Bahdanau,
abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neu-ral machine translation often belong to a family of encoder-decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture , and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
archivePrefix = {arXiv},
arxivId = {1409.0473v7},
author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
eprint = {1409.0473v7},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Bahdanau, Cho, Bengio - Unknown - NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE.pdf:pdf},
title = {{NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE}}
}
@article{Hercig2018,
abstract = {Sentiment analysis is a wide area with great potential and many research directions. One direction is stance detection, which is somewhat similar to sentiment analysis. We supplement stance detection dataset with sentiment annotation and explore the similarities of these tasks. We show that stance detection and sentiment analysis can be mutually beneficial by using gold label for one task as features for the other task. We analysed the presence of target entities for stance detection in the dataset. We outperform the state-of-the-art results for stance detection in Czech and set new state-of-the-art results for the newly created sentiment analysis part of the extended dataset.},
author = {Hercig, Tom{\'{a}}{\v{s}} and Krejzl, Peter and Kr{\'{a}}l, Pavel},
doi = {10.13053/CyS-22-3-3014},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Hercig, Krejzl, Kr{\'{a}}l - 2018 - Stance and sentiment in Czech.pdf:pdf},
issn = {20079737},
journal = {Comput. y Sist.},
keywords = {Czech,Natural language processing,Sentiment analysis,Stance detection,necitovane},
mendeley-tags = {necitovane},
number = {3},
pages = {787--794},
publisher = {Instituto Politecnico Nacional},
title = {{Stance and sentiment in Czech}},
url = {http://nlp.kiv.zcu.cz/research/sentiment{\#}stance.},
volume = {22},
year = {2018}
}
@techreport{Dai2015,
abstract = {We present two approaches that use unlabeled data to improve sequence learning with recurrent networks. The first approach is to predict what comes next in a sequence, which is a conventional language model in natural language processing. The second approach is to use a sequence autoencoder, which reads the input sequence into a vector and predicts the input sequence again. These two algorithms can be used as a "pretraining" step for a later supervised sequence learning algorithm. In other words, the parameters obtained from the unsupervised step can be used as a starting point for other supervised training models. In our experiments, we find that long short term memory recurrent networks after being pretrained with the two approaches are more stable and generalize better. With pretraining, we are able to train long short term memory recurrent networks up to a few hundred timesteps, thereby achieving strong performance in many text classification tasks, such as IMDB, DBpedia and 20 Newsgroups.},
archivePrefix = {arXiv},
arxivId = {1511.01432v1},
author = {Dai, Andrew M and Le, Quoc V},
eprint = {1511.01432v1},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Dai, Le - 2015 - Semi-supervised Sequence Learning.pdf:pdf},
keywords = {()},
title = {{Semi-supervised Sequence Learning}},
url = {http://ai.stanford.edu/amaas/data/sentiment/index.html},
year = {2015}
}
@article{Krizhevsky2017,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%}, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used nonsaturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
doi = {10.1145/3065386},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Krizhevsky, Sutskever, Hinton - 2017 - ImageNet classification with deep convolutional neural networks.pdf:pdf},
issn = {15577317},
journal = {Commun. ACM},
month = {jun},
number = {6},
pages = {84--90},
publisher = {Association for Computing Machinery},
title = {{ImageNet classification with deep convolutional neural networks}},
url = {https://dl.acm.org/doi/10.1145/3065386},
volume = {60},
year = {2017}
}
@techreport{Ling,
abstract = {We introduce a model for constructing vector representations of words by composing characters using bidirectional LSTMs. Relative to traditional word representation models that have independent vectors for each word type, our model requires only a single vector per character type and a fixed set of parameters for the compositional model. Despite the compactness of this model and, more importantly, the arbitrary nature of the form-function relationship in language , our "composed" word representations yield state-of-the-art results in language modeling and part-of-speech tagging. Benefits over traditional baselines are particularly pronounced in morphologically rich languages (e.g., Turkish).},
archivePrefix = {arXiv},
arxivId = {1508.02096v2},
author = {Ling, Wang and Lu{\'{i}}s, Tiago and Marujo, Lu{\'{i}}s and Fernandez, Ram{\'{o}}n and Amir, Astudillo Silvio and Dyer, Chris and Black, Alan W and Trancoso, Isabel},
eprint = {1508.02096v2},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Ling et al. - Unknown - Finding Function in Form Compositional Character Models for Open Vocabulary Word Representation(2).pdf:pdf},
keywords = {embeddings},
mendeley-tags = {embeddings},
title = {{Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation}},
year = {2016}
}
@article{Liu2019,
abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
archivePrefix = {arXiv},
arxivId = {1907.11692},
author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
eprint = {1907.11692},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Liu et al. - 2019 - RoBERTa A Robustly Optimized BERT Pretraining Approach(2).pdf:pdf},
journal = {arXiv},
keywords = {necitovane},
mendeley-tags = {necitovane},
month = {jul},
publisher = {arXiv},
title = {{RoBERTa: A Robustly Optimized BERT Pretraining Approach}},
url = {http://arxiv.org/abs/1907.11692},
year = {2019}
}
@techreport{Turian2010,
abstract = {If we take an existing supervised NLP system , a simple and general way to improve accuracy is to use unsupervised word representations as extra word features. We evaluate Brown clusters, Collobert and Weston (2008) embeddings, and HLBL (Mnih {\&} Hinton, 2009) embeddings of words on both NER and chunking. We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accuracy of these baselines. We find further improvements by combining different word representations. You can download our word features, for off-the-shelf use in existing NLP systems, as well as our code, here: http://metaoptimize. com/projects/wordreprs/},
author = {Turian, Joseph and Ratinov, Lev and Bengio, Yoshua},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Turian, Ratinov, Bengio - 2010 - Word representations A simple and general method for semi-supervised learning.pdf:pdf},
pages = {11--16},
publisher = {Association for Computational Linguistics},
title = {{Word representations: A simple and general method for semi-supervised learning}},
url = {http://metaoptimize.},
year = {2010}
}
@article{Chronopoulou2019,
abstract = {A growing number of state-of-the-art transfer learning methods employ language models pretrained on large generic corpora. In this paper we present a conceptually simple and effective transfer learning approach that addresses the problem of catastrophic forgetting. Specifically, we combine the task-specific optimization function with an auxiliary language model objective, which is adjusted during the training process. This preserves language regularities captured by language models, while enabling sufficient adaptation for solving the target task. Our method does not require pretraining or finetuning separate components of the network and we train our models end-to-end in a single step. We present results on a variety of challenging affective and text classification tasks, surpassing well established transfer learning methods with greater level of complexity.},
archivePrefix = {arXiv},
arxivId = {1902.10547},
author = {Chronopoulou, Alexandra and Baziotis, Christos and Potamianos, Alexandros},
eprint = {1902.10547},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Chronopoulou, Baziotis, Potamianos - 2019 - An Embarrassingly Simple Approach for Transfer Learning from Pretrained Language Models.pdf:pdf},
journal = {NAACL HLT 2019 - 2019 Conf. North Am. Chapter Assoc. Comput. Linguist. Hum. Lang. Technol. - Proc. Conf.},
keywords = {moznosti,necitovane},
mendeley-tags = {moznosti,necitovane},
month = {feb},
pages = {2089--2095},
publisher = {Association for Computational Linguistics (ACL)},
title = {{An Embarrassingly Simple Approach for Transfer Learning from Pretrained Language Models}},
url = {http://arxiv.org/abs/1902.10547},
volume = {1},
year = {2019}
}
@article{Virtanen2019,
abstract = {Deep learning-based language models pretrained on large unannotated text corpora have been demonstrated to allow efficient transfer learning for natural language processing, with recent approaches such as the transformer-based BERT model advancing the state of the art across a variety of tasks. While most work on these models has focused on high-resource languages, in particular English, a number of recent efforts have introduced multilingual models that can be fine-tuned to address tasks in a large number of different languages. However, we still lack a thorough understanding of the capabilities of these models, in particular for lower-resourced languages. In this paper, we focus on Finnish and thoroughly evaluate the multilingual BERT model on a range of tasks, comparing it with a new Finnish BERT model trained from scratch. The new language-specific model is shown to systematically and clearly outperform the multilingual. While the multilingual model largely fails to reach the performance of previously proposed methods, the custom Finnish BERT model establishes new state-of-the-art results on all corpora for all reference tasks: part-of-speech tagging, named entity recognition, and dependency parsing. We release the model and all related resources created for this study with open licenses at https://turkunlp.org/finbert .},
archivePrefix = {arXiv},
arxivId = {1912.07076},
author = {Virtanen, Antti and Kanerva, Jenna and Ilo, Rami and Luoma, Jouni and Luotolahti, Juhani and Salakoski, Tapio and Ginter, Filip and Pyysalo, Sampo},
eprint = {1912.07076},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Virtanen et al. - 2019 - Multilingual is not enough BERT for Finnish.pdf:pdf},
keywords = {bertology,necitovane,practise},
mendeley-tags = {bertology,necitovane,practise},
month = {dec},
title = {{Multilingual is not enough: BERT for Finnish}},
url = {http://arxiv.org/abs/1912.07076},
year = {2019}
}
@inproceedings{Brill1998,
abstract = {One of the most exciting recent directions in machine learning is the discovery that the combination of multiple classifiers often results in significantly better performance than what can be achieved with a single classifier. In this paper, we first show that the errors made from three different state of the art part of speech taggers are strongly complementary. Next, we show how this complementary behavior can be used to our advantage. By using contextual cues to guide tagger combination, we are able to derive a new tagger that achieves performance significantly greater than any of the individual taggers. Introduction Part of speech tagging has been a central problem in natural language processing for many years. Since the advent of manually tagged corpora such as the Brown Corpus and the Penn Treebank (Francis(1982), Marcus(1993)), the efficacy of machine learning for training a tagger has been demonstrated using a wide array of techniques, including: Markov models, decision trees...},
author = {Brill, Eric and Wu, Jun},
doi = {10.3115/980845.980876},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Brill, Wu - 1998 - Classifier combination for improved lexical disambiguation.pdf:pdf},
keywords = {necitovane},
mendeley-tags = {necitovane},
pages = {191},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Classifier combination for improved lexical disambiguation}},
url = {http://www.cis.upenn.edu/-adwait},
year = {1998}
}
@techreport{Hajic-BarboraHladka,
author = {{Haji{\v{c}} -Barbora Hladk{\'{a}}}, Jan},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Haji{\v{c}} -Barbora Hladk{\'{a}} - Unknown - Morfologick{\'{e}} zna{\v{c}}kov{\'{a}}n{\'{i}} korpusu {\v{c}}esk{\'{y}}ch textů stochastickou metodou.pdf:pdf},
keywords = {necitovane},
mendeley-tags = {necitovane},
title = {{Morfologick{\'{e}} zna{\v{c}}kov{\'{a}}n{\'{i}} korpusu {\v{c}}esk{\'{y}}ch textů stochastickou metodou}}
}
@techreport{Huh,
abstract = {The tremendous success of ImageNet-trained deep features on a wide range of transfer tasks raises the question: what is it about the ImageNet dataset that makes the learnt features as good as they are? This work provides an empirical investigation into the various facets of this question, such as, looking at the importance of the amount of examples , number of classes, balance between images-per-class and classes, and the role of fine and coarse grained recognition. We pre-train CNN features on various subsets of the ImageNet dataset and evaluate transfer performance on a variety of standard vision tasks. Our overall findings suggest that most changes in the choice of pre-training data long thought to be critical, do not significantly affect transfer performance.},
archivePrefix = {arXiv},
arxivId = {1608.08614v2},
author = {Huh, Minyoung and Agrawal, Pulkit and Efros, Alexei A},
eprint = {1608.08614v2},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Huh, Agrawal, Efros - Unknown - What makes ImageNet good for transfer learning.pdf:pdf},
title = {{What makes ImageNet good for transfer learning?}}
}
@techreport{Li,
abstract = {In this paper, we investigate the modeling power of contextualized embeddings from pre-trained language models, e.g. BERT, on the E2E-ABSA task. Specifically, we build a series of simple yet insightful neural base-lines to deal with E2E-ABSA. The experimental results show that even with a simple linear classification layer, our BERT-based architecture can outperform state-of-the-art works. Besides, we also standardize the comparative study by consistently utilizing a hold-out development dataset for model selection, which is largely ignored by previous works. Therefore , our work can serve as a BERT-based benchmark for E2E-ABSA. 1},
author = {Li, Xin and Bing, Lidong and Zhang, Wenxuan and Lam, Wai},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Li et al. - Unknown - Exploiting BERT for End-to-End Aspect-based Sentiment Analysis.pdf:pdf},
keywords = {necitovane},
mendeley-tags = {necitovane},
pages = {34--41},
title = {{Exploiting BERT for End-to-End Aspect-based Sentiment Analysis *}}
}
@article{Wu2016,
abstract = {Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units ("wordpieces") for both input and output. This method provides a good balance between the flexibility of "character"-delimited models and the efficiency of "word"-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60{\%} compared to Google's phrase-based production system.},
archivePrefix = {arXiv},
arxivId = {1609.08144},
author = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V. and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and Klingner, Jeff and Shah, Apurva and Johnson, Melvin and Liu, Xiaobing and Kaiser, {\L}ukasz and Gouws, Stephan and Kato, Yoshikiyo and Kudo, Taku and Kazawa, Hideto and Stevens, Keith and Kurian, George and Patil, Nishant and Wang, Wei and Young, Cliff and Smith, Jason and Riesa, Jason and Rudnick, Alex and Vinyals, Oriol and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
eprint = {1609.08144},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Wu et al. - 2016 - Google's Neural Machine Translation System Bridging the Gap between Human and Machine Translation.pdf:pdf},
keywords = {transformers},
mendeley-tags = {transformers},
month = {sep},
title = {{Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation}},
url = {http://arxiv.org/abs/1609.08144},
year = {2016}
}
@techreport{Russell1995,
author = {Russell, Stuart J and Norvig, Peter and Canny, John F and Malik, Jitendra M and Edwards, Douglas D},
isbn = {0131038052},
pages = {529},
title = {{Artificial Intelligence A Modern Approach}},
year = {1995}
}
@techreport{Straka2019b,
abstract = {We present our contribution to the SIGMOR-PHON 2019 Shared Task: Crosslinguality and Context in Morphology, Task 2: contextual morphological analysis and lemmatization. We submitted a modification of the UDPipe 2.0, one of best-performing systems of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies and an overall winner of the The 2018 Shared Task on Extrinsic Parser Evaluation. As our first improvement, we use the pre-trained contextualized embeddings (BERT) as additional inputs to the network; secondly, we use individual morphological features as reg-ularization; and finally, we merge the selected corpora of the same language. In the lemmatization task, our system exceeds all the submitted systems by a wide margin with lemmatization accuracy 95.78 (second best was 95.00, third 94.46). In the morphological analysis, our system placed tightly second: our morphological analysis accuracy was 93.19, the winning system's 93.23.},
author = {Straka, Milan and Strakov{\'{a}}, Jana and Haji{\v{c}}, Jan},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Straka, Strakov{\'{a}}, Haji{\v{c}} - 2019 - Regularization with Morphological Categories, Corpora Merging.pdf:pdf},
keywords = {necitovane},
mendeley-tags = {necitovane},
pages = {95--103},
title = {{Regularization with Morphological Categories, Corpora Merging}},
url = {https://github.com/google-research/},
year = {2019}
}
@article{Minsky2017,
author = {Minsky, M and Papert, SA},
title = {{Perceptrons: An introduction to computational geometry}},
url = {https://www.google.com/books?hl=cs{\&}lr={\&}id=PLQ5DwAAQBAJ{\&}oi=fnd{\&}pg=PR5{\&}dq=perceptrons+an+introduction+to+computational+geometry{\&}ots=zzCzAMspY0{\&}sig=x0HLubdLNN3Irw4cZUlmdyHK8BM},
year = {2017}
}
@article{Santos2016,
abstract = {In this work, we propose Attentive Pooling (AP), a two-way attention mechanism for discriminative model training. In the context of pair-wise ranking or classification with neural networks, AP enables the pooling layer to be aware of the current input pair, in a way that information from the two input items can directly influence the computation of each other's representations. Along with such representations of the paired inputs, AP jointly learns a similarity measure over projected segments (e.g. trigrams) of the pair, and subsequently, derives the corresponding attention vector for each input to guide the pooling. Our two-way attention mechanism is a general framework independent of the underlying representation learning, and it has been applied to both convolutional neural networks (CNNs) and recurrent neural networks (RNNs) in our studies. The empirical results, from three very different benchmark tasks of question answering/answer selection, demonstrate that our proposed models outperform a variety of strong baselines and achieve state-of-the-art performance in all the benchmarks.},
archivePrefix = {arXiv},
arxivId = {1602.03609},
author = {dos Santos, Cicero and Tan, Ming and Xiang, Bing and Zhou, Bowen},
eprint = {1602.03609},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Santos et al. - 2016 - Attentive Pooling Networks.pdf:pdf},
month = {feb},
title = {{Attentive Pooling Networks}},
url = {http://arxiv.org/abs/1602.03609},
year = {2016}
}
@inproceedings{Ramachandran2017,
abstract = {This work presents a general unsupervised learning method to improve the accuracy of sequence to sequence (seq2seq) models. In our method, the weights of the encoder and decoder of a seq2seq model are initialized with the pretrained weights of two language models and then fine-tuned with labeled data. We apply this method to challenging benchmarks in machine translation and abstractive summarization and find that it significantly improves the subsequent supervised models. Our main result is that pretraining improves the generalization of seq2seq models. We achieve state-of-the-art results on the WMT English→German task, surpassing a range of methods using both phrase-based machine translation and neural machine translation. Our method achieves a significant improvement of 1.3 BLEU from the previous best models on both WMT'14 and WMT'15 English→German. We also conduct human evaluations on abstractive summarization and find that our method outperforms a purely supervised learning baseline in a statistically significant manner.},
archivePrefix = {arXiv},
arxivId = {1611.02683},
author = {Ramachandran, Prajit and Liu, Peter J. and Le, Quoc V.},
booktitle = {EMNLP 2017 - Conf. Empir. Methods Nat. Lang. Process. Proc.},
doi = {10.18653/v1/d17-1039},
eprint = {1611.02683},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Ramachandran, Liu, Le - 2017 - Unsupervised pretraining for sequence to sequence learning.pdf:pdf},
isbn = {9781945626838},
pages = {383--391},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Unsupervised pretraining for sequence to sequence learning}},
year = {2017}
}
@article{Hoerl1970,
abstract = {In multiple regression it is shown that parameter estimates based on minimum residual sum of squares have a high probability of being unsatisfactory, if not incorrect, if the prediction vectors are not orthogonal. Proposed is an estimation procedure based on adding small positive quantities to the diagonal of X′X. Introduced is the ridge trace, a method for showing in two dimensions the effects of nonorthogonality. It is then shown how to augment X′X to obtain biased estimates with smaller mean square error. {\textcopyright} 1970 Taylor and Francis Group, LLC.},
author = {Hoerl, Arthur E. and Kennard, Robert W.},
doi = {10.1080/00401706.1970.10488634},
issn = {15372723},
journal = {Technometrics},
number = {1},
pages = {55--67},
title = {{Ridge Regression: Biased Estimation for Nonorthogonal Problems}},
volume = {12},
year = {1970}
}
@techreport{Sun,
abstract = {Language model pre-training has proven to be useful in learning universal language representations. As a state-of-the-art language model pre-training model, BERT (Bidirectional En-coder Representations from Transformers) has achieved amazing results in many language understanding tasks. In this paper, we conduct exhaustive experiments to investigate different fine-tuning methods of BERT on text classification task and provide a general solution for BERT fine-tuning. Finally, the proposed solution obtains new state-of-the-art results on eight widely-studied text classification datasets. 1},
archivePrefix = {arXiv},
arxivId = {1905.05583v3},
author = {Sun, Chi and Qiu, Xipeng and Xu, Yige and Huang, Xuanjing},
eprint = {1905.05583v3},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Sun et al. - Unknown - How to Fine-Tune BERT for Text Classification.pdf:pdf},
keywords = {bertology,necitovane,practise},
mendeley-tags = {bertology,necitovane,practise},
title = {{How to Fine-Tune BERT for Text Classification?}},
url = {https://github.}
}
@techreport{Goldberg,
author = {Goldberg, Yoav},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Goldberg - Unknown - A Primer on Neural Network Models for Natural Language Processing.pdf:pdf},
keywords = {modeling,necitovane},
mendeley-tags = {modeling,necitovane},
title = {{A Primer on Neural Network Models for Natural Language Processing}},
url = {http://www.cs.biu.}
}
@inproceedings{Cho2014,
abstract = {In this paper, we propose a novel neural network model called RNN Encoder- Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
archivePrefix = {arXiv},
arxivId = {1406.1078},
author = {Cho, Kyunghyun and {Van Merri{\"{e}}nboer}, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
booktitle = {EMNLP 2014 - 2014 Conf. Empir. Methods Nat. Lang. Process. Proc. Conf.},
doi = {10.3115/v1/d14-1179},
eprint = {1406.1078},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Cho et al. - 2014 - Learning phrase representations using RNN encoder-decoder for statistical machine translation.pdf:pdf},
isbn = {9781937284961},
keywords = {gru,transformers},
mendeley-tags = {gru,transformers},
month = {jun},
pages = {1724--1734},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Learning phrase representations using RNN encoder-decoder for statistical machine translation}},
url = {https://arxiv.org/abs/1406.1078v3},
year = {2014}
}
@inproceedings{Zhou2020,
abstract = {Head-driven phrase structure grammar (HPSG) enjoys a uniform formalism representing rich contextual syntactic and even semantic meanings. This paper makes the first attempt to formulate a simplified HPSG by integrating constituent and dependency formal representations into head-driven phrase structure. Then two parsing algorithms are respectively proposed for two converted tree representations, division span and joint span. As HPSG encodes both constituent and dependency structure information, the proposed HPSG parsers may be regarded as a sort of joint decoder for both types of structures and thus are evaluated in terms of extracted or converted constituent and dependency parsing trees. Our parser achieves new state-of-the-art performance for both parsing tasks on Penn Treebank (PTB) and Chinese Penn Treebank, verifying the effectiveness of joint learning constituent and dependency structures. In details, we report 95.84 F1 of constituent parsing and 97.00{\%} UAS of dependency parsing on PTB.},
archivePrefix = {arXiv},
arxivId = {1907.02684},
author = {Zhou, Junru and Zhao, Hai},
booktitle = {ACL 2019 - 57th Annu. Meet. Assoc. Comput. Linguist. Proc. Conf.},
doi = {10.18653/v1/p19-1230},
eprint = {1907.02684},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Zhou, Zhao - 2020 - Head-driven phrase structure grammar parsing on Penn treebank.pdf:pdf},
isbn = {9781950737482},
keywords = {necitovane},
mendeley-tags = {necitovane},
pages = {2396--2408},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Head-driven phrase structure grammar parsing on Penn treebank}},
url = {http://nlp.cs.lth.se/software/treebank},
year = {2020}
}
@article{Raffel2019,
abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
archivePrefix = {arXiv},
arxivId = {1910.10683},
author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
eprint = {1910.10683},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Raffel et al. - 2019 - Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.pdf:pdf},
journal = {arXiv},
keywords = {attention-based models,deep learning,multi-task learning,natural language processing,transfer learning},
month = {oct},
pages = {1--67},
publisher = {arXiv},
title = {{Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer}},
url = {http://arxiv.org/abs/1910.10683},
volume = {21},
year = {2019}
}
@techreport{Lenc2016,
author = {Lenc, Ladislav and Hercig, Tom{\'{a}}{\v{s}}},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Lenc, Hercig - 2016 - Neural Networks for Sentiment Analysis in Czech.pdf:pdf},
title = {{Neural Networks for Sentiment Analysis in Czech}},
year = {2016}
}
@techreport{Bengio2003,
abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.},
author = {Bengio, Yoshua and Ducharme, R{\'{e}}jean and Vincent, Pascal and Jauvin, Christian and Ca, Jauvinc@iro Umontreal and Kandola, Jaz and Hofmann, Thomas and Poggio, Tomaso and Shawe-Taylor, John},
booktitle = {J. Mach. Learn. Res.},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Bengio et al. - 2003 - A Neural Probabilistic Language Model.pdf:pdf},
keywords = {Statistical language modeling,artificial neural networks,curse of dimensionality,distributed representation,embeddings},
mendeley-tags = {embeddings},
pages = {1137--1155},
title = {{A Neural Probabilistic Language Model}},
volume = {3},
year = {2003}
}
@techreport{Hajic,
abstract = {Statistical modeling is now the prevailing method using in automatic procedures of analysis of a natural language. Such an analysis can be performed at various levels, from phonetics to semantics. Two levels of representation are described: a morphological one and a syntactic one that is further subdivided into a surface syntax and deep syntax (tectogrammatics). The role of linguistically annotated corpora will be stressed as a necessary prerequisite for any supervised machine learning algorithms, showing examples from the Prague Dependency Treebank (PDT) being developed at Charles University, Prague. A possible application of some of the tools created during (and thanks to) the development of the PDT will be shown, namely, a machine translation system translating from Czech to Slovak.},
author = {Haji{\v{c}}, Jan},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Haji{\v{c}} - Unknown - Statistick{\'{e}} modelov{\'{a}}n{\'{i}} a automatick{\'{a}} anal{\'{y}}za přirozen{\'{e}}ho jazyka (morfologie, syntax, překlad).pdf:pdf},
keywords = {necitovane},
mendeley-tags = {necitovane},
title = {{Statistick{\'{e}} modelov{\'{a}}n{\'{i}} a automatick{\'{a}} anal{\'{y}}za přirozen{\'{e}}ho jazyka (morfologie, syntax, překlad)}}
}
@techreport{Hutchins,
author = {Hutchins, John},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Hutchins - Unknown - Two precursors of machine translation Artsrouni and Trojanskij.pdf:pdf},
title = {{Two precursors of machine translation: Artsrouni and Trojanskij}}
}
@article{Straka2019,
abstract = {Contextualized embeddings, which capture appropriate word meaning depending on context, have recently been proposed. We evaluate two meth ods for precomputing such embeddings, BERT and Flair, on four Czech text processing tasks: part-of-speech (POS) tagging, lemmatization, dependency pars ing and named entity recognition (NER). The first three tasks, POS tagging, lemmatization and dependency parsing, are evaluated on two corpora: the Prague Dependency Treebank 3.5 and the Universal Dependencies 2.3. The named entity recognition (NER) is evaluated on the Czech Named Entity Corpus 1.1 and 2.0. We report state-of-the-art results for the above mentioned tasks and corpora.},
archivePrefix = {arXiv},
arxivId = {1909.03544},
author = {Straka, Milan and Strakov{\'{a}}, Jana and Haji{\v{c}}, Jan},
eprint = {1909.03544},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Straka, Strakov{\'{a}}, Haji{\v{c}} - 2019 - Czech Text Processing with Contextual Embeddings POS Tagging, Lemmatization, Parsing and NER.pdf:pdf},
journal = {Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics)},
keywords = {BERT,Contextualized embeddings,Czech,Dependency parsing,Flair,Lemmatization,Named entity recognition,POS tagging},
month = {sep},
pages = {137--150},
publisher = {Springer Verlag},
title = {{Czech Text Processing with Contextual Embeddings: POS Tagging, Lemmatization, Parsing and NER}},
url = {http://arxiv.org/abs/1909.03544},
volume = {11697 LNAI},
year = {2019}
}
@article{Santos2016a,
abstract = {In this work, we propose Attentive Pooling (AP), a two-way attention mechanism for discriminative model training. In the context of pair-wise ranking or classification with neural networks, AP enables the pooling layer to be aware of the current input pair, in a way that information from the two input items can directly influence the computation of each other's representations. Along with such representations of the paired inputs, AP jointly learns a similarity measure over projected segments (e.g. trigrams) of the pair, and subsequently, derives the corresponding attention vector for each input to guide the pooling. Our two-way attention mechanism is a general framework independent of the underlying representation learning, and it has been applied to both convolutional neural networks (CNNs) and recurrent neural networks (RNNs) in our studies. The empirical results, from three very different benchmark tasks of question answering/answer selection, demonstrate that our proposed models outperform a variety of strong baselines and achieve state-of-the-art performance in all the benchmarks.},
archivePrefix = {arXiv},
arxivId = {1602.03609},
author = {dos Santos, Cicero and Tan, Ming and Xiang, Bing and Zhou, Bowen},
eprint = {1602.03609},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Santos et al. - 2016 - Attentive Pooling Networks(2).pdf:pdf},
keywords = {necitovane},
mendeley-tags = {necitovane},
month = {feb},
title = {{Attentive Pooling Networks}},
url = {http://arxiv.org/abs/1602.03609},
year = {2016}
}
