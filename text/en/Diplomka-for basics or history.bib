Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Krizhevsky2017,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%}, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used nonsaturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
doi = {10.1145/3065386},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Krizhevsky, Sutskever, Hinton - 2017 - ImageNet classification with deep convolutional neural networks.pdf:pdf},
issn = {15577317},
journal = {Commun. ACM},
month = {jun},
number = {6},
pages = {84--90},
publisher = {Association for Computing Machinery},
title = {{ImageNet classification with deep convolutional neural networks}},
url = {https://dl.acm.org/doi/10.1145/3065386},
volume = {60},
year = {2017}
}
@techreport{Bahdanau,
abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neu-ral machine translation often belong to a family of encoder-decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture , and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
archivePrefix = {arXiv},
arxivId = {1409.0473v7},
author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
eprint = {1409.0473v7},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Bahdanau, Cho, Bengio - Unknown - NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE.pdf:pdf},
title = {{NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE}}
}
@article{Rosenblatt1958,
abstract = {To answer the questions of how information about the physical world is sensed, in what form is information remembered, and how does information retained in memory influence recognition and behavior, a theory is developed for a hypothetical nervous system called a perceptron. The theory serves as a bridge between biophysics and psychology. It is possible to predict learning curves from neurological variables and vice versa. The quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems. 18 references. (PsycINFO Database Record (c) 2006 APA, all rights reserved). {\textcopyright} 1958 American Psychological Association.},
author = {Rosenblatt, F.},
doi = {10.1037/h0042519},
issn = {0033295X},
journal = {Psychol. Rev.},
keywords = {PERCEPTION, AS INFORMATION STORAGE MODEL INFORMATI},
month = {nov},
number = {6},
pages = {386--408},
pmid = {13602029},
title = {{The perceptron: A probabilistic model for information storage and organization in the brain}},
volume = {65},
year = {1958}
}
@inproceedings{Ruder2019,
author = {Ruder, Sebastian and Peters, Matthew E and Swayamdipta, Swabha and Wolf, Thomas},
booktitle = {Proc. 2019 Conf. North Am. Chapter Assoc. Comput. Linguist. Tutorials},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Ruder, Breslin, Ghaffari - 2019 - Neural Transfer Learning for Natural Language Processing.pdf:pdf},
pages = {15--18},
title = {{Neural Transfer Learning for Natural Language Processing}},
year = {2019}
}
@article{Kitaev2018,
abstract = {We show that constituency parsing benefits from unsupervised pre-training across a variety of languages and a range of pre-training conditions. We first compare the benefits of no pre-training, fastText, ELMo, and BERT for English and find that BERT outperforms ELMo, in large part due to increased model capacity, whereas ELMo in turn outperforms the non-contextual fastText embeddings. We also find that pre-training is beneficial across all 11 languages tested; however, large model sizes (more than 100 million parameters) make it computationally expensive to train separate models for each language. To address this shortcoming, we show that joint multilingual pre-training and fine-tuning allows sharing all but a small number of parameters between ten languages in the final model. The 10x reduction in model size compared to fine-tuning one model per language causes only a 3.2{\%} relative error increase in aggregate. We further explore the idea of joint fine-tuning and show that it gives low-resource languages a way to benefit from the larger datasets of other languages. Finally, we demonstrate new state-of-the-art results for 11 languages, including English (95.8 F1) and Chinese (91.8 F1).},
archivePrefix = {arXiv},
arxivId = {1812.11760},
author = {Kitaev, Nikita and Cao, Steven and Klein, Dan},
eprint = {1812.11760},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Kitaev, Cao, Klein - 2018 - Multilingual Constituency Parsing with Self-Attention and Pre-Training.pdf:pdf},
journal = {ACL 2019 - 57th Annu. Meet. Assoc. Comput. Linguist. Proc. Conf.},
month = {dec},
pages = {3499--3505},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Multilingual Constituency Parsing with Self-Attention and Pre-Training}},
url = {http://arxiv.org/abs/1812.11760},
year = {2018}
}
@article{Collobert2011,
abstract = {We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including: part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.},
archivePrefix = {arXiv},
arxivId = {1103.0398},
author = {Collobert, Ronan and Weston, Jason and Bottou, Leon and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel},
eprint = {1103.0398},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Collobert et al. - 2011 - Natural Language Processing (almost) from Scratch.pdf:pdf},
journal = {J. Mach. Learn. Res.},
keywords = {Natural language processing,Neural networks},
month = {mar},
pages = {2493--2537},
title = {{Natural Language Processing (almost) from Scratch}},
url = {http://arxiv.org/abs/1103.0398},
volume = {12},
year = {2011}
}
@techreport{Hochreiter1997,
abstract = {Learning to store information over extended time intervals via recurrent backpropagation takes a very long time, mostly due to insuucient, decaying error back ow. We brieey review Hochreiter's 1991 analysis of this problem, then address it by introducing a novel, eecient, gradient-based method called $\backslash$Long Short-Term Memory" (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete time steps by enforcing constant error ow through $\backslash$constant error carrousels" within special units. Multiplicative gate units learn to open and close access to the constant error ow. LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artiicial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with RTRL, BPTT, Recurrent Cascade-Correlation, Elman nets, and Neural Sequence Chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artiicial long time lag tasks that have never been solved by previous recurrent network algorithms.},
author = {Hochreiter, Sepp and {Urgen Schmidhuber}, J J},
booktitle = {Mem. Neural Comput.},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Hochreiter, Urgen Schmidhuber - 1997 - Long short-term memory.pdf:pdf},
number = {8},
pages = {1735--1780},
title = {{Long short-term memory}},
url = {http://www7.informatik.tu-muenchen.de/{~}hochreithttp://www.idsia.ch/{~}juergen},
volume = {9},
year = {1997}
}
@techreport{Allen19,
abstract = {Recent developments in neural algorithms provide a new approach to natural language processing. Two sets of brief studies show how networks may be developed for processing simple demonstratives and analogies. Two longer studies consider pronoun reference and natural language translation. Taken together, the studies provide additional support for the applicability of these algorithms to natural language processing.},
author = {Allen, Robert B},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Allen - Unknown - Several Studies on Natural Language {\textperiodcentered} and Back-Propagation.pdf:pdf},
keywords = {transformers},
mendeley-tags = {transformers},
title = {{Several Studies on Natural Language {\textperiodcentered} and Back-Propagation}},
year = {1987}
}
@article{Tenney2019,
abstract = {Pre-trained text encoders have rapidly advanced the state of the art on many NLP tasks. We focus on one such model, BERT, and aim to quantify where linguistic information is captured within the network. We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lower-level decisions on the basis of disambiguating information from higher-level representations.},
archivePrefix = {arXiv},
arxivId = {1905.05950},
author = {Tenney, Ian and Das, Dipanjan and Pavlick, Ellie},
eprint = {1905.05950},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Tenney, Das, Pavlick - 2019 - BERT Rediscovers the Classical NLP Pipeline.pdf:pdf},
journal = {ACL 2019 - 57th Annu. Meet. Assoc. Comput. Linguist. Proc. Conf.},
keywords = {necitovane},
mendeley-tags = {necitovane},
month = {may},
pages = {4593--4601},
publisher = {Association for Computational Linguistics (ACL)},
title = {{BERT Rediscovers the Classical NLP Pipeline}},
url = {http://arxiv.org/abs/1905.05950},
year = {2019}
}
@article{Pan2009,
abstract = {A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classification, regression and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multi-task learning and sample selection bias, as well as co-variate shift. We also explore some potential future issues in transfer learning research.},
author = {Pan, Sinno Jialin and Yang, Qiang},
doi = {10.1109/TKDE.2009.191},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Pan, Yang - 2009 - A Survey on Transfer Learning.pdf:pdf},
keywords = {Data Mining,Index Terms-Transfer Learning,Machine Learning,Survey},
title = {{A Survey on Transfer Learning}},
url = {http://socrates.acadiau.ca/courses/comp/dsilver/NIPS95},
year = {2009}
}
@article{Taylor1953,
abstract = {{\textless}p{\textgreater}Here is the first comprehensive statement of a research method and its theory which were introduced briefly during a workshop at the 1953 AEJ convention. Included are findings from three pilot studies and two experiments in which “cloze procedure” results are compared with those of two readability formulas.{\textless}/p{\textgreater}},
author = {Taylor, Wilson L.},
doi = {10.1177/107769905303000401},
issn = {0022-5533},
journal = {Journal. Q.},
month = {sep},
number = {4},
pages = {415--433},
publisher = {SAGE Publications},
title = {{“Cloze Procedure”: A New Tool for Measuring Readability}},
url = {http://journals.sagepub.com/doi/10.1177/107769905303000401},
volume = {30},
year = {1953}
}
@techreport{Schwenk2006,
abstract = {Statistical machine translation systems are based on one or more translation models and a language model of the target language. While many different translation models and phrase extraction algorithms have been proposed, a standard word n-gram back-off language model is used in most systems. In this work, we propose to use a new statistical language model that is based on a continuous representation of the words in the vocabulary. A neural network is used to perform the projection and the probability estimation. We consider the translation of European Parliament Speeches. This task is part of an international evaluation organized by the TC-STAR project in 2006. The proposed method achieves consistent improvements in the BLEU score on the development and test data. We also present algorithms to improve the estimation of the language model probabilities when splitting long sentences into shorter chunks.},
author = {Schwenk, Holger and Dchelotte, Daniel and Gauvain, Jean-Luc},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Schwenk, Dchelotte, Gauvain - 2006 - Continuous Space Language Models for Statistical Machine Translation.pdf:pdf},
keywords = {modeling},
mendeley-tags = {modeling},
pages = {723--730},
title = {{Continuous Space Language Models for Statistical Machine Translation}},
year = {2006}
}
@article{Hoerl1970,
abstract = {In multiple regression it is shown that parameter estimates based on minimum residual sum of squares have a high probability of being unsatisfactory, if not incorrect, if the prediction vectors are not orthogonal. Proposed is an estimation procedure based on adding small positive quantities to the diagonal of X′X. Introduced is the ridge trace, a method for showing in two dimensions the effects of nonorthogonality. It is then shown how to augment X′X to obtain biased estimates with smaller mean square error. {\textcopyright} 1970 Taylor and Francis Group, LLC.},
author = {Hoerl, Arthur E. and Kennard, Robert W.},
doi = {10.1080/00401706.1970.10488634},
issn = {15372723},
journal = {Technometrics},
number = {1},
pages = {55--67},
title = {{Ridge Regression: Biased Estimation for Nonorthogonal Problems}},
volume = {12},
year = {1970}
}
@techreport{Turian2010,
abstract = {If we take an existing supervised NLP system , a simple and general way to improve accuracy is to use unsupervised word representations as extra word features. We evaluate Brown clusters, Collobert and Weston (2008) embeddings, and HLBL (Mnih {\&} Hinton, 2009) embeddings of words on both NER and chunking. We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accuracy of these baselines. We find further improvements by combining different word representations. You can download our word features, for off-the-shelf use in existing NLP systems, as well as our code, here: http://metaoptimize. com/projects/wordreprs/},
author = {Turian, Joseph and Ratinov, Lev and Bengio, Yoshua},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Turian, Ratinov, Bengio - 2010 - Word representations A simple and general method for semi-supervised learning.pdf:pdf},
pages = {11--16},
publisher = {Association for Computational Linguistics},
title = {{Word representations: A simple and general method for semi-supervised learning}},
url = {http://metaoptimize.},
year = {2010}
}
@techreport{Bengio2003,
abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.},
author = {Bengio, Yoshua and Ducharme, R{\'{e}}jean and Vincent, Pascal and Jauvin, Christian and Ca, Jauvinc@iro Umontreal and Kandola, Jaz and Hofmann, Thomas and Poggio, Tomaso and Shawe-Taylor, John},
booktitle = {J. Mach. Learn. Res.},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Bengio et al. - 2003 - A Neural Probabilistic Language Model.pdf:pdf},
keywords = {Statistical language modeling,artificial neural networks,curse of dimensionality,distributed representation,embeddings},
mendeley-tags = {embeddings},
pages = {1137--1155},
title = {{A Neural Probabilistic Language Model}},
volume = {3},
year = {2003}
}
@article{Chronopoulou2019,
abstract = {A growing number of state-of-the-art transfer learning methods employ language models pretrained on large generic corpora. In this paper we present a conceptually simple and effective transfer learning approach that addresses the problem of catastrophic forgetting. Specifically, we combine the task-specific optimization function with an auxiliary language model objective, which is adjusted during the training process. This preserves language regularities captured by language models, while enabling sufficient adaptation for solving the target task. Our method does not require pretraining or finetuning separate components of the network and we train our models end-to-end in a single step. We present results on a variety of challenging affective and text classification tasks, surpassing well established transfer learning methods with greater level of complexity.},
archivePrefix = {arXiv},
arxivId = {1902.10547},
author = {Chronopoulou, Alexandra and Baziotis, Christos and Potamianos, Alexandros},
eprint = {1902.10547},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Chronopoulou, Baziotis, Potamianos - 2019 - An Embarrassingly Simple Approach for Transfer Learning from Pretrained Language Models.pdf:pdf},
journal = {NAACL HLT 2019 - 2019 Conf. North Am. Chapter Assoc. Comput. Linguist. Hum. Lang. Technol. - Proc. Conf.},
keywords = {moznosti,necitovane},
mendeley-tags = {moznosti,necitovane},
month = {feb},
pages = {2089--2095},
publisher = {Association for Computational Linguistics (ACL)},
title = {{An Embarrassingly Simple Approach for Transfer Learning from Pretrained Language Models}},
url = {http://arxiv.org/abs/1902.10547},
volume = {1},
year = {2019}
}
@techreport{Cheng,
abstract = {In this paper we address the question of how to render sequence-level networks better at handling structured input. We propose a machine reading simulator which processes text incrementally from left to right and performs shallow reasoning with memory and attention. The reader extends the Long Short-Term Memory architecture with a memory network in place of a single memory cell. This enables adaptive memory usage during recurrence with neural attention, offering a way to weakly induce relations among tokens. The system is initially designed to process a single sequence but we also demonstrate how to integrate it with an encoder-decoder architecture. Experiments on language modeling, sentiment analysis, and natural language inference show that our model matches or outperforms the state of the art.},
archivePrefix = {arXiv},
arxivId = {1601.06733v7},
author = {Cheng, Jianpeng and Dong, Li and Lapata, Mirella},
booktitle = {arxiv.org},
eprint = {1601.06733v7},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Cheng, Dong, Lapata - Unknown - Long Short-Term Memory-Networks for Machine Reading.pdf:pdf},
title = {{Long Short-Term Memory-Networks for Machine Reading}},
url = {https://arxiv.org/abs/1601.06733},
year = {2016}
}
@techreport{Rumelhart,
author = {Rumelhart, DE and Hinton, GE and Nature, RJ Williams - and 1986, Undefined},
booktitle = {nature.com},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Rumelhart et al. - Unknown - Learning representations by back-propagating errors.pdf:pdf},
title = {{Learning representations by back-propagating errors}},
url = {https://www.nature.com/articles/323533a0},
year = {1986}
}
@techreport{Wilks,
abstract = {The article surveys fifty years of work in computational language processing and machine translation, and suggests that a great number of the important ideas were present in the earliest days and hampered only back lack of computational power. Sections review the influence of linguistics proper on the computational area, as well as the influence of artificial intelligence and concerns from logic and knowledge representation. Later, corpora and machine readable dictionaries were made available, which in turn made possible the recent statistically-based empirical emphasis in the subject, a trend that began in machine translation under the influence of success in automatic speech processing. Finally, it is suggested that, despite these many influences on the field from outside, there is nonetheless a distinctive process-based computational linguistics and examples are suggested. Keywords: machine translation information retrieval information extraction parsing thesaurus beliefs syntactic structures semantic representations logic statistics question answering summarization psychology word-sense part-of-speech-tagging sense and reference performance case grammar agents computational semantics},
author = {Wilks, Yorick},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Wilks - Unknown - The History of Natural Language Processing and Machine Translation(2).pdf:pdf},
title = {{The History of Natural Language Processing and Machine Translation}},
year = {2005}
}
@inproceedings{Forcada1997,
abstract = {This paper presents a modification of Pollack's RAAM (Recursive Auto-Associative Memory), called a Recursive Hetero-Associative Memory (RHAM), and shows that it is capable of learning simple translation tasks, by building a state-space representation of each input string and unfolding it to obtain the corresponding output string. RHAM-based translators are computationally more powerful and easier to train than their corresponding double-RAAM counterparts in the literature.},
author = {Forcada, Mikel L. and {\~{N}}eco, Ram{\'{o}}n P.},
booktitle = {Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics)},
doi = {10.1007/bfb0032504},
isbn = {3540630473},
issn = {16113349},
keywords = {transformers},
mendeley-tags = {transformers},
pages = {453--462},
publisher = {Springer Verlag},
title = {{Recursive hetero-Associative memories for translation}},
url = {https://link.springer.com/chapter/10.1007/BFb0032504},
volume = {1240 LNCS},
year = {1997}
}
@techreport{Huh,
abstract = {The tremendous success of ImageNet-trained deep features on a wide range of transfer tasks raises the question: what is it about the ImageNet dataset that makes the learnt features as good as they are? This work provides an empirical investigation into the various facets of this question, such as, looking at the importance of the amount of examples , number of classes, balance between images-per-class and classes, and the role of fine and coarse grained recognition. We pre-train CNN features on various subsets of the ImageNet dataset and evaluate transfer performance on a variety of standard vision tasks. Our overall findings suggest that most changes in the choice of pre-training data long thought to be critical, do not significantly affect transfer performance.},
archivePrefix = {arXiv},
arxivId = {1608.08614v2},
author = {Huh, Minyoung and Agrawal, Pulkit and Efros, Alexei A},
eprint = {1608.08614v2},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Huh, Agrawal, Efros - Unknown - What makes ImageNet good for transfer learning.pdf:pdf},
title = {{What makes ImageNet good for transfer learning?}}
}
@article{Yang2019,
abstract = {With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.},
archivePrefix = {arXiv},
arxivId = {1906.08237},
author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V.},
eprint = {1906.08237},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Yang et al. - 2019 - XLNet Generalized Autoregressive Pretraining for Language Understanding.pdf:pdf},
journal = {arXiv},
month = {jun},
publisher = {arXiv},
title = {{XLNet: Generalized Autoregressive Pretraining for Language Understanding}},
url = {http://arxiv.org/abs/1906.08237},
year = {2019}
}
@article{Sutskever2014,
abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
archivePrefix = {arXiv},
arxivId = {1409.3215},
author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
eprint = {1409.3215},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Sutskever, Vinyals, Le - 2014 - Sequence to Sequence Learning with Neural Networks.pdf:pdf},
journal = {Adv. Neural Inf. Process. Syst.},
keywords = {transformers},
mendeley-tags = {transformers},
month = {sep},
number = {January},
pages = {3104--3112},
publisher = {Neural information processing systems foundation},
title = {{Sequence to Sequence Learning with Neural Networks}},
url = {http://arxiv.org/abs/1409.3215},
volume = {4},
year = {2014}
}
@techreport{Plisson,
abstract = {Lemmatization is the process of finding the normalized form of a word. It is the same as looking for a transformation to apply on a word to get its normalized form. The approach presented in this paper focuses on word endings: what word suffix should be removed and/or added to get the normalized form. This paper compares the results of two word lemmatization algorithms, one based on if-then rules and the other based on ripple down rules induction algorithms. It presents the problem of lemmatization of words from Slovene free text and explains why the Ripple Down Rules (RDR) approach is very well suited for the task. When learning from a corpus of lemmatized Slovene words the RDR approach results in easy to understand rules of improved classification accuracy compared to the results of rule learning achieved in previous work.},
author = {Plisson, Jo{\"{e}}l and Lavrac, Nada and Mladenic, Dunja},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Plisson, Lavrac, Mladenic - Unknown - A Rule based Approach to Word Lemmatization.pdf:pdf},
keywords = {lemmatization,necitovane},
mendeley-tags = {lemmatization,necitovane},
title = {{A Rule based Approach to Word Lemmatization}}
}
@book{Mitchell1997,
author = {Mitchell, Tom M.},
edition = {McGraw-Hil},
pages = {99},
publisher = {New York : McGraw-Hill},
title = {{Machine Learning}},
year = {1997}
}
@article{Russakovsky2015,
abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.},
archivePrefix = {arXiv},
arxivId = {1409.0575},
author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
doi = {10.1007/s11263-015-0816-y},
eprint = {1409.0575},
issn = {15731405},
journal = {Int. J. Comput. Vis.},
keywords = {Benchmark,Dataset,Large-scale,Object detection,Object recognition},
month = {dec},
number = {3},
pages = {211--252},
publisher = {Springer New York LLC},
title = {{ImageNet Large Scale Visual Recognition Challenge}},
url = {https://link.springer.com/article/10.1007/s11263-015-0816-y},
volume = {115},
year = {2015}
}
@article{Tibshirani1996,
abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
author = {Tibshirani, Robert},
doi = {10.1111/j.2517-6161.1996.tb02080.x},
issn = {0035-9246},
journal = {J. R. Stat. Soc. Ser. B},
keywords = {quadratic programming,regression,shrinkage,subset selection},
month = {jan},
number = {1},
pages = {267--288},
publisher = {Wiley},
title = {{Regression Shrinkage and Selection Via the Lasso}},
url = {https://rss.onlinelibrary.wiley.com/doi/full/10.1111/j.2517-6161.1996.tb02080.x https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1996.tb02080.x https://rss.onlinelibrary.wiley.com/doi/10.1111/j.2517-6161.1996.tb02080.x},
volume = {58},
year = {1996}
}
@article{Minsky2017,
author = {Minsky, M and Papert, SA},
title = {{Perceptrons: An introduction to computational geometry}},
url = {https://www.google.com/books?hl=cs{\&}lr={\&}id=PLQ5DwAAQBAJ{\&}oi=fnd{\&}pg=PR5{\&}dq=perceptrons+an+introduction+to+computational+geometry{\&}ots=zzCzAMspY0{\&}sig=x0HLubdLNN3Irw4cZUlmdyHK8BM},
year = {2017}
}
@inproceedings{Hewitt2020,
abstract = {Probes, supervised models trained to predict properties (like parts-of-speech) from representations (like ELMo), have achieved high accuracy on a range of linguistic tasks. But does this mean that the representations encode linguistic structure or just that the probe has learned the linguistic task? In this paper, we propose control tasks, which associate word types with random outputs, to complement linguistic tasks. By construction, these tasks can only be learned by the probe itself. So a good probe, (one that reflects the representation), should be selective, achieving high linguistic task accuracy and low control task accuracy. The selectivity of a probe puts linguistic task accuracy in context with the probe's capacity to memorize from word types. We construct control tasks for English part-of-speech tagging and dependency edge prediction, and show that popular probes on ELMo representations are not selective. We also find that dropout, commonly used to control probe complexity, is ineffective for improving selectivity of MLPs, but that other forms of regularization are effective. Finally, we find that while probes on the first layer of ELMo yield slightly better part-of-speech tagging accuracy than the second, probes on the second layer are substantially more selective, which raises the question of which layer better represents parts-of-speech.},
archivePrefix = {arXiv},
arxivId = {1909.03368},
author = {Hewitt, John and Liang, Percy},
booktitle = {EMNLP-IJCNLP 2019 - 2019 Conf. Empir. Methods Nat. Lang. Process. 9th Int. Jt. Conf. Nat. Lang. Process. Proc. Conf.},
doi = {10.18653/v1/d19-1275},
eprint = {1909.03368},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Hewitt, Liang - 2020 - Designing and interpreting probes with control tasks.pdf:pdf},
isbn = {9781950737901},
pages = {2733--2743},
publisher = {Association for Computational Linguistics},
title = {{Designing and interpreting probes with control tasks}},
year = {2020}
}
@article{Marcus1993,
author = {Marcus, Mitchell and Santorini, Beatrice and Marcinkiewicz, Mary Ann},
journal = {Tech. Reports},
month = {oct},
title = {{Building a Large Annotated Corpus of English: The Penn Treebank}},
url = {https://repository.upenn.edu/cis{\_}reports/237},
year = {1993}
}
@techreport{McCulloch,
author = {McCulloch, WS and bulletin of mathematical Biophysics, W Pitts - The and 1943, Undefined},
booktitle = {Springer},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/McCulloch, biophysics, 1943 - Unknown - A logical calculus of the ideas immanent in nervous activity.pdf:pdf},
keywords = {perceptron},
mendeley-tags = {perceptron},
title = {{A logical calculus of the ideas immanent in nervous activity}},
url = {https://link.springer.com/article/10.1007{\%}252FBF02478259}
}
@techreport{Brown,
abstract = {In this paper, we present a statistical approach to machine translation. We describe the application of our approach to translation from French to English and give preliminary results.},
author = {Brown, Peter F and Cocke, John and {Della Pietra}, Stephen A and {Della Pietra}, Vincent J and Jelinek, Fredrick and Lafferty, John D and Mercer, Robert L and Roossin, Paul S},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Brown et al. - Unknown - A STATISTICAL APPROACH TO MACHINE TRANSLATION.pdf:pdf},
title = {{A STATISTICAL APPROACH TO MACHINE TRANSLATION}},
year = {1990}
}
@article{Santos2016a,
abstract = {In this work, we propose Attentive Pooling (AP), a two-way attention mechanism for discriminative model training. In the context of pair-wise ranking or classification with neural networks, AP enables the pooling layer to be aware of the current input pair, in a way that information from the two input items can directly influence the computation of each other's representations. Along with such representations of the paired inputs, AP jointly learns a similarity measure over projected segments (e.g. trigrams) of the pair, and subsequently, derives the corresponding attention vector for each input to guide the pooling. Our two-way attention mechanism is a general framework independent of the underlying representation learning, and it has been applied to both convolutional neural networks (CNNs) and recurrent neural networks (RNNs) in our studies. The empirical results, from three very different benchmark tasks of question answering/answer selection, demonstrate that our proposed models outperform a variety of strong baselines and achieve state-of-the-art performance in all the benchmarks.},
archivePrefix = {arXiv},
arxivId = {1602.03609},
author = {dos Santos, Cicero and Tan, Ming and Xiang, Bing and Zhou, Bowen},
eprint = {1602.03609},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Santos et al. - 2016 - Attentive Pooling Networks(2).pdf:pdf},
keywords = {necitovane},
mendeley-tags = {necitovane},
month = {feb},
title = {{Attentive Pooling Networks}},
url = {http://arxiv.org/abs/1602.03609},
year = {2016}
}
@techreport{Szegedy2015,
abstract = {We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the Im-ageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular in-carnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
booktitle = {cv-foundation.org},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Szegedy et al. - Unknown - Going Deeper with Convolutions.pdf:pdf},
keywords = {labelsmoothing},
mendeley-tags = {labelsmoothing},
title = {{Going Deeper with Convolutions}},
url = {https://www.cv-foundation.org/openaccess/content{\_}cvpr{\_}2015/html/Szegedy{\_}Going{\_}Deeper{\_}With{\_}2015{\_}CVPR{\_}paper.html},
year = {2015}
}
@inproceedings{Ramachandran2017,
abstract = {This work presents a general unsupervised learning method to improve the accuracy of sequence to sequence (seq2seq) models. In our method, the weights of the encoder and decoder of a seq2seq model are initialized with the pretrained weights of two language models and then fine-tuned with labeled data. We apply this method to challenging benchmarks in machine translation and abstractive summarization and find that it significantly improves the subsequent supervised models. Our main result is that pretraining improves the generalization of seq2seq models. We achieve state-of-the-art results on the WMT English→German task, surpassing a range of methods using both phrase-based machine translation and neural machine translation. Our method achieves a significant improvement of 1.3 BLEU from the previous best models on both WMT'14 and WMT'15 English→German. We also conduct human evaluations on abstractive summarization and find that our method outperforms a purely supervised learning baseline in a statistically significant manner.},
archivePrefix = {arXiv},
arxivId = {1611.02683},
author = {Ramachandran, Prajit and Liu, Peter J. and Le, Quoc V.},
booktitle = {EMNLP 2017 - Conf. Empir. Methods Nat. Lang. Process. Proc.},
doi = {10.18653/v1/d17-1039},
eprint = {1611.02683},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Ramachandran, Liu, Le - 2017 - Unsupervised pretraining for sequence to sequence learning.pdf:pdf},
isbn = {9781945626838},
pages = {383--391},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Unsupervised pretraining for sequence to sequence learning}},
year = {2017}
}
@article{Liu2020,
abstract = {Contextual embeddings, such as ELMo and BERT, move beyond global word representations like Word2Vec and achieve ground-breaking performance on a wide range of natural language processing tasks. Contextual embeddings assign each word a representation based on its context, thereby capturing uses of words across varied contexts and encoding knowledge that transfers across languages. In this survey, we review existing contextual embedding models, cross-lingual polyglot pre-training, the application of contextual embeddings in downstream tasks, model compression, and model analyses.},
archivePrefix = {arXiv},
arxivId = {2003.07278},
author = {Liu, Qi and Kusner, Matt J. and Blunsom, Phil},
eprint = {2003.07278},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Liu, Kusner, Blunsom - 2020 - A Survey on Contextual Embeddings.pdf:pdf},
journal = {arXiv},
month = {mar},
publisher = {arXiv},
title = {{A Survey on Contextual Embeddings}},
url = {http://arxiv.org/abs/2003.07278},
year = {2020}
}
@techreport{Russell1995,
author = {Russell, Stuart J and Norvig, Peter and Canny, John F and Malik, Jitendra M and Edwards, Douglas D},
isbn = {0131038052},
pages = {529},
title = {{Artificial Intelligence A Modern Approach}},
year = {1995}
}
@techreport{Mikolov2013,
abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
booktitle = {papers.nips.cc},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Mikolov et al. - Unknown - Distributed Representations of Words and Phrases and their Compositionality.pdf:pdf},
title = {{Distributed Representations of Words and Phrases and their Compositionality}},
url = {http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and},
year = {2013}
}
@techreport{Hutchins1996,
author = {Hutchins, John},
booktitle = {books.google.com},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Hutchins - 1996 - ALPAC the (in)famous report.pdf:pdf},
pages = {131--135},
publisher = {The MIT Press},
title = {{ALPAC: the (in)famous report}},
url = {https://books.google.com/books?hl=cs{\&}lr={\&}id=yx3lEVJMBmMC{\&}oi=fnd{\&}pg=PA131{\&}dq=alpac+report{\&}ots=se2vhONMHp{\&}sig=ByL2IgJLxRwF3f6n9bqOPFx88r4},
volume = {14},
year = {1996}
}
@techreport{Ling,
abstract = {We introduce a model for constructing vector representations of words by composing characters using bidirectional LSTMs. Relative to traditional word representation models that have independent vectors for each word type, our model requires only a single vector per character type and a fixed set of parameters for the compositional model. Despite the compactness of this model and, more importantly, the arbitrary nature of the form-function relationship in language , our "composed" word representations yield state-of-the-art results in language modeling and part-of-speech tagging. Benefits over traditional baselines are particularly pronounced in morphologically rich languages (e.g., Turkish).},
archivePrefix = {arXiv},
arxivId = {1508.02096v2},
author = {Ling, Wang and Lu{\'{i}}s, Tiago and Marujo, Lu{\'{i}}s and Fernandez, Ram{\'{o}}n and Amir, Astudillo Silvio and Dyer, Chris and Black, Alan W and Trancoso, Isabel},
eprint = {1508.02096v2},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Ling et al. - Unknown - Finding Function in Form Compositional Character Models for Open Vocabulary Word Representation(2).pdf:pdf},
keywords = {embeddings},
mendeley-tags = {embeddings},
title = {{Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation}},
year = {2016}
}
@techreport{Goldberg,
author = {Goldberg, Yoav},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Goldberg - Unknown - A Primer on Neural Network Models for Natural Language Processing.pdf:pdf},
keywords = {modeling,necitovane},
mendeley-tags = {modeling,necitovane},
title = {{A Primer on Neural Network Models for Natural Language Processing}},
url = {http://www.cs.biu.}
}
@article{Santos2016,
abstract = {In this work, we propose Attentive Pooling (AP), a two-way attention mechanism for discriminative model training. In the context of pair-wise ranking or classification with neural networks, AP enables the pooling layer to be aware of the current input pair, in a way that information from the two input items can directly influence the computation of each other's representations. Along with such representations of the paired inputs, AP jointly learns a similarity measure over projected segments (e.g. trigrams) of the pair, and subsequently, derives the corresponding attention vector for each input to guide the pooling. Our two-way attention mechanism is a general framework independent of the underlying representation learning, and it has been applied to both convolutional neural networks (CNNs) and recurrent neural networks (RNNs) in our studies. The empirical results, from three very different benchmark tasks of question answering/answer selection, demonstrate that our proposed models outperform a variety of strong baselines and achieve state-of-the-art performance in all the benchmarks.},
archivePrefix = {arXiv},
arxivId = {1602.03609},
author = {dos Santos, Cicero and Tan, Ming and Xiang, Bing and Zhou, Bowen},
eprint = {1602.03609},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Santos et al. - 2016 - Attentive Pooling Networks.pdf:pdf},
month = {feb},
title = {{Attentive Pooling Networks}},
url = {http://arxiv.org/abs/1602.03609},
year = {2016}
}
@article{Wu2016,
abstract = {Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units ("wordpieces") for both input and output. This method provides a good balance between the flexibility of "character"-delimited models and the efficiency of "word"-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60{\%} compared to Google's phrase-based production system.},
archivePrefix = {arXiv},
arxivId = {1609.08144},
author = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V. and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and Klingner, Jeff and Shah, Apurva and Johnson, Melvin and Liu, Xiaobing and Kaiser, {\L}ukasz and Gouws, Stephan and Kato, Yoshikiyo and Kudo, Taku and Kazawa, Hideto and Stevens, Keith and Kurian, George and Patil, Nishant and Wang, Wei and Young, Cliff and Smith, Jason and Riesa, Jason and Rudnick, Alex and Vinyals, Oriol and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
eprint = {1609.08144},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Wu et al. - 2016 - Google's Neural Machine Translation System Bridging the Gap between Human and Machine Translation.pdf:pdf},
keywords = {transformers},
mendeley-tags = {transformers},
month = {sep},
title = {{Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation}},
url = {http://arxiv.org/abs/1609.08144},
year = {2016}
}
@article{Raffel2019,
abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
archivePrefix = {arXiv},
arxivId = {1910.10683},
author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
eprint = {1910.10683},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Raffel et al. - 2019 - Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.pdf:pdf},
journal = {arXiv},
keywords = {attention-based models,deep learning,multi-task learning,natural language processing,transfer learning},
month = {oct},
pages = {1--67},
publisher = {arXiv},
title = {{Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer}},
url = {http://arxiv.org/abs/1910.10683},
volume = {21},
year = {2019}
}
@techreport{Pennington,
abstract = {Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic , but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global log-bilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word co-occurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful sub-structure, as evidenced by its performance of 75{\%} on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.},
author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Pennington, Socher, Manning - Unknown - GloVe Global Vectors for Word Representation.pdf:pdf},
title = {{GloVe: Global Vectors for Word Representation}},
url = {http://nlp.},
year = {2014}
}
@inproceedings{Cho2014,
abstract = {In this paper, we propose a novel neural network model called RNN Encoder- Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
archivePrefix = {arXiv},
arxivId = {1406.1078},
author = {Cho, Kyunghyun and {Van Merri{\"{e}}nboer}, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
booktitle = {EMNLP 2014 - 2014 Conf. Empir. Methods Nat. Lang. Process. Proc. Conf.},
doi = {10.3115/v1/d14-1179},
eprint = {1406.1078},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Cho et al. - 2014 - Learning phrase representations using RNN encoder-decoder for statistical machine translation.pdf:pdf},
isbn = {9781937284961},
keywords = {gru,transformers},
mendeley-tags = {gru,transformers},
month = {jun},
pages = {1724--1734},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Learning phrase representations using RNN encoder-decoder for statistical machine translation}},
url = {https://arxiv.org/abs/1406.1078v3},
year = {2014}
}
@inproceedings{Kingma2015,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P. and Ba, Jimmy Lei},
booktitle = {3rd Int. Conf. Learn. Represent. ICLR 2015 - Conf. Track Proc.},
eprint = {1412.6980},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Kingma, Ba - 2015 - Adam A method for stochastic optimization.pdf:pdf},
month = {dec},
publisher = {International Conference on Learning Representations, ICLR},
title = {{Adam: A method for stochastic optimization}},
url = {https://arxiv.org/abs/1412.6980v9},
year = {2015}
}
@inproceedings{Peters2018,
abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-Trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
archivePrefix = {arXiv},
arxivId = {1802.05365},
author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
booktitle = {NAACL HLT 2018 - 2018 Conf. North Am. Chapter Assoc. Comput. Linguist. Hum. Lang. Technol. - Proc. Conf.},
doi = {10.18653/v1/n18-1202},
eprint = {1802.05365},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Peters et al. - 2018 - Deep contextualized word representations.pdf:pdf},
isbn = {9781948087278},
month = {feb},
pages = {2227--2237},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Deep contextualized word representations}},
url = {http://allennlp.org/elmo},
volume = {1},
year = {2018}
}
@techreport{Dai2015,
abstract = {We present two approaches that use unlabeled data to improve sequence learning with recurrent networks. The first approach is to predict what comes next in a sequence, which is a conventional language model in natural language processing. The second approach is to use a sequence autoencoder, which reads the input sequence into a vector and predicts the input sequence again. These two algorithms can be used as a "pretraining" step for a later supervised sequence learning algorithm. In other words, the parameters obtained from the unsupervised step can be used as a starting point for other supervised training models. In our experiments, we find that long short term memory recurrent networks after being pretrained with the two approaches are more stable and generalize better. With pretraining, we are able to train long short term memory recurrent networks up to a few hundred timesteps, thereby achieving strong performance in many text classification tasks, such as IMDB, DBpedia and 20 Newsgroups.},
archivePrefix = {arXiv},
arxivId = {1511.01432v1},
author = {Dai, Andrew M and Le, Quoc V},
eprint = {1511.01432v1},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Dai, Le - 2015 - Semi-supervised Sequence Learning.pdf:pdf},
keywords = {()},
title = {{Semi-supervised Sequence Learning}},
url = {http://ai.stanford.edu/amaas/data/sentiment/index.html},
year = {2015}
}
@article{Vaswani2017,
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
author = {Vaswani, Ashish and Brain, Google and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Vaswani et al. - Unknown - Attention Is All You Need.pdf:pdf},
title = {{Attention Is All You Need}},
year = {2017}
}
@article{Yosinski2014,
abstract = {Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.},
archivePrefix = {arXiv},
arxivId = {1411.1792},
author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
eprint = {1411.1792},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Yosinski et al. - 2014 - How transferable are features in deep neural networks.pdf:pdf},
journal = {Adv. Neural Inf. Process. Syst.},
month = {nov},
number = {January},
pages = {3320--3328},
publisher = {Neural information processing systems foundation},
title = {{How transferable are features in deep neural networks?}},
url = {http://arxiv.org/abs/1411.1792},
volume = {4},
year = {2014}
}
