Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@techreport{Russell1995,
author = {Russell, Stuart J and Norvig, Peter and Canny, John F and Malik, Jitendra M and Edwards, Douglas D},
isbn = {0131038052},
pages = {529},
title = {{Artificial Intelligence A Modern Approach}},
year = {1995}
}
@techreport{Hana2005,
author = {Hana, Jiř{\'{i}} and Zeman, Daniel},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Hana, Zeman - 2005 - Manual for Morphological Annotation Revision for the Prague Dependency Treebank 2.0 ´ UFAL.pdf:pdf},
title = {{Manual for Morphological Annotation Revision for the Prague Dependency Treebank 2.0 UFAL}},
year = {2005}
}
@techreport{Rumelhart,
author = {Rumelhart, DE and Hinton, GE and Nature, RJ Williams - and 1986, Undefined},
booktitle = {nature.com},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Rumelhart et al. - Unknown - Learning representations by back-propagating errors.pdf:pdf},
title = {{Learning representations by back-propagating errors}},
url = {https://www.nature.com/articles/323533a0},
year = {1986}
}
@techreport{Hladka,
author = {Hladk{\'{a}}},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - Part of Speech Tags for Automatic Tagging and Syntactic Structures 1.pdf:pdf},
title = {{Part of Speech Tags for Automatic Tagging and Syntactic Structures 1}}
}
@article{Straka2018,
abstract = {UDPipe is a trainable pipeline which performs sentence segmentation, tokeniza-tion, POS tagging, lemmatization and dependency parsing (Straka et al., 2016). We present a prototype for UDPipe 2.0 and evaluate it in the CoNLL 2018 UD Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, which employs three metrics for submission ranking. Out of 26 participants, the prototype placed first in the MLAS ranking, third in the LAS ranking and third in the BLEX ranking. In extrinsic parser evaluation EPE 2018, the system ranked first in the overall score. The prototype utilizes an artificial neu-ral network with a single joint model for POS tagging, lemmatization and dependency parsing, and is trained only using the CoNLL-U training data and pretrained word embeddings, contrary to both systems surpassing the prototype in the LAS and BLEX ranking in the shared task. The open-source code of the prototype is available at http://github.com/ CoNLL-UD-2018/UDPipe-Future. After the shared task, we slightly refined the model architecture, resulting in better performance both in the intrinsic evaluation (corresponding to first, second and second rank in MLAS, LAS and BLEX shared task metrics) and the extrinsic evaluation. The improved models will be available shortly in UDPipe at},
author = {Straka, Milan},
doi = {10.18653/v1/K18-2020},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Straka - Unknown - UDPipe 2.0 Prototype at CoNLL 2018 UD Shared Task.pdf:pdf},
journal = {Proc. CoNLL 2018 Shar. Task Multiling. Parsing from Raw Text to Univers. Depend.},
pages = {197--207},
title = {{UDPipe 2.0 Prototype at CoNLL 2018 UD Shared Task}},
url = {http://ufal.mff.cuni.cz/udpipe.},
year = {2018}
}
@techreport{Plank,
abstract = {Bidirectional long short-term memory (bi-LSTM) networks have recently proven successful for various NLP sequence mod-eling tasks, but little is known about their reliance to input representations, target languages, data set size, and label noise. We address these issues and evaluate bi-LSTMs with word, character, and unicode byte embeddings for POS tagging. We compare bi-LSTMs to traditional POS taggers across languages and data sizes. We also present a novel bi-LSTM model, which combines the POS tagging loss function with an auxiliary loss function that accounts for rare words. The model obtains state-of-the-art performance across 22 languages, and works especially well for morphologically complex languages. Our analysis suggests that bi-LSTMs are less sensitive to training data size and label corruptions (at small noise levels) than previously assumed.},
archivePrefix = {arXiv},
arxivId = {1604.05529v3},
author = {Plank, Barbara and S{\o}gaard, Anders and Goldberg, Yoav},
eprint = {1604.05529v3},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Plank, S{\o}gaard, Goldberg - Unknown - Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary.pdf:pdf},
title = {{Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss}},
url = {https://github.com/clab/cnn}
}
@techreport{Schwenk2006,
abstract = {Statistical machine translation systems are based on one or more translation models and a language model of the target language. While many different translation models and phrase extraction algorithms have been proposed, a standard word n-gram back-off language model is used in most systems. In this work, we propose to use a new statistical language model that is based on a continuous representation of the words in the vocabulary. A neural network is used to perform the projection and the probability estimation. We consider the translation of European Parliament Speeches. This task is part of an international evaluation organized by the TC-STAR project in 2006. The proposed method achieves consistent improvements in the BLEU score on the development and test data. We also present algorithms to improve the estimation of the language model probabilities when splitting long sentences into shorter chunks.},
author = {Schwenk, Holger and Dchelotte, Daniel and Gauvain, Jean-Luc},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Schwenk, Dchelotte, Gauvain - 2006 - Continuous Space Language Models for Statistical Machine Translation.pdf:pdf},
pages = {723--730},
title = {{Continuous Space Language Models for Statistical Machine Translation}},
year = {2006}
}
@inproceedings{Forcada1997,
abstract = {This paper presents a modification of Pollack's RAAM (Recursive Auto-Associative Memory), called a Recursive Hetero-Associative Memory (RHAM), and shows that it is capable of learning simple translation tasks, by building a state-space representation of each input string and unfolding it to obtain the corresponding output string. RHAM-based translators are computationally more powerful and easier to train than their corresponding double-RAAM counterparts in the literature.},
author = {Forcada, Mikel L. and {\~{N}}eco, Ram{\'{o}}n P.},
booktitle = {Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics)},
doi = {10.1007/bfb0032504},
isbn = {3540630473},
issn = {16113349},
pages = {453--462},
publisher = {Springer Verlag},
title = {{Recursive hetero-Associative memories for translation}},
url = {https://link.springer.com/chapter/10.1007/BFb0032504},
volume = {1240 LNCS},
year = {1997}
}
@techreport{Brown,
abstract = {In this paper, we present a statistical approach to machine translation. We describe the application of our approach to translation from French to English and give preliminary results.},
author = {Brown, Peter F and Cocke, John and {Della Pietra}, Stephen A and {Della Pietra}, Vincent J and Jelinek, Fredrick and Lafferty, John D and Mercer, Robert L and Roossin, Paul S},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Brown et al. - Unknown - A STATISTICAL APPROACH TO MACHINE TRANSLATION.pdf:pdf},
title = {{A STATISTICAL APPROACH TO MACHINE TRANSLATION}},
year = {1990}
}
@techreport{Allen19,
abstract = {Recent developments in neural algorithms provide a new approach to natural language processing. Two sets of brief studies show how networks may be developed for processing simple demonstratives and analogies. Two longer studies consider pronoun reference and natural language translation. Taken together, the studies provide additional support for the applicability of these algorithms to natural language processing.},
author = {Allen, Robert B},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Allen - Unknown - Several Studies on Natural Language {\textperiodcentered} and Back-Propagation.pdf:pdf},
title = {{Several Studies on Natural Language {\textperiodcentered} and Back-Propagation}},
year = {1987}
}
@article{Sutskever2014,
abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
archivePrefix = {arXiv},
arxivId = {1409.3215},
author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
eprint = {1409.3215},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Sutskever, Vinyals, Le - 2014 - Sequence to Sequence Learning with Neural Networks.pdf:pdf},
journal = {Adv. Neural Inf. Process. Syst.},
month = {sep},
number = {January},
pages = {3104--3112},
publisher = {Neural information processing systems foundation},
title = {{Sequence to Sequence Learning with Neural Networks}},
url = {http://arxiv.org/abs/1409.3215},
volume = {4},
year = {2014}
}
@techreport{Szegedy2015,
abstract = {We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the Im-ageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular in-carnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
booktitle = {cv-foundation.org},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Szegedy et al. - Unknown - Going Deeper with Convolutions.pdf:pdf},
title = {{Going Deeper with Convolutions}},
url = {https://www.cv-foundation.org/openaccess/content{\_}cvpr{\_}2015/html/Szegedy{\_}Going{\_}Deeper{\_}With{\_}2015{\_}CVPR{\_}paper.html},
year = {2015}
}
@inproceedings{Arkhipov2019,
abstract = {Our paper addresses the problem of multilingual named entity recognition on the material of 4 languages: Russian, Bulgarian, Czech and Polish. We solve this task using the BERT model. We use a hundred languages multilingual model as base for transfer to the mentioned Slavic languages. Unsupervised pre-training of the BERT model on these 4 languages allows to significantly outperform baseline neural approaches and multilingual BERT. Additional improvement is achieved by extending BERT with a word-level CRF layer. Our system was submitted to BSNLP 2019 Shared Task on Multilingual Named Entity Recognition and took the 1st place in 3 competition metrics out of 4 we participated in. We open-sourced NER models and BERT model pre-trained on the four Slavic languages.},
author = {Arkhipov, Mikhail and Trofimova, Maria and Kuratov, Yuri and Sorokin, Alexey},
doi = {10.18653/v1/w19-3712},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Arkhipov et al. - 2019 - Tuning Multilingual Transformers for Language-Specific Named Entity Recognition.pdf:pdf},
month = {sep},
pages = {89--93},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Tuning Multilingual Transformers for Language-Specific Named Entity Recognition}},
url = {https://github.com/google-research/},
year = {2019}
}
@article{Clark2020,
abstract = {Masked language modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK] and then train a model to reconstruct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the original identities of the corrupted tokens, we train a discriminative model that predicts whether each token in the corrupted input was replaced by a generator sample or not. Thorough experiments demonstrate this new pre-training task is more efficient than MLM because the task is defined over all input tokens rather than just the small subset that was masked out. As a result, the contextual representations learned by our approach substantially outperform the ones learned by BERT given the same model size, data, and compute. The gains are particularly strong for small models; for example, we train a model on one GPU for 4 days that outperforms GPT (trained using 30x more compute) on the GLUE natural language understanding benchmark. Our approach also works well at scale, where it performs comparably to RoBERTa and XLNet while using less than 1/4 of their compute and outperforms them when using the same amount of compute.},
archivePrefix = {arXiv},
arxivId = {2003.10555},
author = {Clark, Kevin and Luong, Minh-Thang and Le, Quoc V. and Manning, Christopher D.},
eprint = {2003.10555},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Clark et al. - 2020 - ELECTRA Pre-training Text Encoders as Discriminators Rather Than Generators.pdf:pdf},
month = {mar},
title = {{ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators}},
url = {http://arxiv.org/abs/2003.10555},
year = {2020}
}
@article{Wu2016,
abstract = {Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units ("wordpieces") for both input and output. This method provides a good balance between the flexibility of "character"-delimited models and the efficiency of "word"-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60{\%} compared to Google's phrase-based production system.},
archivePrefix = {arXiv},
arxivId = {1609.08144},
author = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V. and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and Klingner, Jeff and Shah, Apurva and Johnson, Melvin and Liu, Xiaobing and Kaiser, {\L}ukasz and Gouws, Stephan and Kato, Yoshikiyo and Kudo, Taku and Kazawa, Hideto and Stevens, Keith and Kurian, George and Patil, Nishant and Wang, Wei and Young, Cliff and Smith, Jason and Riesa, Jason and Rudnick, Alex and Vinyals, Oriol and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
eprint = {1609.08144},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Wu et al. - 2016 - Google's Neural Machine Translation System Bridging the Gap between Human and Machine Translation.pdf:pdf},
month = {sep},
title = {{Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation}},
url = {http://arxiv.org/abs/1609.08144},
year = {2016}
}
