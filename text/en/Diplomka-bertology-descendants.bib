Automatically generated by Mendeley Desktop 1.19.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Yang2019a,
abstract = {With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.},
archivePrefix = {arXiv},
arxivId = {1906.08237},
author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V.},
eprint = {1906.08237},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Yang et al. - 2019 - XLNet Generalized Autoregressive Pretraining for Language Understanding(2).pdf:pdf},
journal = {arXiv},
month = {jun},
publisher = {arXiv},
title = {{XLNet: Generalized Autoregressive Pretraining for Language Understanding}},
url = {http://arxiv.org/abs/1906.08237},
year = {2019}
}
@article{Ganesh2020,
abstract = {Transformer-based models pre-trained on large-scale corpora achieve state-of-the-art accuracy for natural language processing tasks, but are too resource-hungry and compute-intensive to suit low-capability devices or applications with strict latency requirements. One potential remedy is model compression, which has attracted extensive attention. This paper summarizes the branches of research on compressing Transformers, focusing on the especially popular BERT model. BERT's complex architecture means that a compression technique that is highly effective on one part of the model, e.g., attention layers, may be less successful on another part, e.g., fully connected layers. In this systematic study, we identify the state of the art in compression for each part of BERT, clarify current best practices for compressing large-scale Transformer models, and provide insights into the inner workings of various methods. Our categorization and analysis also shed light on promising future research directions for achieving a lightweight, accurate, and generic natural language processing model.},
archivePrefix = {arXiv},
arxivId = {2002.11985},
author = {Ganesh, Prakhar and Chen, Yao and Lou, Xin and Khan, Mohammad Ali and Yang, Yin and Chen, Deming and Winslett, Marianne and Sajjad, Hassan and Nakov, Preslav},
eprint = {2002.11985},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Ganesh et al. - 2020 - Compressing Large-Scale Transformer-Based Models A Case Study on BERT.pdf:pdf},
journal = {arXiv},
month = {feb},
publisher = {arXiv},
title = {{Compressing Large-Scale Transformer-Based Models: A Case Study on BERT}},
url = {http://arxiv.org/abs/2002.11985},
year = {2020}
}
@article{Lewis2019,
abstract = {We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most influence end-task performance.},
archivePrefix = {arXiv},
arxivId = {1910.13461},
author = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
eprint = {1910.13461},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Lewis et al. - 2019 - BART Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension.pdf:pdf},
journal = {arXiv},
month = {oct},
publisher = {arXiv},
title = {{BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension}},
url = {http://arxiv.org/abs/1910.13461},
year = {2019}
}
@article{Dai2019,
abstract = {Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80{\%} longer than RNNs and 450{\%} longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.},
archivePrefix = {arXiv},
arxivId = {1901.02860},
author = {Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V. and Salakhutdinov, Ruslan},
eprint = {1901.02860},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Dai et al. - 2019 - Transformer-XL Attentive Language Models Beyond a Fixed-Length Context.pdf:pdf},
journal = {ACL 2019 - 57th Annu. Meet. Assoc. Comput. Linguist. Proc. Conf.},
month = {jan},
pages = {2978--2988},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context}},
url = {http://arxiv.org/abs/1901.02860},
year = {2019}
}
@article{Liu2019,
abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
archivePrefix = {arXiv},
arxivId = {1907.11692},
author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
eprint = {1907.11692},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Liu et al. - 2019 - RoBERTa A Robustly Optimized BERT Pretraining Approach(2).pdf:pdf},
journal = {arXiv},
keywords = {necitovane},
mendeley-tags = {necitovane},
month = {jul},
publisher = {arXiv},
title = {{RoBERTa: A Robustly Optimized BERT Pretraining Approach}},
url = {http://arxiv.org/abs/1907.11692},
year = {2019}
}
@article{Lan2019,
abstract = {Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and $\backslash$squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.},
archivePrefix = {arXiv},
arxivId = {1909.11942},
author = {Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
eprint = {1909.11942},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Lan et al. - 2019 - ALBERT A Lite BERT for Self-supervised Learning of Language Representations.pdf:pdf},
journal = {arXiv},
month = {sep},
publisher = {arXiv},
title = {{ALBERT: A Lite BERT for Self-supervised Learning of Language Representations}},
url = {http://arxiv.org/abs/1909.11942},
year = {2019}
}
@article{Zhang2019,
abstract = {Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding. We argue that informative entities in KGs can enhance language representation with external knowledge. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks. The source code of this paper can be obtained from https://github.com/thunlp/ERNIE.},
archivePrefix = {arXiv},
arxivId = {1905.07129},
author = {Zhang, Zhengyan and Han, Xu and Liu, Zhiyuan and Jiang, Xin and Sun, Maosong and Liu, Qun},
eprint = {1905.07129},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Zhang et al. - 2019 - ERNIE Enhanced Language Representation with Informative Entities(2).pdf:pdf},
journal = {ACL 2019 - 57th Annu. Meet. Assoc. Comput. Linguist. Proc. Conf.},
month = {may},
pages = {1441--1451},
publisher = {Association for Computational Linguistics (ACL)},
title = {{ERNIE: Enhanced Language Representation with Informative Entities}},
url = {http://arxiv.org/abs/1905.07129},
year = {2019}
}
@article{Raffel2019a,
abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
archivePrefix = {arXiv},
arxivId = {1910.10683},
author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
eprint = {1910.10683},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Raffel et al. - 2019 - Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer(3).pdf:pdf},
journal = {arXiv},
keywords = {T5,attention-based models,deep learning,multi-task learning,natural language processing,transfer learning},
mendeley-tags = {T5},
month = {oct},
pages = {1--67},
publisher = {arXiv},
title = {{Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer}},
url = {http://arxiv.org/abs/1910.10683},
volume = {21},
year = {2019}
}
@article{Clark2020,
abstract = {Masked language modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK] and then train a model to reconstruct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the original identities of the corrupted tokens, we train a discriminative model that predicts whether each token in the corrupted input was replaced by a generator sample or not. Thorough experiments demonstrate this new pre-training task is more efficient than MLM because the task is defined over all input tokens rather than just the small subset that was masked out. As a result, the contextual representations learned by our approach substantially outperform the ones learned by BERT given the same model size, data, and compute. The gains are particularly strong for small models; for example, we train a model on one GPU for 4 days that outperforms GPT (trained using 30x more compute) on the GLUE natural language understanding benchmark. Our approach also works well at scale, where it performs comparably to RoBERTa and XLNet while using less than 1/4 of their compute and outperforms them when using the same amount of compute.},
archivePrefix = {arXiv},
arxivId = {2003.10555},
author = {Clark, Kevin and Luong, Minh-Thang and Le, Quoc V. and Manning, Christopher D.},
eprint = {2003.10555},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Clark et al. - 2020 - ELECTRA Pre-training Text Encoders as Discriminators Rather Than Generators.pdf:pdf},
journal = {arXiv Prepr.},
keywords = {necitovane},
mendeley-tags = {necitovane},
month = {mar},
title = {{ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators}},
url = {http://arxiv.org/abs/2003.10555},
year = {2020}
}
@article{Dong2019,
abstract = {This paper presents a new Unified pre-trained Language Model (UniLM) that can be fine-tuned for both natural language understanding and generation tasks. The model is pre-trained using three types of language modeling tasks: unidirectional, bidirectional, and sequence-to-sequence prediction. The unified modeling is achieved by employing a shared Transformer network and utilizing specific self-attention masks to control what context the prediction conditions on. UniLM compares favorably with BERT on the GLUE benchmark, and the SQuAD 2.0 and CoQA question answering tasks. Moreover, UniLM achieves new state-of-the-art results on five natural language generation datasets, including improving the CNN/DailyMail abstractive summarization ROUGE-L to 40.51 (2.04 absolute improvement), the Gigaword abstractive summarization ROUGE-L to 35.75 (0.86 absolute improvement), the CoQA generative question answering F1 score to 82.5 (37.1 absolute improvement), the SQuAD question generation BLEU-4 to 22.12 (3.75 absolute improvement), and the DSTC7 document-grounded dialog response generation NIST-4 to 2.67 (human performance is 2.65). The code and pre-trained models are available at https://github.com/microsoft/unilm.},
archivePrefix = {arXiv},
arxivId = {1905.03197},
author = {Dong, Li and Yang, Nan and Wang, Wenhui and Wei, Furu and Liu, Xiaodong and Wang, Yu and Gao, Jianfeng and Zhou, Ming and Hon, Hsiao-Wuen},
eprint = {1905.03197},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Dong et al. - 2019 - Unified Language Model Pre-training for Natural Language Understanding and Generation.pdf:pdf},
journal = {arXiv},
month = {may},
publisher = {arXiv},
title = {{Unified Language Model Pre-training for Natural Language Understanding and Generation}},
url = {http://arxiv.org/abs/1905.03197},
year = {2019}
}
