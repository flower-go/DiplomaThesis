Automatically generated by Mendeley Desktop 1.19.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Yang2019a,
abstract = {With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.},
archivePrefix = {arXiv},
arxivId = {1906.08237},
author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V.},
eprint = {1906.08237},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Yang et al. - 2019 - XLNet Generalized Autoregressive Pretraining for Language Understanding(2).pdf:pdf},
journal = {arXiv},
month = {jun},
publisher = {arXiv},
title = {{XLNet: Generalized Autoregressive Pretraining for Language Understanding}},
url = {http://arxiv.org/abs/1906.08237},
year = {2019}
}
@article{Radfort2018,
abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9{\%} on commonsense reasoning (Stories Cloze Test), 5.7{\%} on question answering (RACE), and 1.5{\%} on textual entailment (MultiNLI). 1},
author = {Radfort, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
file = {:Users/petravysusilova/Downloads/radford2018improving.pdf:pdf},
journal = {OpenAI},
title = {{Improving Language Understanding by Generative Pre-Training}},
year = {2018}
}
@incollection{Sun,
abstract = {Language model pre-training has proven to be useful in learning universal language representations. As a state-of-the-art language model pre-training model, BERT (Bidirectional En-coder Representations from Transformers) has achieved amazing results in many language understanding tasks. In this paper, we conduct exhaustive experiments to investigate different fine-tuning methods of BERT on text classification task and provide a general solution for BERT fine-tuning. Finally, the proposed solution obtains new state-of-the-art results on eight widely-studied text classification datasets. 1},
archivePrefix = {arXiv},
arxivId = {1905.05583v3},
author = {Sun, Chi and Qiu, Xipeng and Xu, Yige and Huang, Xuanjing},
booktitle = {Chinese Comput. Linguist.},
eprint = {1905.05583v3},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Sun et al. - Unknown - How to Fine-Tune BERT for Text Classification.pdf:pdf},
keywords = {bertology,practise},
mendeley-tags = {bertology,practise},
pages = {194--206},
publisher = {Springer International Publishing},
title = {{How to Fine-Tune BERT for Text Classification?}},
url = {https://github.},
year = {2019}
}
@article{Yang2019b,
abstract = {Transformer-based pre-trained language models have proven to be effective for learning contextualized language representation. However, current approaches only take advantage of the output of the encoder's final layer when fine-tuning the downstream tasks. We argue that only taking single layer's output restricts the power of pre-trained representation. Thus we deepen the representation learned by the model by fusing the hidden representation in terms of an explicit HIdden Representation Extractor (HIRE), which automatically absorbs the complementary representation with respect to the output from the final layer. Utilizing RoBERTa as the backbone encoder, our proposed improvement over the pre-trained models is shown effective on multiple natural language understanding tasks and help our model rival with the state-of-the-art models on the GLUE benchmark.},
archivePrefix = {arXiv},
arxivId = {1911.01940},
author = {Yang, Junjie and Zhao, Hai},
eprint = {1911.01940},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Yang, Zhao - 2019 - Deepening Hidden Representations from Pre-trained Language Models.pdf:pdf},
journal = {arXiv},
month = {nov},
title = {{Deepening Hidden Representations from Pre-trained Language Models}},
url = {http://arxiv.org/abs/1911.01940},
year = {2019}
}
@article{Clark2020,
abstract = {Masked language modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK] and then train a model to reconstruct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the original identities of the corrupted tokens, we train a discriminative model that predicts whether each token in the corrupted input was replaced by a generator sample or not. Thorough experiments demonstrate this new pre-training task is more efficient than MLM because the task is defined over all input tokens rather than just the small subset that was masked out. As a result, the contextual representations learned by our approach substantially outperform the ones learned by BERT given the same model size, data, and compute. The gains are particularly strong for small models; for example, we train a model on one GPU for 4 days that outperforms GPT (trained using 30x more compute) on the GLUE natural language understanding benchmark. Our approach also works well at scale, where it performs comparably to RoBERTa and XLNet while using less than 1/4 of their compute and outperforms them when using the same amount of compute.},
archivePrefix = {arXiv},
arxivId = {2003.10555},
author = {Clark, Kevin and Luong, Minh-Thang and Le, Quoc V. and Manning, Christopher D.},
eprint = {2003.10555},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Clark et al. - 2020 - ELECTRA Pre-training Text Encoders as Discriminators Rather Than Generators.pdf:pdf},
journal = {arXiv Prepr.},
keywords = {necitovane},
mendeley-tags = {necitovane},
month = {mar},
title = {{ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators}},
url = {http://arxiv.org/abs/2003.10555},
year = {2020}
}
@article{Tenney2019a,
abstract = {Contextualized representation models such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018) have recently achieved state-of-the-art results on a diverse array of downstream NLP tasks. Building on recent token-level probing work, we introduce a novel edge probing task design and construct a broad suite of sub-sentence tasks derived from the traditional structured NLP pipeline. We probe word-level contextual representations from four recent models and investigate how they encode sentence structure across a range of syntactic, semantic, local, and long-range phenomena. We find that existing models trained on language modeling and translation produce strong representations for syntactic phenomena, but only offer comparably small improvements on semantic tasks over a non-contextual baseline.},
archivePrefix = {arXiv},
arxivId = {1905.06316},
author = {Tenney, Ian and Xia, Patrick and Chen, Berlin and Wang, Alex and Poliak, Adam and McCoy, R Thomas and Kim, Najoung and {Van Durme}, Benjamin and Bowman, Samuel R. and Das, Dipanjan and Pavlick, Ellie},
eprint = {1905.06316},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Tenney et al. - 2019 - What do you learn from context Probing for sentence structure in contextualized word representations.pdf:pdf},
journal = {arXiv},
month = {may},
publisher = {arXiv},
title = {{What do you learn from context? Probing for sentence structure in contextualized word representations}},
url = {http://arxiv.org/abs/1905.06316},
year = {2019}
}
@article{Wu2020,
abstract = {Multilingual BERT (mBERT) trained on 104 languages has shown surprisingly good cross-lingual performance on several NLP tasks, even without explicit cross-lingual signals. However, these evaluations have focused on cross-lingual transfer with high-resource languages, covering only a third of the languages covered by mBERT. We explore how mBERT performs on a much wider set of languages, focusing on the quality of representation for low-resource languages, measured by within-language performance. We consider three tasks: Named Entity Recognition (99 languages), Part-of-speech Tagging, and Dependency Parsing (54 languages each). mBERT does better than or comparable to baselines on high resource languages but does much worse for low resource languages. Furthermore, monolingual BERT models for these languages do even worse. Paired with similar languages, the performance gap between monolingual BERT and mBERT can be narrowed. We find that better models for low resource languages require more efficient pretraining techniques or more data.},
archivePrefix = {arXiv},
arxivId = {2005.09093},
author = {Wu, Shijie and Dredze, Mark},
eprint = {2005.09093},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Wu, Dredze - 2020 - Are All Languages Created Equal in Multilingual BERT.pdf:pdf},
keywords = {bertology,necitovane},
mendeley-tags = {bertology,necitovane},
month = {may},
pages = {120--130},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Are All Languages Created Equal in Multilingual BERT?}},
url = {http://arxiv.org/abs/2005.09093},
year = {2020}
}
@article{Rogers2020,
abstract = {Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model. We review the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue and approaches to compression. We then outline directions for future research.},
archivePrefix = {arXiv},
arxivId = {2002.12327},
author = {Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
eprint = {2002.12327},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Rogers, Kovaleva, Rumshisky - 2020 - A Primer in BERTology What we know about how BERT works(2).pdf:pdf},
journal = {arXiv},
month = {feb},
publisher = {arXiv},
title = {{A Primer in BERTology: What we know about how BERT works}},
url = {http://arxiv.org/abs/2002.12327},
year = {2020}
}
@misc{Brown2020,
abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art finetuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
booktitle = {arXiv},
issn = {23318422},
title = {{Language models are few-shot learners}},
year = {2020}
}
@article{Lan2019,
abstract = {Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and $\backslash$squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.},
archivePrefix = {arXiv},
arxivId = {1909.11942},
author = {Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
eprint = {1909.11942},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Lan et al. - 2019 - ALBERT A Lite BERT for Self-supervised Learning of Language Representations.pdf:pdf},
journal = {arXiv},
month = {sep},
publisher = {arXiv},
title = {{ALBERT: A Lite BERT for Self-supervised Learning of Language Representations}},
url = {http://arxiv.org/abs/1909.11942},
year = {2019}
}
@article{Michel2019,
abstract = {Attention is a powerful and ubiquitous mechanism for allowing neural models to focus on particular salient pieces of information by taking their weighted average when making predictions. In particular, multi-headed attention is a driving force behind many recent state-of-the-art NLP models such as Transformer-based MT models and BERT. These models apply multiple attention mechanisms in parallel, with each attention "head" potentially focusing on different parts of the input, which makes it possible to express sophisticated functions beyond the simple weighted average. In this paper we make the surprising observation that even if models have been trained using multiple heads, in practice, a large percentage of attention heads can be removed at test time without significantly impacting performance. In fact, some layers can even be reduced to a single head. We further examine greedy algorithms for pruning down models, and the potential speed, memory efficiency, and accuracy improvements obtainable therefrom. Finally, we analyze the results with respect to which parts of the model are more reliant on having multiple heads, and provide precursory evidence that training dynamics play a role in the gains provided by multi-head attention.},
archivePrefix = {arXiv},
arxivId = {1905.10650},
author = {Michel, Paul and Levy, Omer and Neubig, Graham},
eprint = {1905.10650},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Michel, Levy, Neubig - 2019 - Are Sixteen Heads Really Better than One.pdf:pdf},
journal = {arXiv},
month = {may},
publisher = {arXiv},
title = {{Are Sixteen Heads Really Better than One?}},
url = {http://arxiv.org/abs/1905.10650},
year = {2019}
}
@inproceedings{Peters2017,
abstract = {Pre-trained word embeddings learned from unlabeled text have become a standard component of neural network architectures for NLP tasks. However, in most cases, the recurrent network that operates on word-level representations to produce context sensitive representations is trained on relatively little labeled data. In this paper, we demonstrate a general semi-supervised approach for adding pre-trained context embeddings from bidirectional language models to NLP systems and apply it to sequence labeling tasks. We evaluate our model on two standard datasets for named entity recognition (NER) and chunking, and in both cases achieve state of the art results, surpassing previous systems that use other forms of transfer or joint learning with additional labeled data and task specific gazetteers.},
author = {Peters, Matthew E. and Ammar, Waleed and Bhagavatula, Chandra and Power, Russell},
booktitle = {ACL 2017 - 55th Annu. Meet. Assoc. Comput. Linguist. Proc. Conf. (Long Pap.},
doi = {10.18653/v1/P17-1161},
file = {:Users/petravysusilova/Downloads/peters.pdf:pdf},
title = {{Semi-supervised sequence tagging with bidirectional language models}},
volume = {1},
year = {2017}
}
@article{Dong2019,
abstract = {This paper presents a new Unified pre-trained Language Model (UniLM) that can be fine-tuned for both natural language understanding and generation tasks. The model is pre-trained using three types of language modeling tasks: unidirectional, bidirectional, and sequence-to-sequence prediction. The unified modeling is achieved by employing a shared Transformer network and utilizing specific self-attention masks to control what context the prediction conditions on. UniLM compares favorably with BERT on the GLUE benchmark, and the SQuAD 2.0 and CoQA question answering tasks. Moreover, UniLM achieves new state-of-the-art results on five natural language generation datasets, including improving the CNN/DailyMail abstractive summarization ROUGE-L to 40.51 (2.04 absolute improvement), the Gigaword abstractive summarization ROUGE-L to 35.75 (0.86 absolute improvement), the CoQA generative question answering F1 score to 82.5 (37.1 absolute improvement), the SQuAD question generation BLEU-4 to 22.12 (3.75 absolute improvement), and the DSTC7 document-grounded dialog response generation NIST-4 to 2.67 (human performance is 2.65). The code and pre-trained models are available at https://github.com/microsoft/unilm.},
archivePrefix = {arXiv},
arxivId = {1905.03197},
author = {Dong, Li and Yang, Nan and Wang, Wenhui and Wei, Furu and Liu, Xiaodong and Wang, Yu and Gao, Jianfeng and Zhou, Ming and Hon, Hsiao-Wuen},
eprint = {1905.03197},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Dong et al. - 2019 - Unified Language Model Pre-training for Natural Language Understanding and Generation.pdf:pdf},
journal = {arXiv},
month = {may},
publisher = {arXiv},
title = {{Unified Language Model Pre-training for Natural Language Understanding and Generation}},
url = {http://arxiv.org/abs/1905.03197},
year = {2019}
}
@article{Chronopoulou2019,
abstract = {A growing number of state-of-the-art transfer learning methods employ language models pretrained on large generic corpora. In this paper we present a conceptually simple and effective transfer learning approach that addresses the problem of catastrophic forgetting. Specifically, we combine the task-specific optimization function with an auxiliary language model objective, which is adjusted during the training process. This preserves language regularities captured by language models, while enabling sufficient adaptation for solving the target task. Our method does not require pretraining or finetuning separate components of the network and we train our models end-to-end in a single step. We present results on a variety of challenging affective and text classification tasks, surpassing well established transfer learning methods with greater level of complexity.},
archivePrefix = {arXiv},
arxivId = {1902.10547},
author = {Chronopoulou, Alexandra and Baziotis, Christos and Potamianos, Alexandros},
eprint = {1902.10547},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Chronopoulou, Baziotis, Potamianos - 2019 - An Embarrassingly Simple Approach for Transfer Learning from Pretrained Language Models.pdf:pdf},
journal = {NAACL HLT 2019 - 2019 Conf. North Am. Chapter Assoc. Comput. Linguist. Hum. Lang. Technol. - Proc. Conf.},
keywords = {moznosti,necitovane},
mendeley-tags = {moznosti,necitovane},
month = {feb},
pages = {2089--2095},
publisher = {Association for Computational Linguistics (ACL)},
title = {{An Embarrassingly Simple Approach for Transfer Learning from Pretrained Language Models}},
url = {http://arxiv.org/abs/1902.10547},
volume = {1},
year = {2019}
}
@article{Feijo2020,
abstract = {BERT (Bidirectional Encoder Representations from Transformers) and ALBERT (A Lite BERT) are methods for pre-training language models which can later be fine-tuned for a variety of Natural Language Understanding tasks. These methods have been applied to a number of such tasks (mostly in English), achieving results that outperform the state-of-the-art. In this paper, our contribution is twofold. First, we make available our trained BERT and Albert model for Portuguese. Second, we compare our monolingual and the standard multilingual models using experiments in semantic textual similarity, recognizing textual entailment, textual category classification, sentiment analysis, offensive comment detection, and fake news detection, to assess the effectiveness of the generated language representations. The results suggest that both monolingual and multilingual models are able to achieve state-of-the-art and the advantage of training a single language model, if any, is small.},
archivePrefix = {arXiv},
arxivId = {2007.09757},
author = {Feijo, Diego de Vargas and Moreira, Viviane Pereira},
eprint = {2007.09757},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Feijo, Moreira - 2020 - Mono vs Multilingual Transformer-based Models a Comparison across Several Language Tasks.pdf:pdf},
journal = {arXiv},
month = {jul},
title = {{Mono vs Multilingual Transformer-based Models: a Comparison across Several Language Tasks}},
url = {http://arxiv.org/abs/2007.09757},
year = {2020}
}
@article{Ganesh2020,
abstract = {Transformer-based models pre-trained on large-scale corpora achieve state-of-the-art accuracy for natural language processing tasks, but are too resource-hungry and compute-intensive to suit low-capability devices or applications with strict latency requirements. One potential remedy is model compression, which has attracted extensive attention. This paper summarizes the branches of research on compressing Transformers, focusing on the especially popular BERT model. BERT's complex architecture means that a compression technique that is highly effective on one part of the model, e.g., attention layers, may be less successful on another part, e.g., fully connected layers. In this systematic study, we identify the state of the art in compression for each part of BERT, clarify current best practices for compressing large-scale Transformer models, and provide insights into the inner workings of various methods. Our categorization and analysis also shed light on promising future research directions for achieving a lightweight, accurate, and generic natural language processing model.},
archivePrefix = {arXiv},
arxivId = {2002.11985},
author = {Ganesh, Prakhar and Chen, Yao and Lou, Xin and Khan, Mohammad Ali and Yang, Yin and Chen, Deming and Winslett, Marianne and Sajjad, Hassan and Nakov, Preslav},
eprint = {2002.11985},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Ganesh et al. - 2020 - Compressing Large-Scale Transformer-Based Models A Case Study on BERT.pdf:pdf},
journal = {arXiv},
month = {feb},
publisher = {arXiv},
title = {{Compressing Large-Scale Transformer-Based Models: A Case Study on BERT}},
url = {http://arxiv.org/abs/2002.11985},
year = {2020}
}
@techreport{Pruksachatkun,
abstract = {While pretrained models such as BERT have shown large gains across natural language understanding tasks, their performance can be improved by further training the model on a data-rich intermediate task, before fine-tuning it on a target task. However, it is still poorly understood when and why intermediate-task training is beneficial for a given target task. To investigate this, we perform a large-scale study on the pretrained RoBERTa model with 110 intermediate-target task combinations. We further evaluate all trained models with 25 probing tasks meant to reveal the specific skills that drive transfer. We observe that intermediate tasks requiring high-level inference and reasoning abilities tend to work best. We also observe that target task performance is strongly correlated with higher-level abilities such as coreference resolution. However , we fail to observe more granular correlations between probing and target task performance , highlighting the need for further work on broad-coverage probing benchmarks. We also observe evidence that the forgetting of knowledge learned during pretraining may limit our analysis, highlighting the need for further work on transfer learning methods in these settings.},
archivePrefix = {arXiv},
arxivId = {2005.00628v2},
author = {Pruksachatkun, Yada and Phang, Jason and Liu, Haokun and {Mon Htut}, Phu and Zhang, Xiaoyi and Pang, Richard Yuanzhe and Vania, Clara and Kann, Katharina and Bowman, Samuel R},
booktitle = {arxiv.org},
eprint = {2005.00628v2},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Pruksachatkun et al. - Unknown - Intermediate-Task Transfer Learning with Pretrained Models for Natural Language Understanding When and.pdf:pdf},
title = {{Intermediate-Task Transfer Learning with Pretrained Models for Natural Language Understanding: When and Why Does It Work?}},
url = {http://data.quora.com/First-Quora-DatasetRelease-},
year = {2020}
}
@article{Lewis2019,
abstract = {We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most influence end-task performance.},
archivePrefix = {arXiv},
arxivId = {1910.13461},
author = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
eprint = {1910.13461},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Lewis et al. - 2019 - BART Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension.pdf:pdf},
journal = {arXiv},
month = {oct},
publisher = {arXiv},
title = {{BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension}},
url = {http://arxiv.org/abs/1910.13461},
year = {2019}
}
@inproceedings{Ruder2018,
abstract = {Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24{\%} on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100Ã— more data. We open-source our pretrained models and code1},
author = {Howard, Jeremy and Ruder, Sebastian},
booktitle = {ACL 2018 - 56th Annu. Meet. Assoc. Comput. Linguist. Proc. Conf. (Long Pap.},
doi = {10.18653/v1/p18-1031},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Howard, Ruder - 2018 - Universal language model fine-tuning for text classification(2).pdf:pdf},
title = {{Universal language model fine-tuning for text classification}},
volume = {1},
year = {2018}
}
@inproceedings{Stickland2019,
abstract = {Multi-task learning shares information between related tasks, sometimes reducing the number of parameters required. State-of-the-art results across multiple natural language understanding tasks in the GLUE benchmark have previously used transfer from a single large task: unsuper-vised pre-training with BERT, where a separate BERT model was fine-tuned for each task. We explore multi-task approaches that share a single BERT model with a small number of additional task-specific parameters. Using new adaptation modules, PALs or 'projected attention layers', we match the performance of separately fine-tuned models on the GLUE benchmark with {\~{}}7 times fewer parameters, and obtain state-of-the-art results on the Recognizing Textual Entailment dataset.},
author = {Stickland, Asa Cooper and Murray, Iain},
booktitle = {36th Int. Conf. Mach. Learn. ICML 2019},
file = {:Users/petravysusilova/Downloads/stickland19a.pdf:pdf},
title = {{BERT and PALs: Projected attention layers for efficient adaptation in multi-task learning}},
volume = {2019-June},
year = {2019}
}
@article{Zhang2019,
abstract = {Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding. We argue that informative entities in KGs can enhance language representation with external knowledge. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks. The source code of this paper can be obtained from https://github.com/thunlp/ERNIE.},
archivePrefix = {arXiv},
arxivId = {1905.07129},
author = {Zhang, Zhengyan and Han, Xu and Liu, Zhiyuan and Jiang, Xin and Sun, Maosong and Liu, Qun},
eprint = {1905.07129},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Zhang et al. - 2019 - ERNIE Enhanced Language Representation with Informative Entities(2).pdf:pdf},
journal = {ACL 2019 - 57th Annu. Meet. Assoc. Comput. Linguist. Proc. Conf.},
month = {may},
pages = {1441--1451},
publisher = {Association for Computational Linguistics (ACL)},
title = {{ERNIE: Enhanced Language Representation with Informative Entities}},
url = {http://arxiv.org/abs/1905.07129},
year = {2019}
}
@article{Rosa2019,
abstract = {We use the English model of BERT and explore how a deletion of one word in a sentence changes representations of other words. Our hypothesis is that removing a reducible word (e.g. an adjective) does not affect the representation of other words so much as removing e.g. the main verb, which makes the sentence ungrammatical and of "high surprise" for the language model. We estimate reducibilities of individual words and also of longer continuous phrases (word n-grams), study their syntax-related properties, and then also use them to induce full dependency trees.},
archivePrefix = {arXiv},
arxivId = {1906.11511},
author = {Rosa, Rudolf and Mare{\v{c}}ek, David},
eprint = {1906.11511},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Rosa, Mare{\v{c}}ek - 2019 - Inducing Syntactic Trees from BERT Representations.pdf:pdf},
journal = {arXiv},
month = {jun},
publisher = {arXiv},
title = {{Inducing Syntactic Trees from BERT Representations}},
url = {http://arxiv.org/abs/1906.11511},
year = {2019}
}
@article{Raffel2019a,
abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
archivePrefix = {arXiv},
arxivId = {1910.10683},
author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
eprint = {1910.10683},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Raffel et al. - 2019 - Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer(3).pdf:pdf},
journal = {arXiv},
keywords = {T5,attention-based models,deep learning,multi-task learning,natural language processing,transfer learning},
mendeley-tags = {T5},
month = {oct},
pages = {1--67},
publisher = {arXiv},
title = {{Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer}},
url = {http://arxiv.org/abs/1910.10683},
volume = {21},
year = {2019}
}
@article{Tenney2019,
abstract = {Pre-trained text encoders have rapidly advanced the state of the art on many NLP tasks. We focus on one such model, BERT, and aim to quantify where linguistic information is captured within the network. We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lower-level decisions on the basis of disambiguating information from higher-level representations.},
archivePrefix = {arXiv},
arxivId = {1905.05950},
author = {Tenney, Ian and Das, Dipanjan and Pavlick, Ellie},
eprint = {1905.05950},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Tenney, Das, Pavlick - 2019 - BERT Rediscovers the Classical NLP Pipeline.pdf:pdf},
journal = {ACL 2019 - 57th Annu. Meet. Assoc. Comput. Linguist. Proc. Conf.},
keywords = {necitovane},
mendeley-tags = {necitovane},
month = {may},
pages = {4593--4601},
publisher = {Association for Computational Linguistics (ACL)},
title = {{BERT Rediscovers the Classical NLP Pipeline}},
url = {http://arxiv.org/abs/1905.05950},
year = {2019}
}
@article{Virtanen2019,
abstract = {Deep learning-based language models pretrained on large unannotated text corpora have been demonstrated to allow efficient transfer learning for natural language processing, with recent approaches such as the transformer-based BERT model advancing the state of the art across a variety of tasks. While most work on these models has focused on high-resource languages, in particular English, a number of recent efforts have introduced multilingual models that can be fine-tuned to address tasks in a large number of different languages. However, we still lack a thorough understanding of the capabilities of these models, in particular for lower-resourced languages. In this paper, we focus on Finnish and thoroughly evaluate the multilingual BERT model on a range of tasks, comparing it with a new Finnish BERT model trained from scratch. The new language-specific model is shown to systematically and clearly outperform the multilingual. While the multilingual model largely fails to reach the performance of previously proposed methods, the custom Finnish BERT model establishes new state-of-the-art results on all corpora for all reference tasks: part-of-speech tagging, named entity recognition, and dependency parsing. We release the model and all related resources created for this study with open licenses at https://turkunlp.org/finbert .},
archivePrefix = {arXiv},
arxivId = {1912.07076},
author = {Virtanen, Antti and Kanerva, Jenna and Ilo, Rami and Luoma, Jouni and Luotolahti, Juhani and Salakoski, Tapio and Ginter, Filip and Pyysalo, Sampo},
eprint = {1912.07076},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Virtanen et al. - 2019 - Multilingual is not enough BERT for Finnish.pdf:pdf},
journal = {arXiv},
keywords = {bertology,necitovane,practise},
mendeley-tags = {bertology,necitovane,practise},
month = {dec},
title = {{Multilingual is not enough: BERT for Finnish}},
url = {http://arxiv.org/abs/1912.07076},
year = {2019}
}
@article{Clark2019,
abstract = {Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data. Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classifiers). Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to BERT. BERT's attention heads exhibit patterns such as attending to delimiter tokens, specific positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors. We further show that certain attention heads correspond well to linguistic notions of syntax and coreference. For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy. Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in BERT's attention.},
archivePrefix = {arXiv},
arxivId = {1906.04341},
author = {Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D.},
eprint = {1906.04341},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Clark et al. - 2019 - What Does BERT Look At An Analysis of BERT's Attention.pdf:pdf},
journal = {arXiv},
month = {jun},
publisher = {arXiv},
title = {{What Does BERT Look At? An Analysis of BERT's Attention}},
url = {http://arxiv.org/abs/1906.04341},
year = {2019}
}
@inproceedings{Devlin2019,
abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5{\%} (7.7{\%} point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
archivePrefix = {arXiv},
arxivId = {1810.04805},
author = {Devlin, Jacob and Chang, Ming Wei and Lee, Kenton and Toutanova, Kristina},
booktitle = {NAACL HLT 2019 - 2019 Conf. North Am. Chapter Assoc. Comput. Linguist. Hum. Lang. Technol. - Proc. Conf.},
eprint = {1810.04805},
file = {:Users/petravysusilova/Downloads/1810.04805.pdf:pdf},
isbn = {9781950737130},
pages = {4171--4186},
publisher = {Association for Computational Linguistics (ACL)},
title = {{BERT: Pre-training of deep bidirectional transformers for language understanding}},
volume = {1},
year = {2019}
}
@article{RadfordAlec2019,
abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension , and summarization, are typically approached with supervised learning on task-specific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset-matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
author = {{Radford Alec} and {Wu Jeffrey} and {Child Rewon} and {Luan David} and {Amodei Dario} and {Sutskever Ilya}},
file = {:Users/petravysusilova/Downloads/radford2019language.pdf:pdf},
journal = {OpenAI Blog},
number = {8},
title = {{Language Models are Unsupervised Multitask Learners | Enhanced Reader}},
volume = {1},
year = {2019}
}
@article{Liu2019,
abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
archivePrefix = {arXiv},
arxivId = {1907.11692},
author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
eprint = {1907.11692},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Liu et al. - 2019 - RoBERTa A Robustly Optimized BERT Pretraining Approach(2).pdf:pdf},
journal = {arXiv},
keywords = {necitovane},
mendeley-tags = {necitovane},
month = {jul},
publisher = {arXiv},
title = {{RoBERTa: A Robustly Optimized BERT Pretraining Approach}},
url = {http://arxiv.org/abs/1907.11692},
year = {2019}
}
@misc{Du2019,
abstract = {Word Sense Disambiguation (WSD), which aims to identify the correct sense of a given polyseme, is a long-standing problem in NLP. In this paper, we propose to use BERT to extract better polyseme representations for WSD and explore several ways of combining BERT and the classifier. We also utilize sense definitions to train a unified classifier for all words, which enables the model to disambiguate unseen polysemes. Experiments show that our model achieves the state-of-the-art results on the standard English All-word WSD evaluation.},
author = {Du, Jiaju and Qi, Fanchao and Sun, Maosong},
booktitle = {arXiv},
issn = {23318422},
title = {{Using BERT for word sense disambiguation}},
year = {2019}
}
@article{Pan2009,
abstract = {A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classification, regression and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multi-task learning and sample selection bias, as well as co-variate shift. We also explore some potential future issues in transfer learning research.},
author = {Pan, Sinno Jialin and Yang, Qiang},
doi = {10.1109/TKDE.2009.191},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Pan, Yang - 2009 - A Survey on Transfer Learning.pdf:pdf},
journal = {IEEE Trans. Knowl. Data Eng.},
keywords = {Data Mining,Index Terms-Transfer Learning,Machine Learning,Survey},
number = {10},
pages = {1345--1359},
title = {{A Survey on Transfer Learning}},
url = {http://socrates.acadiau.ca/courses/comp/dsilver/NIPS95},
volume = {22},
year = {2010}
}
@article{Kondratyuk2019,
abstract = {We present UDify, a multilingual multi-task model capable of accurately predicting universal part-of-speech, morphological features, lemmas, and dependency trees simultaneously for all 124 Universal Dependencies treebanks across 75 languages. By leveraging a multilingual BERT self-attention model pretrained on 104 languages, we found that fine-tuning it on all datasets concatenated together with simple softmax classifiers for each UD task can result in state-of-the-art UPOS, UFeats, Lemmas, UAS, and LAS scores, without requiring any recurrent or language-specific components. We evaluate UDify for multilingual learning, showing that low-resource languages benefit the most from cross-linguistic annotations. We also evaluate for zero-shot learning, with results suggesting that multilingual training provides strong UD predictions even for languages that neither UDify nor BERT have ever been trained on. Code for UDify is available at https://github.com/hyperparticle/udify.},
archivePrefix = {arXiv},
arxivId = {1904.02099},
author = {Kondratyuk, Dan and Straka, Milan},
eprint = {1904.02099},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Kondratyuk, Straka - 2019 - 75 Languages, 1 Model Parsing Universal Dependencies Universally.pdf:pdf},
journal = {EMNLP-IJCNLP 2019 - 2019 Conf. Empir. Methods Nat. Lang. Process. 9th Int. Jt. Conf. Nat. Lang. Process. Proc. Conf.},
month = {apr},
pages = {2779--2795},
publisher = {Association for Computational Linguistics},
title = {{75 Languages, 1 Model: Parsing Universal Dependencies Universally}},
url = {http://arxiv.org/abs/1904.02099},
year = {2019}
}
@article{Ettinger2019,
abstract = {Pre-training by language modeling has become a popular and successful approach to NLP tasks, but we have yet to understand exactly what linguistic capacities these pre-training processes confer upon models. In this paper we introduce a suite of diagnostics drawn from human language experiments, which allow us to ask targeted questions about the information used by language models for generating predictions in context. As a case study, we apply these diagnostics to the popular BERT model, finding that it can generally distinguish good from bad completions involving shared category or role reversal, albeit with less sensitivity than humans, and it robustly retrieves noun hypernyms, but it struggles with challenging inferences and role-based event prediction -- and in particular, it shows clear insensitivity to the contextual impacts of negation.},
archivePrefix = {arXiv},
arxivId = {1907.13528},
author = {Ettinger, Allyson},
eprint = {1907.13528},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Ettinger - 2019 - What BERT is not Lessons from a new suite of psycholinguistic diagnostics for language models.pdf:pdf},
journal = {arXiv},
month = {jul},
publisher = {arXiv},
title = {{What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models}},
url = {http://arxiv.org/abs/1907.13528},
year = {2019}
}
@article{Dai2019,
abstract = {Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80{\%} longer than RNNs and 450{\%} longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.},
archivePrefix = {arXiv},
arxivId = {1901.02860},
author = {Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V. and Salakhutdinov, Ruslan},
eprint = {1901.02860},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Dai et al. - 2019 - Transformer-XL Attentive Language Models Beyond a Fixed-Length Context.pdf:pdf},
journal = {ACL 2019 - 57th Annu. Meet. Assoc. Comput. Linguist. Proc. Conf.},
month = {jan},
pages = {2978--2988},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context}},
url = {http://arxiv.org/abs/1901.02860},
year = {2019}
}
@inproceedings{McCann2017,
abstract = {Computer vision has benefited from initializing multiple deep layers with weights pretrained on large supervised training sets like ImageNet. Natural language processing (NLP) typically sees initialization of only the lowest layer of deep models with pretrained word vectors. In this paper, we use a deep LSTM encoder from an attentional sequence-to-sequence model trained for machine translation (MT) to contextualize word vectors. We show that adding these context vectors (CoVe) improves performance over using only unsupervised word and character vectors on a wide variety of common NLP tasks: sentiment analysis (SST, IMDb), question classification (TREC), entailment (SNLI), and question answering (SQuAD). For fine-grained sentiment analysis and entailment, CoVe improves performance of our baseline models to the state of the art.},
author = {McCann, Bryan and Bradbury, James and Xiong, Caiming and Socher, Richard},
booktitle = {Adv. Neural Inf. Process. Syst.},
file = {:Users/petravysusilova/Downloads/1708.00107.pdf:pdf},
issn = {10495258},
title = {{Learned in translation: Contextualized word vectors}},
volume = {2017-Decem},
year = {2017}
}
@inproceedings{Hewitt2020,
abstract = {Probes, supervised models trained to predict properties (like parts-of-speech) from representations (like ELMo), have achieved high accuracy on a range of linguistic tasks. But does this mean that the representations encode linguistic structure or just that the probe has learned the linguistic task? In this paper, we propose control tasks, which associate word types with random outputs, to complement linguistic tasks. By construction, these tasks can only be learned by the probe itself. So a good probe, (one that reflects the representation), should be selective, achieving high linguistic task accuracy and low control task accuracy. The selectivity of a probe puts linguistic task accuracy in context with the probe's capacity to memorize from word types. We construct control tasks for English part-of-speech tagging and dependency edge prediction, and show that popular probes on ELMo representations are not selective. We also find that dropout, commonly used to control probe complexity, is ineffective for improving selectivity of MLPs, but that other forms of regularization are effective. Finally, we find that while probes on the first layer of ELMo yield slightly better part-of-speech tagging accuracy than the second, probes on the second layer are substantially more selective, which raises the question of which layer better represents parts-of-speech.},
archivePrefix = {arXiv},
arxivId = {1909.03368},
author = {Hewitt, John and Liang, Percy},
booktitle = {EMNLP-IJCNLP 2019 - 2019 Conf. Empir. Methods Nat. Lang. Process. 9th Int. Jt. Conf. Nat. Lang. Process. Proc. Conf.},
doi = {10.18653/v1/d19-1275},
eprint = {1909.03368},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Hewitt, Liang - 2020 - Designing and interpreting probes with control tasks.pdf:pdf},
isbn = {9781950737901},
pages = {2733--2743},
publisher = {Association for Computational Linguistics},
title = {{Designing and interpreting probes with control tasks}},
year = {2020}
}
@article{Straka2021,
abstract = {We present RobeCzech, a monolingual RoBERTa language representation model trained on Czech data. RoBERTa is a robustly optimized Transformer-based pretraining approach. We show that RobeCzech considerably outperforms equally-sized multilingual and Czech-trained contextualized language representation models, surpasses current state of the art in all five evaluated NLP tasks and reaches state-of-theart results in four of them. The RobeCzech model is released publicly at https://hdl.handle.net/11234/1-3691 and https://huggingface.co/ufal/robeczech-base.},
archivePrefix = {arXiv},
arxivId = {2105.11314},
author = {Straka, Milan and N{\'{a}}plava, Jakub and Strakov{\'{a}}, Jana and Samuel, David},
eprint = {2105.11314},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Straka et al. - 2021 - RobeCzech Czech RoBERTa, a monolingual contextualized language representation model.pdf:pdf},
keywords = {Czech,Czech {\textperiodcentered},RoBER,Robe,Ta,Ta {\textperiodcentered}},
month = {may},
title = {{RobeCzech: Czech RoBERTa, a monolingual contextualized language representation model}},
url = {http://arxiv.org/abs/2105.11314},
year = {2021}
}
@article{Sun2019,
abstract = {Pre-trained language models such as BERT have proven to be highly effective for natural language processing (NLP) tasks. However, the high demand for computing resources in training such models hinders their application in practice. In order to alleviate this resource hunger in large-scale model training, we propose a Patient Knowledge Distillation approach to compress an original large model (teacher) into an equally-effective lightweight shallow network (student). Different from previous knowledge distillation methods, which only use the output from the last layer of the teacher network for distillation, our student model patiently learns from multiple intermediate layers of the teacher model for incremental knowledge extraction, following two strategies: ({\$}i{\$}) PKD-Last: learning from the last {\$}k{\$} layers; and ({\$}ii{\$}) PKD-Skip: learning from every {\$}k{\$} layers. These two patient distillation schemes enable the exploitation of rich information in the teacher's hidden layers, and encourage the student model to patiently learn from and imitate the teacher through a multi-layer distillation process. Empirically, this translates into improved results on multiple NLP tasks with significant gain in training efficiency, without sacrificing model accuracy.},
archivePrefix = {arXiv},
arxivId = {1908.09355},
author = {Sun, Siqi and Cheng, Yu and Gan, Zhe and Liu, Jingjing},
eprint = {1908.09355},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Sun et al. - 2019 - Patient Knowledge Distillation for BERT Model Compression.pdf:pdf},
journal = {EMNLP-IJCNLP 2019 - 2019 Conf. Empir. Methods Nat. Lang. Process. 9th Int. Jt. Conf. Nat. Lang. Process. Proc. Conf.},
month = {aug},
pages = {4323--4332},
publisher = {Association for Computational Linguistics},
title = {{Patient Knowledge Distillation for BERT Model Compression}},
url = {http://arxiv.org/abs/1908.09355},
year = {2019}
}
@article{Yang2020,
abstract = {Pre-trained language models, such as BERT, have achieved significant accuracy gain in many natural language processing tasks. Despite its effectiveness, the huge number of parameters makes training a BERT model computationally very challenging. In this paper, we propose an efficient multi-stage layerwise training (MSLT) approach to reduce the training time of BERT. We decompose the whole training process into several stages. The training is started from a small model with only a few encoder layers and we gradually increase the depth of the model by adding new encoder layers. At each stage, we only train the top (near the output layer) few encoder layers which are newly added. The parameters of the other layers which have been trained in the previous stages will not be updated in the current stage. In BERT training, the backward computation is much more time-consuming than the forward computation, especially in the distributed training setting in which the backward computation time further includes the communication time for gradient synchronization. In the proposed training strategy, only top few layers participate in backward computation, while most layers only participate in forward computation. Hence both the computation and communication efficiencies are greatly improved. Experimental results show that the proposed method can achieve more than 110{\%} training speedup without significant performance degradation.},
archivePrefix = {arXiv},
arxivId = {2011.13635},
author = {Yang, Cheng and Wang, Shengnan and Yang, Chao and Li, Yuechuan and He, Ru and Zhang, Jingqiao},
eprint = {2011.13635},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Yang et al. - 2020 - Progressively Stacking 2.0 A Multi-stage Layerwise Training Method for BERT Training Speedup.pdf:pdf},
journal = {arXiv},
month = {nov},
publisher = {arXiv},
title = {{Progressively Stacking 2.0: A Multi-stage Layerwise Training Method for BERT Training Speedup}},
url = {http://arxiv.org/abs/2011.13635},
year = {2020}
}
@inproceedings{Peters2018,
abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-Trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
archivePrefix = {arXiv},
arxivId = {1802.05365},
author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
booktitle = {NAACL HLT 2018 - 2018 Conf. North Am. Chapter Assoc. Comput. Linguist. Hum. Lang. Technol. - Proc. Conf.},
doi = {10.18653/v1/n18-1202},
eprint = {1802.05365},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Peters et al. - 2018 - Deep contextualized word representations.pdf:pdf},
isbn = {9781948087278},
month = {feb},
pages = {2227--2237},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Deep contextualized word representations}},
url = {http://allennlp.org/elmo},
volume = {1},
year = {2018}
}
@article{Huan2021,
author = {Huan, Liu and Zhixiong, Zhang and Yufei, Wang and Huan, Liu and Zhixiong, Zhang and Yufei, Wang},
doi = {10.11925/INFOTECH.2096-3467.2020.0965},
issn = {2096-3467},
journal = {Data Anal. Knowl. Discov.},
keywords = {BERT,Knowledge Integration,Model Compression,Pre-Training},
month = {feb},
number = {1},
pages = {3--15},
title = {{A Review on Main Optimization Methods of BERT}},
url = {http://manu44.magtech.com.cn/Jwk{\_}infotech{\_}wk3/EN/abstract/abstract4997.shtml},
volume = {5},
year = {2021}
}
@article{Akbik2018,
abstract = {Recent advances in language modeling using recurrent neural networks have made it viable to model language as distributions over characters. By learning to predict the next character on the basis of previous characters, such models have been shown to automatically internalize linguistic concepts such as words, sentences, subclauses and even sentiment. In this paper, we propose to leverage the internal states of a trained character language model to produce a novel type of word embedding which we refer to as contextual string embeddings. Our proposed embeddings have the distinct properties that they (a) are trained without any explicit notion of words and thus fundamentally model words as sequences of characters, and (b) are contextualized by their surrounding text, meaning that the same word will have different embeddings depending on its contextual use. We conduct a comparative evaluation against previous embeddings and find that our embeddings are highly useful for downstream tasks: across four classic sequence labeling tasks we consistently outperform the previous state-of-the-art. In particular, we significantly outperform previous work on English and German named entity recognition (NER), allowing us to report new state-of-the-art F1-scores on the CONLL03 shared task. We release all code and pre-trained language models in a simple-to-use framework to the research community, to enable reproduction of these experiments and application of our proposed embeddings to other tasks: https://github.com/zalandoresearch/flair},
author = {Akbik, Alan and Blythe, Duncan and Vollgraf, Roland},
file = {:Users/petravysusilova/Downloads/C18-1139.pdf:pdf},
journal = {Proc. 27th Int. Conf. Comput. Linguist.},
title = {{Contextual String Embeddings for Sequence Labeling}},
year = {2018}
}
