Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Huang2015,
abstract = {In this paper, we propose a variety of Long Short-Term Memory (LSTM) based models for sequence tagging. These models include LSTM networks, bidirectional LSTM (BI-LSTM) networks, LSTM with a Conditional Random Field (CRF) layer (LSTM-CRF) and bidirectional LSTM with a CRF layer (BI-LSTM-CRF). Our work is the first to apply a bidirectional LSTM CRF (denoted as BI-LSTM-CRF) model to NLP benchmark sequence tagging data sets. We show that the BI-LSTM-CRF model can efficiently use both past and future input features thanks to a bidirectional LSTM component. It can also use sentence level tag information thanks to a CRF layer. The BI-LSTM-CRF model can produce state of the art (or close to) accuracy on POS, chunking and NER data sets. In addition, it is robust and has less dependence on word embedding as compared to previous observations.},
archivePrefix = {arXiv},
arxivId = {1508.01991},
author = {Huang, Zhiheng and Xu, Wei and Yu, Kai},
eprint = {1508.01991},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Huang, Xu, Yu - 2015 - Bidirectional LSTM-CRF Models for Sequence Tagging.pdf:pdf},
month = {aug},
title = {{Bidirectional LSTM-CRF Models for Sequence Tagging}},
url = {http://arxiv.org/abs/1508.01991},
year = {2015}
}
@inproceedings{Arkhipov2019,
abstract = {Our paper addresses the problem of multilingual named entity recognition on the material of 4 languages: Russian, Bulgarian, Czech and Polish. We solve this task using the BERT model. We use a hundred languages multilingual model as base for transfer to the mentioned Slavic languages. Unsupervised pre-training of the BERT model on these 4 languages allows to significantly outperform baseline neural approaches and multilingual BERT. Additional improvement is achieved by extending BERT with a word-level CRF layer. Our system was submitted to BSNLP 2019 Shared Task on Multilingual Named Entity Recognition and took the 1st place in 3 competition metrics out of 4 we participated in. We open-sourced NER models and BERT model pre-trained on the four Slavic languages.},
author = {Arkhipov, Mikhail and Trofimova, Maria and Kuratov, Yuri and Sorokin, Alexey},
doi = {10.18653/v1/w19-3712},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Arkhipov et al. - 2019 - Tuning Multilingual Transformers for Language-Specific Named Entity Recognition.pdf:pdf},
month = {sep},
pages = {89--93},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Tuning Multilingual Transformers for Language-Specific Named Entity Recognition}},
url = {https://github.com/google-research/},
year = {2019}
}
@article{Wang2015,
abstract = {Bidirectional Long Short-Term Memory Recurrent Neural Network (BLSTM-RNN) has been shown to be very effective for tagging sequential data, e.g. speech utterances or handwritten documents. While word embedding has been demoed as a powerful representation for characterizing the statistical properties of natural language. In this study, we propose to use BLSTM-RNN with word embedding for part-of-speech (POS) tagging task. When tested on Penn Treebank WSJ test set, a state-of-the-art performance of 97.40 tagging accuracy is achieved. Without using morphological features, this approach can also achieve a good performance comparable with the Stanford POS tagger.},
archivePrefix = {arXiv},
arxivId = {1510.06168},
author = {Wang, Peilu and Qian, Yao and Soong, Frank K. and He, Lei and Zhao, Hai},
eprint = {1510.06168},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Wang et al. - 2015 - Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Recurrent Neural Network.pdf:pdf},
keywords = {()},
month = {oct},
title = {{Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Recurrent Neural Network}},
url = {http://arxiv.org/abs/1510.06168},
year = {2015}
}
@article{Straka2019a,
abstract = {We present an extensive evaluation of three recently proposed methods for contextualized embeddings on 89 corpora in 54 languages of the Universal Dependencies 2.3 in three tasks: POS tagging, lemmatization, and dependency parsing. Employing the BERT, Flair and ELMo as pretrained embedding inputs in a strong baseline of UDPipe 2.0, one of the best-performing systems of the CoNLL 2018 Shared Task and an overall winner of the EPE 2018, we present a one-to-one comparison of the three contextualized word embedding methods, as well as a comparison with word2vec-like pretrained embeddings and with end-to-end character-level word embeddings. We report state-of-the-art results in all three tasks as compared to results on UD 2.2 in the CoNLL 2018 Shared Task.},
archivePrefix = {arXiv},
arxivId = {1908.07448},
author = {Straka, Milan and Strakov{\'{a}}, Jana and Haji{\v{c}}, Jan},
eprint = {1908.07448},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Straka, Strakov{\'{a}}, Haji{\v{c}} - 2019 - Evaluating Contextualized Embeddings on 54 Languages in POS Tagging, Lemmatization and Dependency Par.pdf:pdf},
month = {aug},
title = {{Evaluating Contextualized Embeddings on 54 Languages in POS Tagging, Lemmatization and Dependency Parsing}},
url = {http://arxiv.org/abs/1908.07448},
year = {2019}
}
@techreport{Horsmann,
abstract = {A recent study by Plank et al. (2016) found that LSTM-based PoS taggers considerably improve over the current state-of-the-art when evaluated on the corpora of the Universal Dependencies project that use a coarse-grained tagset. We replicate this study using a fresh collection of 27 corpora of 21 languages that are annotated with fine-grained tagsets of varying size. Our replication confirms the result in general , and we additionally find that the advantage of LSTMs is even bigger for larger tagsets. However, we also find that for the very large tagsets of morphologically rich languages, hand-crafted morphological lexicons are still necessary to reach state-of-the-art performance.},
author = {Horsmann, Tobias and Zesch, Torsten},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Horsmann, Zesch - Unknown - Do LSTMs really work so well for PoS tagging-A replication study.pdf:pdf},
pages = {727--736},
title = {{Do LSTMs really work so well for PoS tagging?-A replication study}}
}
@techreport{Hladka,
author = {Hladk{\'{a}}},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - Part of Speech Tags for Automatic Tagging and Syntactic Structures 1.pdf:pdf},
title = {{Part of Speech Tags for Automatic Tagging and Syntactic Structures 1}}
}
@techreport{Straka2019b,
abstract = {We present our contribution to the SIGMOR-PHON 2019 Shared Task: Crosslinguality and Context in Morphology, Task 2: contextual morphological analysis and lemmatization. We submitted a modification of the UDPipe 2.0, one of best-performing systems of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies and an overall winner of the The 2018 Shared Task on Extrinsic Parser Evaluation. As our first improvement, we use the pre-trained contextualized embeddings (BERT) as additional inputs to the network; secondly, we use individual morphological features as reg-ularization; and finally, we merge the selected corpora of the same language. In the lemmatization task, our system exceeds all the submitted systems by a wide margin with lemmatization accuracy 95.78 (second best was 95.00, third 94.46). In the morphological analysis, our system placed tightly second: our morphological analysis accuracy was 93.19, the winning system's 93.23.},
author = {Straka, Milan and Strakov{\'{a}}, Jana and Haji{\v{c}}, Jan},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Straka, Strakov{\'{a}}, Haji{\v{c}} - 2019 - Regularization with Morphological Categories, Corpora Merging.pdf:pdf},
pages = {95--103},
title = {{Regularization with Morphological Categories, Corpora Merging}},
url = {https://github.com/google-research/},
year = {2019}
}
@techreport{Plank,
abstract = {Bidirectional long short-term memory (bi-LSTM) networks have recently proven successful for various NLP sequence mod-eling tasks, but little is known about their reliance to input representations, target languages, data set size, and label noise. We address these issues and evaluate bi-LSTMs with word, character, and unicode byte embeddings for POS tagging. We compare bi-LSTMs to traditional POS taggers across languages and data sizes. We also present a novel bi-LSTM model, which combines the POS tagging loss function with an auxiliary loss function that accounts for rare words. The model obtains state-of-the-art performance across 22 languages, and works especially well for morphologically complex languages. Our analysis suggests that bi-LSTMs are less sensitive to training data size and label corruptions (at small noise levels) than previously assumed.},
archivePrefix = {arXiv},
arxivId = {1604.05529v3},
author = {Plank, Barbara and S{\o}gaard, Anders and Goldberg, Yoav},
eprint = {1604.05529v3},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Plank, S{\o}gaard, Goldberg - Unknown - Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary.pdf:pdf},
title = {{Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss}},
url = {https://github.com/clab/cnn}
}
@article{Straka2019,
abstract = {Contextualized embeddings, which capture appropriate word meaning depending on context, have recently been proposed. We evaluate two meth ods for precomputing such embeddings, BERT and Flair, on four Czech text processing tasks: part-of-speech (POS) tagging, lemmatization, dependency pars ing and named entity recognition (NER). The first three tasks, POS tagging, lemmatization and dependency parsing, are evaluated on two corpora: the Prague Dependency Treebank 3.5 and the Universal Dependencies 2.3. The named entity recognition (NER) is evaluated on the Czech Named Entity Corpus 1.1 and 2.0. We report state-of-the-art results for the above mentioned tasks and corpora.},
archivePrefix = {arXiv},
arxivId = {1909.03544},
author = {Straka, Milan and Strakov{\'{a}}, Jana and Haji{\v{c}}, Jan},
eprint = {1909.03544},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Straka, Strakov{\'{a}}, Haji{\v{c}} - 2019 - Czech Text Processing with Contextual Embeddings POS Tagging, Lemmatization, Parsing and NER.pdf:pdf},
journal = {Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics)},
keywords = {BERT,Contextualized embeddings,Czech,Dependency parsing,Flair,Lemmatization,Named entity recognition,POS tagging},
month = {sep},
pages = {137--150},
publisher = {Springer Verlag},
title = {{Czech Text Processing with Contextual Embeddings: POS Tagging, Lemmatization, Parsing and NER}},
url = {http://arxiv.org/abs/1909.03544},
volume = {11697 LNAI},
year = {2019}
}
@inproceedings{Toutanova2003,
abstract = {We present a new part-of-speech tagger that demonstrates the following ideas: (i) explicit use of both preceding and following tag contexts via a dependency network representation, (ii) broad use of lexical features, including jointly conditioning on multiple consecutive words, (iii) effective use of priors in conditional loglinear models, and (iv) Ô¨Åne-grained modeling of unknown word features. Using these ideas together, the resulting tagger gives a 97.24{\%} accuracy on the Penn Treebank WSJ, an error reduction of 4.4{\%} on the best previous single automatically learned tagging result},
address = {Morristown, NJ, USA},
author = {Toutanova, Kristina and Klein, Dan and Manning, Christopher D. and Singer, Yoram},
booktitle = {Proc. 2003 Conf. North Am. Chapter Assoc. Comput. Linguist. Hum. Lang. Technol.  - NAACL '03},
doi = {10.3115/1073445.1073478},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Toutanova et al. - 2003 - Feature-rich part-of-speech tagging with a cyclic dependency network.pdf:pdf},
pages = {173--180},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Feature-rich part-of-speech tagging with a cyclic dependency network}},
url = {http://portal.acm.org/citation.cfm?doid=1073445.1073478},
volume = {1},
year = {2003}
}
