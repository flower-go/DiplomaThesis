Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@techreport{Putra,
abstract = {Compared to English, the amount of labeled data for Indonesian text classification tasks is very small. Recently developed multilingual language models have shown its ability to create multilingual representations effectively. This paper investigates the effect of combining English and Indonesian data on building Indonesian text classification (e.g., sentiment analysis and hate speech) using multilingual language models. Using the feature-based approach, we observe its performance on various data sizes and total added English data. The experiment showed that the addition of English data, especially if the amount of Indonesian data is small, improves performance. Using the fine-tuning approach, we further showed its effectiveness in utilizing the English language to build Indonesian text classification models.},
author = {Putra, Ilham Firdausi and Purwarianti, Ayu and Ai-Vlb, U-Coe},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Putra, Purwarianti, Ai-Vlb - Unknown - Improving Indonesian Text Classification Using Multilingual Language Model.pdf:pdf},
keywords = {Indonesian text,hate speech classification,multilingual language model,sentiment analysis,text classification},
title = {{Improving Indonesian Text Classification Using Multilingual Language Model}},
url = {https://www.yelp.com/dataset}
}
@techreport{Sun,
abstract = {Language model pre-training has proven to be useful in learning universal language representations. As a state-of-the-art language model pre-training model, BERT (Bidirectional En-coder Representations from Transformers) has achieved amazing results in many language understanding tasks. In this paper, we conduct exhaustive experiments to investigate different fine-tuning methods of BERT on text classification task and provide a general solution for BERT fine-tuning. Finally, the proposed solution obtains new state-of-the-art results on eight widely-studied text classification datasets. 1},
archivePrefix = {arXiv},
arxivId = {1905.05583v3},
author = {Sun, Chi and Qiu, Xipeng and Xu, Yige and Huang, Xuanjing},
eprint = {1905.05583v3},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Sun et al. - Unknown - How to Fine-Tune BERT for Text Classification.pdf:pdf},
title = {{How to Fine-Tune BERT for Text Classification?}},
url = {https://github.}
}
@article{Cano2019,
abstract = {In the area of online communication, commerce and transactions, analyzing sentiment polarity of texts written in various natural languages has become crucial. While there have been a lot of contributions in resources and studies for the English language, "smaller" languages like Czech have not received much attention. In this survey, we explore the effectiveness of many existing machine learning algorithms for sentiment analysis of Czech Facebook posts and product reviews. We report the sets of optimal parameter values for each algorithm and the scores in both datasets. We finally observe that support vector machines are the best classifier and efforts to increase performance even more with bagging, boosting or voting ensemble schemes fail to do so.},
archivePrefix = {arXiv},
arxivId = {1901.02780},
author = {{\c{C}}ano, Erion and Bojar, Ond≈ôej},
doi = {10.5220/0007695709730979},
eprint = {1901.02780},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/{\c{C}}ano, Bojar - 2019 - Sentiment Analysis of Czech Texts An Algorithmic Survey.pdf:pdf},
journal = {ICAART 2019 - Proc. 11th Int. Conf. Agents Artif. Intell.},
keywords = {Algorithmic Survey,Czech Text Datasets,Sentiment Analysis,Supervised Learning},
month = {jan},
pages = {973--979},
publisher = {SciTePress},
title = {{Sentiment Analysis of Czech Texts: An Algorithmic Survey}},
url = {http://arxiv.org/abs/1901.02780 http://dx.doi.org/10.5220/0007695709730979},
volume = {2},
year = {2019}
}
