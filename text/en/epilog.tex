\chapter*{Conclusion}
\addcontentsline{toc}{chapter}{Conclusion}
\label{chap:concl}
In this thesis, we implemented Czech \acrlong{pos} tagging, lemmatization and sentiment analysis with the usage of \acrlong{bert}-like architectures. We achieved \acrlong{sota} results in tagging (accuracy 98.57\%) and lemmatization (accuracy 99.00\%) and joint accuracy of these two tasks 98.19\%, which presents the error reduction of 67\% for tagging accuracy and 53\% for lemmatization accuracy compared to previous publicly available model \citep{Strakova}.\footnote{The best publicly available model is to our best knowledge available at \url{https://ufal.mff.cuni.cz/morphodita/users-manual\#czech-morfflex-pdt_model}, which has 95.55\% tagging and 97.96\% lemmatization accuracy.} We also presents new \acrlong{sota} results in sentiment analysis on two used datasets -- \textit{mall} and \textit{csfd}. We also explored various training techniques and showed the good performance of static embeddings compared to any further training of BERT models. This thesis also examines types of errors BERT helps to solve. All code, text and best models are publicly available on GitHub: \url{https://github.com/flower-go/DiplomaThesis}.

\subsubsection{Future Work}
The BERT-like models ale able to transfer knowledge even across very different languages and also previous work suggests, that training on languages from similar family can improve results in all included languages \citep{Arkhipov2019}, therefore one possible improvement can be in both pre-training the new BERT-like model (similarly to \citep{Straka2021}) on joint dataset for i.e., Czech, Slovak and Polish, or use such multilingual data for task-specific training, for example in sentiment analysis. In addition, results with large XLM-Roberta suggests that selecting this architecture for pretraining could be advantageous despite computational demands. Sentiment analysis data are quite small, although they could be retrieved from pairs (review -- star rating) and many Czech companies have access to such data. Experiments with tagging and lemmatization also shows good results of precomputed embedding approach. The question therefore arises as to whether would not be better to simply add precomputed BERT embeddings into existing sentiment analysis solutions.




