Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@techreport{Bengio2003,
abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.},
author = {Bengio, Yoshua and Ducharme, R{\'{e}}jean and Vincent, Pascal and Jauvin, Christian and Ca, Jauvinc@iro Umontreal and Kandola, Jaz and Hofmann, Thomas and Poggio, Tomaso and Shawe-Taylor, John},
booktitle = {J. Mach. Learn. Res.},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Bengio et al. - 2003 - A Neural Probabilistic Language Model.pdf:pdf},
keywords = {Statistical language modeling,artificial neural networks,curse of dimensionality,distributed representation},
pages = {1137--1155},
title = {{A Neural Probabilistic Language Model}},
volume = {3},
year = {2003}
}
@techreport{Wilks,
abstract = {The article surveys fifty years of work in computational language processing and machine translation, and suggests that a great number of the important ideas were present in the earliest days and hampered only back lack of computational power. Sections review the influence of linguistics proper on the computational area, as well as the influence of artificial intelligence and concerns from logic and knowledge representation. Later, corpora and machine readable dictionaries were made available, which in turn made possible the recent statistically-based empirical emphasis in the subject, a trend that began in machine translation under the influence of success in automatic speech processing. Finally, it is suggested that, despite these many influences on the field from outside, there is nonetheless a distinctive process-based computational linguistics and examples are suggested. Keywords: machine translation information retrieval information extraction parsing thesaurus beliefs syntactic structures semantic representations logic statistics question answering summarization psychology word-sense part-of-speech-tagging sense and reference performance case grammar agents computational semantics},
author = {Wilks, Yorick},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Wilks - Unknown - The History of Natural Language Processing and Machine Translation(2).pdf:pdf},
title = {{The History of Natural Language Processing and Machine Translation}}
}
@techreport{Hutchins1996,
author = {Hutchins, John},
booktitle = {books.google.com},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Hutchins - 1996 - ALPAC the (in)famous report.pdf:pdf},
pages = {131--135},
publisher = {The MIT Press},
title = {{ALPAC: the (in)famous report}},
url = {https://books.google.com/books?hl=cs{\&}lr={\&}id=yx3lEVJMBmMC{\&}oi=fnd{\&}pg=PA131{\&}dq=alpac+report{\&}ots=se2vhONMHp{\&}sig=ByL2IgJLxRwF3f6n9bqOPFx88r4},
volume = {14},
year = {1996}
}
@inproceedings{Montoyo2012,
abstract = {In this introduction, we present an overview of the current state of research in the Natural Language Processing tasks of subjectivity and sentiment analysis, as well as their application domains and closely-related research field of emotion detection. Although many definitions exist for these tasks and the research done within their frame spans over approaches with different objectives, we consider subjectivity analysis to deal with the detection of "private states" (opinions, emotions, sentiments, beliefs, speculations) and sentiment analysis as the task of detecting, extracting and classifying opinions and sentiments concerning different topics, as expressed in textual input. After describing the key concepts and research directions in these tasks, we present the main achievements obtained so far and the issues that remain to be tackled. Subsequently, we introduce each of the papers in this volume and present their contribution to the research areas of subjectivity and sentiment analysis. Finally, we conclude on the present state of work in these fields and reflect on the possible future developments. {\textcopyright} 2012 Elsevier B.V. All rights reserved.},
author = {Montoyo, Andr{\'{e}}s and Mart{\'{i}}nez-Barco, Patricio and Balahur, Alexandra},
booktitle = {Decis. Support Syst.},
doi = {10.1016/j.dss.2012.05.022},
issn = {01679236},
keywords = {Emotion detection,Opinion mining,Sentiment analysis,Social media mining,Social network mining,Subjectivity analysis,Text mining},
month = {nov},
number = {4},
pages = {675--679},
title = {{Subjectivity and sentiment analysis: An overview of the current state of the area and envisaged developments}},
volume = {53},
year = {2012}
}
@inproceedings{Cho2014,
abstract = {In this paper, we propose a novel neural network model called RNN Encoder- Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
archivePrefix = {arXiv},
arxivId = {1406.1078},
author = {Cho, Kyunghyun and {Van Merri{\"{e}}nboer}, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
booktitle = {EMNLP 2014 - 2014 Conf. Empir. Methods Nat. Lang. Process. Proc. Conf.},
doi = {10.3115/v1/d14-1179},
eprint = {1406.1078},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Cho et al. - 2014 - Learning phrase representations using RNN encoder-decoder for statistical machine translation.pdf:pdf},
isbn = {9781937284961},
month = {jun},
pages = {1724--1734},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Learning phrase representations using RNN encoder-decoder for statistical machine translation}},
url = {https://arxiv.org/abs/1406.1078v3},
year = {2014}
}
