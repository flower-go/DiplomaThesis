\section{Lemmatization and part-of-speech tagging}
\label{chap:tag}
Lemmatization and \acrshort{pos} tagging tasks are categorized as morphological analysis, share the same architecture and trained network and they will be described together in this section.
\subsection{Task Definition}

\paragraph{\textbf{POS tagging}} \mbox{}\\
\textit{input}: a sequence of  words \\
\textit{output}: tag (for each word), which contains not only part-of-speech (e.g. noun, pronoun, punctuation mark) but also other morphological analysis (case, tense, etc) corresponding to 15-places morphological tagging system by \cite{Hajic2004}. Description of each position can be found in Table \ref{Tab:tagset}.

\paragraph{\textbf{Lemmatization}} \mbox{}\\
\textit{input:} a sequence of words \\
\textit{output:} lemma -- a base form of a given words, for example nominative of singular for nouns or infinitive for verbs. In this work, lemmatization is treated as a classification problem with classes coresponding to generating rules which transform an input word into target lemma. For example of such rules see Figure \ref{fig:lemma_rules}. \\

\begin{table}[!ht]
\centering
\begin{tabular}{ |c|c|c| } 
 \hline
 Position & Name & Description \\ 
 \hline \hline
 1 & POS & Part of speech \\ \hline
 2 & SubPOS & Detailed part of speech \\ \hline
  3 & Gender & Gender \\ \hline
4 & Number & Number \\\hline
  5 & Case & Case \\ \hline
 6 & PossGender & Possessor's gender \\\hline
  7 & PossNumber & Possessor's number \\ \hline
8 & Person & Person \\\hline
  9 & Tense & Tense \\ \hline
 10 & Grade & Degree of comparison\\\hline
  11 & Negation & Negation \\ \hline
 12 & Voice & Voice \\\hline
 13 & Reserve1 & Reserve \\ \hline
14 & Reserve2 & Reserve \\\hline
  15 & Var & Variant, style \\ 
 \hline
\end{tabular}
\caption{Czech morphology developement is dated from 1989 \citep{Hajic2004} 
and in description of words uses 15-places morphological tags as described in this table taken from \url{https://ufal.mff.cuni.cz/pdt2.0/doc/manuals/en/m-layer/html/ch02s02s01.html}. For more detailed  description or for exploration of predictions given by this work is recommended to use website of Institute of Theoretical and Computational linguistics: \url{http://utkl.ff.cuni.cz/~skoumal/morfo/?pos=11\&val=1}}
\label{Tab:tagset}
\end{table}

\paragraph{Metrics} Accuracy is used for the evaluation and is reported separately for several options -- only tags/lemmas, accuracy of joint classification of tags and lemmas, and  also all three variants with an usage of a morphological dictionary (this option is described in more detail in \ref{sub:dataset}).

\begin{figure}[!ht]
\centering
\includegraphics[width=1\textwidth]{../img/lemma_rules}
\protect\caption{
Table 1 from \citep{Straka2019b} presents 10 most common lemma generating rules in English EWT corpus. Each rule has two parts -- casing script for transforming uppercase and lowercase letters, and edit script. Edit script can transform prefix, suffix, or also a root of the word. It uses the Wagner–Fischer algorithm \citep{Wagner}, which finds the longest commont substring between the word and its lemma. Resulting rule is the shortest edit script converting the word into the lemma. More information can be found in \citep{Straka2019b}.
}
\label{fig:lemma_rules}
\end{figure}

\subsection{Related Work}
\subsubsection{Tagging}
This work aims to improve previously published SOTA results for contextualized embeddings in Czech lemmatization and tagging \citep{straka2019czech} and \citep{Straka2021}. POS tagging (for English) is dated back to 1971 with first rule-based approach on Brown Corpus \citep{greene1971automatic}. Good results in POS tagging were achieved after year 2000 using both classical machine learning methods like Hidden Markov Models \citep{tnt} or Support Vector Machines \citep{svmtool}, and perceptrons/neural networks \citep{collins-2002-discriminative}. Actual English \acrshort{sota} known to me is presented in Flair model \citep{Akbik2018}\footnote{More detailed overvirew of English tagging can be found here: https://aclweb.org/aclwiki/}. It is necessary to note that early papers had \acrshort{pos} tagging defined differently than it is in this thesis. They focused only on selecting part of speech (noun, verb etc...), meanwhile the later works (including this thesis) present complex morphological analysis.
\par
One of the first automatic tagging experiments in Czech is described in \citep{Hladka}, which also shows differences between languages with rich inflexion (as Czech,  but also Finnish or Turkish) and ones with more simple morphology (for example English or Spanish). Languages with complicated morphology have incomparably larger set of possible tags -- English has less then one hundred of possible tags, Czech has almost $4,000$ tags.  Current \acrshort{sota} results for tagging (and lemmatization) are presented in \citep{Straka2021}, which uses Czech version of RoBERTa model -- RobeCzech. This is the model also used for some experiments in this work and, as expected,  yields best results. RobeCzech is based upon previous successful morphological analysis with contextual embeddings and BERT-like models \citep{Straka2019b}, \citep{Straka2019a}, \citep{Straka2019}, \citep{Straka2018} (all lastly mentioned models also achieved great results in lemmatization). \par
Although tagging is mostly considered to be a classification into predifined set of tags, the sets themselves can vary. Penn treebank uses a tagset of 54 different tags, which presents parts of speech and additional information like tense or number\footnote{see: \url{https://www.sketchengine.eu/penn-treebank-tagset/}}. There are some differences between this tagset and other English datasets or taggers (e.g., TreeTagger \citep{Schmid95improvementsin} or CLAWS tagset \citep{Chapelle1988TheCA}). All English tagsets are really small comparing to languages like Czech or Turkish. As mentioned before, Czech uses 15-positioning tags, which is  natural solution for such type of language. These positions can be predicted together or for each position separately. First approach creates big tagset but guarantees consistency among positions (e.g. there will be no tense for a noun or a case for a verb). In the case of separate prediction, each position can be treated as a classification problem separately, which causes problems, because the individual parts of tag are not independent. Better approach is to use sequence-to-sequence modelling \citep{Sutskever2014}, which outputs the tag as a sequence of positions and takes into account previously generated position as in \citep{malaviya-etal-2019-simple}.

\subsubsection{Lemmatization}
Lemmatization (both Czech and English) has undergone a similar development as tagging, starting with rule-based approaches and statistical approaches \citep{Plisson}, continuing with neural networks and recently achieving good results with BERT-like models \citep{Kondratyuk2019}.  Lemmatization is typically performed as a sequence-to-sequence model, therefore it takes a word as a sequence of characters and produces a new sequence of characters, which is the lemma. This approach is teoretically better than classification into rules, because it is possible to generate every existing lemma. However, it can generate simply every possible character sequence, which may not be an existing word. Lemmatization as a classification task into edit scripts set firstly appeared in \citep{Chrupala} and was explored further by \citep{Straka2018}. 
Sequence-to-sequence model can be also used for production of edit rules (same rules as used in this work)\citep{chakrabarty2017context}, \citep{muller2015joint} and \citep{Yildiz2019}.

\subsection{Dataset and Preprocessing}
\label{sub:dataset}
Dataset for these tasks is taken from data of Prague Dependency Treebank (PDT) \citep{PDT35}, version 3.5 from year 2018. Data consists of sentences with lemmas and tags. For ambiguous words, data contain all possible analysis. For example, Czech word "psa" has one possible lemma ("pes") but two possible tags because it could be one of two possible grammatical cases -- genitive or accusative. Input data for such word looks as follows: \\
\begin{center}
\texttt{psa pes NNMS2-----A---- NNMS4-----A----}.
\end{center}
Data contains about 1,600 unique tags and 15k different lemma rules. The number of lemmas is significantly smaller than a number of unigue lemmas ($72,000$) \citep{Strakova} or tags, because words with similar morphological function have same way of creating lemma from the word, e.g. words \textit{malého} (=little, accusative,  sg, m.,) and \textit{červeného}  (=red, accusative,  sg, m.,) have the same lemma rule:
\begin{center}
\texttt{$\downarrow$ 0;d\textbrokenbar ---+ý+-+1}.
\end{center} 
\par
Dataset was originally divided into tree parts - train, development and test, which is also used in this work. Input sentences are preprocessed as follows:
\begin{itemize}
\item splitting into sentences and words
\item white space deletion
\item mapping characters and words into numbers -- mapping  words/characters which were found in train dataset into integers (from one to the number of unique words). This means that the network has no information about words/characters which appears only in test or development dataset. All newly appeared words/characters are mapped into one same number (typically $0$) for \textit{UNK} token/character.
\item tokenization -- Tokenizer for corresponding BERT-like model transforms input words into tokens. Each word is transformed into one or more strings, which are converted into numbers. This serves as an input into BERT part of model. To create these input embeddings, the whole sentence for each word is needed as the same words can have different representation in different contexts. More information can be found in \ref{sub:tokens}.
\end{itemize}

\subsection{Architecture and Experiments}

\begin{figure}[!h]
\centering
\includegraphics[width=1\columnwidth]{../img/taggermodel.pdf}
\protect\caption{Tagging and lemmatization joint model architecture.}
\label{pic:lt_arch}
\end{figure}
%TODO kolik je tech lstm a tak,vsechny dimenze (do tabulky)

%For POS tagging, we applied a straightforward model in the lines of Ling et al. (2015) – first rep- resenting each word with its embedding, contextu- alizing them with bidirectional RNNs (Graves and Schmidhuber, 2005), and finally using a softmax classifier to predict the tags. z UDpipe2

The model for lemmatization and tagging is build upon a model (and a code) for previous work on Czech NLP processing with contextual embedding \citep{straka2019czech}. 
Data preprocessing is taken over from the paper as well as the structure of a lemmatizer and a tagger network, which is extended by BERT-like models, hoping for improvements. %TODO k tomu nemam citaci
Previous work showed that training tagging and lemmatization together in one network can be mutually advantageous, so both of these analysis are an output of one network and are trained jointly. Detailed visualisation of network architecture can be found in Figure \ref{pic:lt_arch}. \par The architecture of network can be divided into three parts -- inputs, optional \acrshort{rnn}s, classification head:
\paragraph{Inputs}
An input set of the network consists of five different input types -- characters (charseqs), words (charseq ids), correct responses(word ids), pretrained embeddings and possibly precomputed bert embeddings (depends on the experiment type). Before the further processing of inputs by \acrshort{rnn} cells, two other types of embeddings are created : character-level embeddings and another word embeddings that are, in contrast to BERT and pretrained embeddings, also trained during training process.

\paragraph{RNN cells}
Characterlevel embeddings are further processed via 3 layers of \acrfull{gru} and all inputs (or their embeddings) are processed by recurrent part of network (specifically by \acrfull{lstm} cells).

\paragraph{Classification head(s)}
After the processing by recurrent neural networks, network employs two separate classification heads, one for tagging and another for lemmatization. Both heads use dense layer with tanh activation function to present more non-linearity as used in \citep{Straka2018} and a softmax function for obtaining the probability distribution over target classes. Lemmatization, however, presents another change -- addition of character level data without RNN processing, that are used together with the rest of the weights as an input into softmax following \citep{Straka2018}, as it leads to better performance of lemmatization in the case of shared network between both tasks.

%TODO kondraytuk bere predikci k prvnímu subwordu, jak to delam ja? 


\paragraph{Morphological Dictionary} All classification can be done with or without use of a morphological dictionary MorFlex \citep{11234/1-1834}, that can provide possible pairs \textit{tag-lemma}. If used, generated tag and lemma is a pair with maximal likelihood, but chosen just from the dictionary. This leads to more consistent results. 

%TODO v vysledkove tabulce je RobeCzech a czert v all czech
%TODO do tabulky dat XLM roberta base, protoze ta velka je moc dobra


%TODO nekde mam taky dropouts! zminit
\subsubsection{Experiments}
This part uses all main \textbf{experiment types} as decribed in \ref{sec:expe}: \textit{base, ls, embed, fine, simple} and \textit{full}. Three \textbf{BERT-like models} are used for experiment setup:
\begin{itemize}
\item multilingual BERT (mBERT) \citep{Devlin2019} 
\item XLM-RoBERTa \citep{Conneau2019}), 
\item RobeCzech \citep{Straka2021}.
\end{itemize}
XLM-RoBERTa and mBERT are trained on 100/104 different languages including Czech. RobeCzech is recently published version of RoBERTa,, trained only on Czech data. XLM-RobERTa is used only for embedding and one version of fine-tuning and this model was omitted in other experiements because of weak results and high computational complexity. There exists another monolingual Czech model, Czert \citep{Sido2021}, which uses original BERT architecture and was outperformed by Robeczech \citep{Straka2021}. \par
A selection of layers is made in both ways -- last four layers (\textit{four)} and learning of weighted sum of all layers (\textit{att}). The layer attention is made only for fine-tuning setup and as the weighted sum does not shows a significant benefit, mean of last four layer is the only method used for other experiments.
 \par Learning rate is used as usual for each type of task a and three different learning rate schedules were applied in each combination of hyperparameters: cosine decay \textit{(cos)}, inverted square root decay \textit{(isrd)} and a one epoch warm-up followed by a constant learning rate \textit{(warmup)} inspired by \citep{Kondratyuk2019} and \citep{Ruder2018}. For \textit{embed} experiments, \textit{warmup} is replaced by  a simple division of training into two parts with different learning rates. 
 
 \begin{table}[!h]
 \centering
\begin{tabular}{|l||l|}
\hline
hyperparametr   & \multicolumn{1}{l|}{value} \\ \hline \hline
beta\_2          & 0.99                       \\ \hline
optimizer       & Adam                       \\ \hline
cle\_dim         & 256                        \\ \hline
dropout         & 0.5                        \\ \hline
label smoothing & 0.3                        \\ \hline
rnn\_cell        & LSTM                       \\ \hline
rnn\_cell\_dim    & 512                        \\ \hline
rnn\_layers      & 3                          \\ \hline
we\_dim          & 512                        \\ \hline
word\_dropout    & 0.2                        \\ \hline
batch size & 64 \\ \hline
\end{tabular}
\caption{Hyperparameters of tag and lemmatization common to all experiments (if they make sense in the context of experiments).}
\label{tab:hyp_all}
\end{table}
Batches has size 64, given by the compromise between the pursuit of relatively big batch size and computational resources. Summary of hyperparameters, that are not different across experiments are in the table \ref{tab:hyp_all}. Other hyperparameters for each experiment are in Table \ref{att:1}.
\par 
Reimplementation of \citep{Straka2019a} without any BERT-like model incorporation serves as a baseline.

\subsection{Results and Discussion}
Best presented model (experiment no. 18) achieved the same or better results than current \acrlong{sota} tagging and lemmatization results (table \ref{tab:all_prew}). Complete results are in table \ref{tab:all_res_tl}. \textit{tl\_18} is the version with fine-tuning, Czech monolingual model, RobeCzech, and without layer attention, although the difference from comparable experiment \textit{with} layer attention is insignificant and can be just accidental. The dominance of the Czech model was expected and additional expert knowledge contained in the complicated architecture was also assumed to be better. Experiments also showed that fine-tuning approach achieves better results than full training from the beginning. This may be due to the choice of hyperparameters, especially learning rate, but the standard ones were selected, implying that at best, it is more difficult to find the right parameters for\textit{full} and \textit{simple} variants. Although \textit{simple} experiments presents standard approach of using pretrained BERT models, they turned out being less successful even than \textit{embed} experiments, that are faster to train and less memory intensive. 

\begin{table}[!h]
\centering
  \begin{tabular}{|l||ccc||ccc|}
  \hline
\multirow{2}{*}{Experiment} & \multicolumn{3}{c||}{Without Dictionary}  &
      \multicolumn{3}{c|}{With Dictionary} \\ 
    & Tags & Lemmas & Both & Tags & Lemmas & Both \\ \hline \hline
    \textit{StrakaB} & \textit{97.94} & \textit{98.75} & \textit{97.31} & \textit{98.05} & \textit{98.98} & \textit{97.65 }\\ \hline
   \textit{ StrakaC} & \textit{97.67} & \textit{98.63} & \textit{97.02} & \textit{97.91 }& \textit{98.94} & \textit{97.51} \\ \hline
     \textit{RobeCzech} & \textit{98.43} & \textit{98.79}  & \textit{97.83}  & \textit{98.50} & \textit{\textbf{99.00}}  & \textit{98.11} \\ \hline \hline
      baseline & 97.04  & 98.56  & 96.41 &  97.31   & 98.83  & 96.90 \\ \hline 
    emb(12) & 98.38  &98.79  & 97.80 & 98.48  & 98.99 & 98.10 \\ \hline
    best(18) & \textbf{98.50}  & \textbf{98.80} &\textbf{ 97.90}  & \textbf{98.57}  & \textbf{99.00}  & \textbf{98.19}  \\ \hline  
  \end{tabular}
  \caption{%TODO cite
  \textit{Straka2019B} is the best solution from \citep{Straka2019} paper. \textit{Straka2019C} is a comparable solution  (BERT embeddings only) to \textit{emb}, which was transformed into Tensorflow 2 in this work as a \textit{baseline}. \textit{Emb} is a solution with static BERT embeddings and \textit{best(18) is the best resulting model in this thesis (experiment id = 18). }}
\label{tab:all_prew} 
\end{table}

%TODO vsechny Figure a table se stejnym pismenem
%TODO tabulka results all u taggingu rozdelit nadpis lemmatizace a tagging

\begin{table}[!h]
\resizebox{\columnwidth}{!}{
\begin{tabular}{|l|l|l|l|l|l||llllll|}
\hline
\multicolumn{2}{|c|}{\multirow{2}{*}{Model}} &
  \multirow{2}{*}{EXPE} &
  \multirow{2}{*}{EP} &
  \multirow{2}{*}{LAYERS} &
  \multirow{2}{*}{LR} &
  \multicolumn{2}{c}{Lemmas} &
  \multicolumn{2}{c}{Tags} &
  \multicolumn{2}{c|}{Both} \\
\multicolumn{2}{|l|}{} &  &  &  &  & Raw & Dict & Raw & Dict & Raw & Dict  \\ \hline \hline
0  & NA                           & base                    & A	& NA               & simple & 98.58  & 98.81   & 97.05   & 97.31    & 96.43     & 96.9       \\ \hline
1  & NA                           & ls                      & B	& NA               & simple & 98.55  & 98.81   & 97.12   & 97.34    & 96.51     & 96.94      \\ \hline
2  & \multirow{3}{*}{mBERT}       & \multirow{9}{*}{embed}  & B	& four             & simple & 98.69  & 98.93   & 97.83   & 97.98    & 97.17     & 97.58      \\ \cline{1-1} \cline{4-12}
3  &                              &                         & C	& four                     & cos    & 98.74  & 98.95   & 97.91   & 98.04    & 97.28     & 97.63      \\ \cline{1-1} \cline{4-12}
4  &                              &                         & C	& four                     & isrd   & 98.73  & 98.94   & 97.89   & 98.02    & 97.28     & 97.61      \\ \cline{1-2} \cline{4-12}
5  & \multirow{3}{*}{xlm-Roberta} &                         & B	& four             & simple & 98.57  & 98.8    & 97.33   & 97.54    & 96.68     & 97.12      \\ \cline{1-1} \cline{4-12}
6  &                              &                         & C	& four                     & cos    & 98.6   & 98.83   & 97.45   & 97.62    & 96.81     & 97.21      \\ \cline{1-1} \cline{4-12}
7  &                              &                         & C	& four                     & isrd   & 98.59  & 98.83   & 97.44   & 97.61    & 96.81     & 97.2       \\ \cline{1-2} \cline{4-12}
8  & \multirow{3}{*}{RoBECzech}   &                         & B	& four             & simple & 98.77  & 98.97   & 98.38   & 98.48    & 97.78     & 98.08      \\ \cline{1-1} \cline{4-12}
9  &                              &                         & C	& four                     & cos    & 98.79  & 98.99   & 98.38   & 98.48    & 97.80      & 98.10      \\ \cline{1-1} \cline{4-12}
10 &                              &                         & C	& four                     & isrd   & 98.78  & 98.98   & 98.4    & 98.48    & 97.8      & 98.09      \\ \hline
11 & \multirow{3}{*}{mBERT} & \multirow{9}{*}{fine}         & D	& four     & simple & 98.69 & 98.93 & 97.84 & 97.99 & 97.21 & 97.59 \\ \cline{1-1} \cline{4-12}
12 &                              &                         & E	& four             & cos    & 98.72  & 98.95   & 97.97   & 98.08    & 97.33     & 97.68      \\ \cline{1-1} \cline{4-12}
13 &                              &                         & E	& four             & isrd   & 98.68  & 98.9    & 97.72   & 97.86    & 97.09     & 97.46      \\ \cline{1-2} \cline{4-12}
14 & \multirow{3}{*}{xlm-Roberta} &                         & D	& four     & simple & 98.62  & 98.84   & 97.72   & 97.9     & 97.07     & 97.48      \\ \cline{1-1} \cline{4-12}
15 &                              &                         & E	& four             & cos    & 98.67  & 98.9    & 97.95   & 98.09    & 97.32     & 97.69      \\ \cline{1-1} \cline{4-12}
16 &                              &                         & E	& four             & isrd   & 98.63  & 98.85   & 97.66   & 97.83    & 97.03     & 97.41      \\ \cline{1-2} \cline{4-12}
17 & \multirow{3}{*}{RoBECzech}   &                         & D	& four     & simple & 98.78  & 98.98   & 98.46   & 98.55    & 97.86     & 98.16      \\ \cline{1-1} \cline{4-12}
18 &                              &                         & E	& four             & cos    & \textbf{98.80}   & \textbf{99.00 }     & \textbf{98.50}    & \textbf{98.57}    & \textbf{97.90 }     & \textbf{98.19 }     \\ \cline{1-1} \cline{4-12}
19 &                              &                         & E	& four             & isrd   & 98.76  & 98.95   & 98.33   & 98.41    & 97.72     & 98.02      \\ \cline{1-2} \cline{4-12} \hline
20 & \multirow{3}{*}{mBERT}  &  \multirow{9}{*}{fine att}   & D	& att                       & simple & 98.67  & 98.91   & 97.76   & 97.92    & 97.13     & 97.52      \\ \cline{1-1} \cline{4-12}
21 &                              &                         & E	& att              & cos    & 98.72  & 98.95   & 97.98   & 98.1     & 97.34     & 97.69      \\ \cline{1-1} \cline{4-12}
22 &                              &                         & E	& att              & isrd   & 98.67  & 98.91   & 97.69   & 97.85    & 97.05     & 97.45      \\ \cline{1-2} \cline{4-12}
23 & \multirow{3}{*}{xlm-Roberta} &                         & D	& att      & simple & 98.6   & 98.81   & 97.62   & 97.77    & 96.96     & 97.35      \\ \cline{1-1} \cline{4-12}
24 &                              &                         & E	& att              & cos    & 98.67  & 98.89   & 97.91   & 98.06    & 97.29     & 97.66      \\ \cline{1-1} \cline{4-12}
25 &                              &                         & E	& att              & isrd   & 98.65  & 98.86   & 97.65   & 97.81    & 97.03     & 97.41      \\ \cline{1-2} \cline{4-12}
26 & \multirow{3}{*}{RoBECzech}   &                         & D	& att      & simple & 98.77  & 98.97   & 98.38   & 98.47    & 97.79     & 98.08      \\ \cline{1-1} \cline{4-12}
27 &                              &                         & E	& att              & cos    & \textbf{98.8}   & 98.99   & 98.47   & 98.54    & 97.88     & 98.16      \\ \cline{1-1} \cline{4-12}
28 &                              &                         & E	& att              & isrd   & 98.77  & 98.96   & 98.33   & 98.41    & 97.72     & 98.01      \\ \hline
29 & \multirow{3}{*}{mBERT}       & \multirow{9}{*}{simple} & F	& four                     & warmup & 98.17  &         & 97.32   &          & 96.46     &            \\ \cline{1-1} \cline{4-12}
30 &                              &                         & G	& four                     & cos    & 98.15  &         & 97.39   &          & 96.47     &            \\ \cline{1-1} \cline{4-12}
31 &                              &                         & G	& four                     & isrd   & 98.13  &         & 97.12   &          & 96.29     &            \\ \cline{1-2} \cline{4-12}
35 & \multirow{3}{*}{RoBECzech}   &                         & F	& four                     & warmup & 98.49  &         & 98.28   &          & 97.41     &            \\ \cline{1-1} \cline{4-12}
36 &                              &                         & G	& four                     & cos    & 98.46  &         & 98.30   &          & 97.39     &            \\ \cline{1-1} \cline{4-12}
37 &                              &                         & G	& four                     & isrd   & 98.59  &         & 98.27   &          & 97.53     &            \\ \hline
38 & \multirow{3}{*}{mBERT}       & \multirow{9}{*}{full}   & F	& four                     & warmup & 98.16  & 98.86   & 97.35   & 97.79    & 96.46     & 97.34      \\ \cline{1-1} \cline{4-12}
39 &                              &                         & G	& four                     & cos    & 98.04  & 98.85   & 97.36   & 97.81    & 96.3      & 97.34      \\ \cline{1-1} \cline{4-12}
40 &                              &                         & G	& four                     & isrd   & 98.22  & 98.86   & 97.34   & 97.73    & 96.46     & 97.29      \\ \cline{1-2} \cline{4-12}
44 & \multirow{3}{*}{RoBECzech}   &                         & G	& four                     & warmup & 98.49  & 98.95   & 98.21   & 98.34    & 97.38     & 97.93      \\ \cline{1-1} \cline{4-12}
45 &                              &                         & G	& four                     & cos    & 98.25  & 98.95   & 98.17   & 98.33    & 97.08     & 97.89      \\ \cline{1-1} \cline{4-12}
46 &                              &                         & G	& four                     & isrd   & 98.55  & 98.99   & 98.19   & 98.35    & 97.39     & 97.95      \\ \hline

\end{tabular}
}
\caption{This table presents complete results for tagging and lemmatizationt tasks. Column EP presents number of epochs and corresponding learning rates are explained in Attachement \protect\ref{att:1} }
\label{tab:all_res_tl}
\end{table}

\subsubsection{Error Analysis}
%TODO sjednotit názvy modelů
This section offers a little exploration of difference in error across models. This comparison includes three models: 
\begin{itemize}
\item tl\_18 -- the best model in tagging and lemmatization,
\item tl\_3 -- the best model with mBERT,
\item tl\_1 -- the baseline model with label smoothing.
\end{itemize} 

\paragraph{best vs. baseline}
The best model (tl\_18) improved prediciton in $3,247$ tags and was worse in 421 tag predictions. More than 80\% of newly correctly predicted tags are composed by three tags of speech: NN (noun), AA (adjective) and RR (preposition). Table \ref{att:tags1} presents improved tags with a frequency at least 10. The most frequent tag ( \texttt{NNIS1-----A----}) presents proper names of places (e.g. Jersey, Tenesee), but we can see that other frequent tags are nominatives and accusatives of masculinum, singular,  inainamate (cs: \textit{rod mužský neživotný}) or femininum, plural. These two cases have the same form for mentioned categories, so they are indistinguishable without context, and that is where BERT showed to be very useful. The same situation is with adjectives, again mostly nominative or accusative of the same form, for example words \textit{další} (following) or \textit{stínový} (shadowy). The third category are prepositions that can be connected with both accusative and dative as \textit{na} (on), or \textit{o} (about).

%TODO kolik lemmata

\paragraph{mBERT vs. RobeCzech}
Best mbert is better in 468 tags and worse in 1703 tags than the best model. Most frequent tags improved by \textit{tl\_18} are similar to previous comparison. Nominative and accusative are again the most common cases improved, but the differences between these two models are not so significant. This leads into verbs appearing higher in the table of most frequent tags, although the absolute value of better predictions on verbs is similar to previous comparison. In both situations, improved verbs predictions relate mostly to verbs with the same form in singular and plural of the third-person, e.g. \textit{vyváží} (exports) or \textit{stojí} (stands). Complete table of the most frequent tags is again in \ref{att:tags2}.

%TODO kolik lemmata
%TODO je v necem mBERT lepsi?
