\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}
\section*{Motivation}
People think and communicate in natural languages. They express their opinions, share information and feelings, or persuade others about their ideas, all in natural languages. In the current era of digital technologies, all this information (sadly even information people do not share consciously and with awareness of potential risks) are available online. The amount of data is so enormous that it is not in human power to sort and use them, and that is when \acrfull{nlp} is a necessary step for further processing by computers. For these reasons, many \acrshort{nlp} use cases exist, for example extracting opinions about new products (e.g., via sentiment analysis or topic modeling), using chatbots instead of paying employees in a call center, voice assistance for people with hearing or vision impairment, filtering spam from email, summarizing the content of papers or finding answers in texts. In recent years, neural networks have achieved great success in many areas, for example computer vision, speech recognition, or marketing, and they  get to all areas of research and industry. This work applies the most successful deep learning \acrshort{nlp} methods of recent years to Czech \acrlong{nlp} tasks: \acrfull{pos} tagging, lemmatization and sentiment analysis. First two tasks are low-level tasks used as a part of data processing pipeline for almost every other \acrshort{nlp} tasks. In contrast, sentiment analysis is an example of a task interesting for end user outside the computer science field and this task also demonstrates the help of used models in getting rid of complicated hand-crafted architectures. Tasks were chosen from both semantics and syntax to show how pre-trained multilingual language models can help with different types of \gls{nlp} tasks
\section*{Goals of this work}
This work aims to improve selected \acrfushort{nlp} tasks for Czech with the use of recently published \acrfull{sota} techniques, namely transfer learning of (possibly multilingual) bidirectional language models. This work uses two pre-trained multilingual models (BERT \citep{Devlin2019} and XLM-RoBERTa \citep{Conneau2019}), that were trained in many languages including Czech, and a monolingual Czech variant of RoBERTa called RobeCzech \citep{Straka2021}. Selected tasks are tagging, lemmatization, and sentiment analysis.\par This work builds directly on previous work on tagging and lemmatization contextualized embeddings \citep{straka2019czech}, uses existing datasets for all tasks, and aims to reach new \acrfull{sota} results. In addition to achieving better results, the aim of this work is also to explore some training techniques for transfer learning and compare the results, especially the case of fine-tuning versus full training from the beginning.  The last goal of this work is to produce a set of publicly available models for non-commercial purposes, public source code and an accompanying text, which can serve as an introduction into the problem and a basis for further experiments.
\section*{Text structure}
The following text is divided into four chapters: \hyperref[chap:theandme]{First chapter} presents the theoretical background in \acrshort{nlp} and used \acrfull{ai} methods. This quite general chapter is followed by a description of all performed \hyperref[chap:exp]{experiments}, which is presenting introduction into experiments and thorough description of each implemented task: definition, previous work, state-of-the-art results, methods applied in this work, and their results for \hyperref[chap:tag]{Lemmatization and \acrlong{pos} tagging} and  \hyperref[chap:sent]{Sentiment analysis}. Implementation details like code overview, third-party libraries and informations for personal examination and exploration of presented models can be found in chapter \ref{chap:impl}. Text is closed by a \hyperref[chap:concl]{conclusion} with a summary of contributions and future work proposals.

