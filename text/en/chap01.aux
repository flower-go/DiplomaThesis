\relax 
\providecommand\hyper@newdestlabel[2]{}
\citation{Wilks}
\citation{Hladka}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Theory}{4}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:theandme}{{1}{4}{Theory}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Linguistics and \gls {nlp}}{4}{section.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Morphology}{4}{subsection.1.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Lemmatization task}{4}{section*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Part-of-speech tagging}{4}{section*.4}\protected@file@percent }
\citation{Montoyo2012}
\citation{francis79browncorpus}
\citation{Marcus1993}
\citation{NLTKbook}
\citation{NLTKbook}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Semantics}{5}{subsection.1.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Sentiment analysis}{5}{section*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.3}Language data}{5}{subsection.1.1.3}\protected@file@percent }
\citation{PDT35}
\citation{PDT35}
\citation{oakley1995final}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces  An example of the syntax and dependency tree. The dependency tree, as the name indicates, describes dependencies between words. Such dependencies are of various types; for example, an elephant in the example text is a direct object of the shooting action. A root of such a tree is typically a predicate of the sentence. On the other hand, the syntax tree represents the sentence's syntactic structure according to the grammar. The root of the tree is \textit  {sentence}, which is split into noun and verb phrase. These can be further divided into phrases compound from particular instances of parts of speech (e.g., nouns, adverbs, verbs, prepositions, etc.). \newline  \textit  {Source: \cite  {NLTKbook}}}}{6}{figure.1.1}\protected@file@percent }
\newlabel{fig:trees}{{1.1}{6}{An example of the syntax and dependency tree. The dependency tree, as the name indicates, describes dependencies between words. Such dependencies are of various types; for example, an elephant in the example text is a direct object of the shooting action. A root of such a tree is typically a predicate of the sentence. On the other hand, the syntax tree represents the sentence's syntactic structure according to the grammar. The root of the tree is \textit {sentence}, which is split into noun and verb phrase. These can be further divided into phrases compound from particular instances of parts of speech (e.g., nouns, adverbs, verbs, prepositions, etc.). \newline \textit {Source: \cite {NLTKbook}}}{figure.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces  Prague dependency treebank example \cite  {PDT35} for the sentences: \textit  {Grasshoppers are still in the larvae stadium, crawling only. At this time of the year, it is efficient to fight them using chemicals, but neither the ailing cooperatives nor private farmers can afford them.} czech: \textit  {Saran\IeC {\v c}ata jsou doposud ve stadiu larev a pohybuj\IeC {\'\i } se pouze lezen\IeC {\'\i }m. V tomto obdob\IeC {\'\i } je \IeC {\'u}\IeC {\v c}inn\IeC {\'e} bojovat proti nim chemick\IeC {\'y}mi post\IeC {\v r}iky, ale do\IeC {\v z}\IeC {\'\i }vaj\IeC {\'\i }c\IeC {\'\i } dru\IeC {\v z}stva ani soukrom\IeC {\'\i } roln\IeC {\'\i }ci nemaj\IeC {\'\i } na jejich n\IeC {\'a}kup pot\IeC {\v r}ebn\IeC {\'e} prost\IeC {\v r}edky}. This treebank contains dependency trees, but is is just one of many possibilities. This example is from Prague dependency treebank, which offers different layers of annotations. Red strips over words \textit  {chemick\IeC {\'y}} and \textit  {post\IeC {\v r}ik} marks multiword phrase, conjunction between \textit  {roln\IeC {\'\i }k} and \textit  {dru\IeC {\v z}stvo} is expressed as by one type of nodes, blue lines denotes coreference etc.}}{7}{figure.1.2}\protected@file@percent }
\newlabel{fig:pdt}{{1.2}{7}{Prague dependency treebank example \cite {PDT35} for the sentences: \textit {Grasshoppers are still in the larvae stadium, crawling only. At this time of the year, it is efficient to fight them using chemicals, but neither the ailing cooperatives nor private farmers can afford them.} czech: \textit {Sarančata jsou doposud ve stadiu larev a pohybují se pouze lezením. V tomto období je účinné bojovat proti nim chemickými postřiky, ale dožívající družstva ani soukromí rolníci nemají na jejich nákup potřebné prostředky}. This treebank contains dependency trees, but is is just one of many possibilities. This example is from Prague dependency treebank, which offers different layers of annotations. Red strips over words \textit {chemický} and \textit {postřik} marks multiword phrase, conjunction between \textit {rolník} and \textit {družstvo} is expressed as by one type of nodes, blue lines denotes coreference etc}{figure.1.2}{}}
\citation{Wilks}
\citation{Hutchins}
\citation{Hutchins1996}
\citation{Brown}
\citation{jurafsky2012natural}
\citation{Goodfellow-et-al-2016}
\citation{Bengio2003}
\citation{Schwenk2006}
\citation{Cho2014}
\citation{Sutskever2014}
\citation{Wu2016}
\citation{Goodfellow-et-al-2016}
\citation{McCulloch}
\citation{Rosenblatt1958}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.4}Historical Development}{8}{subsection.1.1.4}\protected@file@percent }
\citation{Minsky2017}
\citation{Goodfellow-et-al-2016}
\citation{Rumelhart}
\citation{Allen19}
\citation{Forcada1997}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Deep Learning}{9}{section.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Deep Learning History}{9}{subsection.1.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces A one-layer perceptron architecture. The result is formed by the application of the activation function on a weighted sum of inputs. Weights are updated during training till it returns satisfactory results. }}{9}{figure.1.3}\protected@file@percent }
\newlabel{pic:perceptron}{{1.3}{9}{A one-layer perceptron architecture. The result is formed by the application of the activation function on a weighted sum of inputs. Weights are updated during training till it returns satisfactory results}{figure.1.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces This picture illustrates XOR problem. Perception can find the correct solution only if the data are linearly separable. It means that they can be divided by a hyperplane. An example of such two-dimensional data can be seen in picture A). The dotted line shows a possible border for separation. Picture B) shows XOR problem. XOR is a logical operation on two boolean variables, which returns true if one variable is True (1) and the other one is False (0), and returns False otherwise. Such data cannot be separated by one hyperplane. Linearly non-separable data can be, for example, separated by a circle (pic. C)}}{9}{figure.1.4}\protected@file@percent }
\newlabel{pic:xor}{{1.4}{9}{This picture illustrates XOR problem. Perception can find the correct solution only if the data are linearly separable. It means that they can be divided by a hyperplane. An example of such two-dimensional data can be seen in picture A). The dotted line shows a possible border for separation. Picture B) shows XOR problem. XOR is a logical operation on two boolean variables, which returns true if one variable is True (1) and the other one is False (0), and returns False otherwise. Such data cannot be separated by one hyperplane. Linearly non-separable data can be, for example, separated by a circle (pic. C)}{figure.1.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces Multilayer perceptron (or feed-forward neural network) is formed input and output layer and a variable number of hidden layers with different sizes. In every layer, the chosen application function is applied to a weighted sum of inputs from the previous layer.}}{10}{figure.1.5}\protected@file@percent }
\newlabel{pic:multilayer}{{1.5}{10}{Multilayer perceptron (or feed-forward neural network) is formed input and output layer and a variable number of hidden layers with different sizes. In every layer, the chosen application function is applied to a weighted sum of inputs from the previous layer}{figure.1.5}{}}
\citation{Mitchell1997}
\citation{Russell1995}
\citation{Tibshirani1996}
\citation{Hoerl1970}
\citation{Szegedy2015}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Machine Learning and Regularization}{11}{subsection.1.2.2}\protected@file@percent }
\newlabel{sub:ml}{{1.2.2}{11}{Machine Learning and Regularization}{subsection.1.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces Figure A) presents overfitting scenario. Figure B) illustrates possible well-generalized solution. }}{12}{figure.1.6}\protected@file@percent }
\newlabel{pic:overfitting}{{1.6}{12}{Figure A) presents overfitting scenario. Figure B) illustrates possible well-generalized solution}{figure.1.6}{}}
\newlabel{eq:lsloss}{{1.1}{12}{Machine Learning and Regularization}{equation.1.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Embeddings}{12}{subsection.1.2.3}\protected@file@percent }
\newlabel{sec:embedd}{{1.2.3}{12}{Embeddings}{subsection.1.2.3}{}}
\newlabel{note1}{{8}{12}{}{Hfootnote.8}{}}
\citation{Bengio2003}
\citation{Ling}
\citation{Mikolov2013}
\citation{Turian2010}
\citation{Pennington}
\citation{Pennington}
\citation{Straka2019a}
\citation{Liu2020}
\citation{Devlin2019}
\citation{Peters2018}
\citation{Yang2019}
\@writefile{toc}{\contentsline {subsubsection}{Non-contextualized embeddings}{13}{section*.6}\protected@file@percent }
\citation{Bahdanau}
\citation{Bahdanau}
\citation{Santos2016}
\@writefile{toc}{\contentsline {subsubsection}{Contextualized embeddings}{14}{section*.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.4}Attention mechanism}{14}{subsection.1.2.4}\protected@file@percent }
\newlabel{sub:attention}{{1.2.4}{14}{Attention mechanism}{subsection.1.2.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.7}{\ignorespaces Figure 3 of \citep  {Bahdanau} }}{14}{figure.1.7}\protected@file@percent }
\newlabel{pic:att_trans}{{1.7}{14}{Figure 3 of \citep {Bahdanau}}{figure.1.7}{}}
\citation{Santos2016}
\citation{Cheng}
\citation{Vaswani2017}
\citation{Vaswani2017}
\@writefile{lof}{\contentsline {figure}{\numberline {1.8}{\ignorespaces Figure 3 of \citep  {Santos2016} }}{15}{figure.1.8}\protected@file@percent }
\newlabel{pic:att_cnn}{{1.8}{15}{Figure 3 of \citep {Santos2016}}{figure.1.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.5}Recurrent Neural Networks}{15}{subsection.1.2.5}\protected@file@percent }
\newlabel{sub:RNN}{{1.2.5}{15}{Recurrent Neural Networks}{subsection.1.2.5}{}}
\citation{Cho2014}
\citation{Hochreiter1997}
\citation{Vaswani2017}
\@writefile{lof}{\contentsline {figure}{\numberline {1.9}{\ignorespaces Self-attention mechanism scheme for one concrete query vector. The result is an embedding, which is improved by the context of the word. This picture illustrates the result for the embedding of the first word (l1) in a four-word long text. Keys, values, and a query are all multiplied by their respective weights ($W_k$, $W_v$, and $W_q$) before any other operation with them. These weights are trained during learning. Dot products between every word and a query are computed. A result is a number for every input word, so four numbers at the end. These numbers are normalized, so the sum of them is equal to 1. These words serve as a weight ($M_x$), which indicates the relationship between the query and every other word. The resulting better embedding for the query is then obtained as a sum of the word embeddings weighted by these obtained weights. }}{16}{figure.1.9}\protected@file@percent }
\newlabel{pic:att_self}{{1.9}{16}{Self-attention mechanism scheme for one concrete query vector. The result is an embedding, which is improved by the context of the word. This picture illustrates the result for the embedding of the first word (l1) in a four-word long text. Keys, values, and a query are all multiplied by their respective weights ($W_k$, $W_v$, and $W_q$) before any other operation with them. These weights are trained during learning. Dot products between every word and a query are computed. A result is a number for every input word, so four numbers at the end. These numbers are normalized, so the sum of them is equal to 1. These words serve as a weight ($M_x$), which indicates the relationship between the query and every other word. The resulting better embedding for the query is then obtained as a sum of the word embeddings weighted by these obtained weights}{figure.1.9}{}}
\citation{rnn_paper}
\citation{Vaswani2017}
\citation{Cho2014}
\citation{Sutskever2014}
\citation{Wu2016}
\@writefile{lof}{\contentsline {figure}{\numberline {1.10}{\ignorespaces Figure 2 from \citep  {Vaswani2017}. }}{17}{figure.1.10}\protected@file@percent }
\newlabel{pic:att_multi}{{1.10}{17}{Figure 2 from \citep {Vaswani2017}}{figure.1.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.6}Transformers}{17}{subsection.1.2.6}\protected@file@percent }
\newlabel{sub:transformers}{{1.2.6}{17}{Transformers}{subsection.1.2.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.11}{\ignorespaces Basic Recurent neural network architecture. It is composed by one rnn cell which recurrently uses informations from previous seen input. For better illustration of working in the time, RNN can be visualised as a chain of cells connected by a result of previous cell. source: \textit  {Picture from https://medium.com/deeplearningbrasilia/deep-learning-recurrent-neural-networks-f9482a24d010}.}}{18}{figure.1.11}\protected@file@percent }
\newlabel{pic:rnn}{{1.11}{18}{Basic Recurent neural network architecture. It is composed by one rnn cell which recurrently uses informations from previous seen input. For better illustration of working in the time, RNN can be visualised as a chain of cells connected by a result of previous cell. source: \textit {Picture from https://medium.com/deeplearningbrasilia/deep-learning-recurrent-neural-networks-f9482a24d010}}{figure.1.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.12}{\ignorespaces Comparison of LSTM and GRU architecture \textit  {http://dprogrammer.org/rnn-lstm-gru}.}}{18}{figure.1.12}\protected@file@percent }
\newlabel{pic:lstm_gru}{{1.12}{18}{Comparison of LSTM and GRU architecture \textit {http://dprogrammer.org/rnn-lstm-gru}}{figure.1.12}{}}
\citation{Russakovsky2015}
\citation{Huh}
\citation{Ruder2019}
\citation{Pan2009}
\citation{Ruder2019}
\citation{Pan2009}
\citation{Feijo2020}
\citation{Hewitt2020}
\citation{Liu2020}
\@writefile{lof}{\contentsline {figure}{\numberline {1.13}{\ignorespaces This picture describes design of one transformers layer. source: http://jalammar.github.io/illustrated-transformer }}{19}{figure.1.13}\protected@file@percent }
\newlabel{pic:enco_deco}{{1.13}{19}{This picture describes design of one transformers layer. source: http://jalammar.github.io/illustrated-transformer}{figure.1.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.7}Transfer learning}{19}{subsection.1.2.7}\protected@file@percent }
\newlabel{sub:models}{{1.2.7}{19}{Transfer learning}{subsection.1.2.7}{}}
\citation{Liu2020}
\citation{Bengio2003}
\citation{Dai2015}
\citation{Ramachandran2017}
\citation{Feijo2020}
\@writefile{lof}{\contentsline {figure}{\numberline {1.14}{\ignorespaces  In transformers, encoder and decoder parts are both composed by many of block of respective types. The input goes first through a series of encoders and than the output of encoder part is put into every decoder in the decoder part. source: http://jalammar.github.io/illustrated-transformer/ }}{20}{figure.1.14}\protected@file@percent }
\newlabel{pic:enco_deco_all}{{1.14}{20}{In transformers, encoder and decoder parts are both composed by many of block of respective types. The input goes first through a series of encoders and than the output of encoder part is put into every decoder in the decoder part. source: http://jalammar.github.io/illustrated-transformer/}{figure.1.14}{}}
\citation{McCann2017}
\citation{Peters2017}
\citation{Peters2018}
\citation{Akbik2018}
\citation{Radfort2018}
\citation{RadfordAlec2019}
\citation{Brown2020}
\citation{McCann2017}
\citation{Peters2017}
\citation{Peters2017}
\@writefile{lof}{\contentsline {figure}{\numberline {1.15}{\ignorespaces Figure from \citep {Ruder2019} offers possible taxonomy for transfer learning. Following definiton in \citep  {Pan2009}, transfer learning's goal is to improve the performance on task $T_1$ from domain $D_1$ by learning knowledge on task $T_0$ from domain $D_0$. Domain is defined as $D = {\chi ,P(X)}$, where $X \in \chi $, $\chi $ is a feature space and $P(X)$ is a marginal probability distribution over the feature space. Transfer learning allows the use of trained models on tasks with different sets of labels or different input data's nature. Input data can vary in the source they come from (wikipedia text versus a novel or a social network posts), they can learn from different features (e.g. different languages) or the distribution of classes is different than it was in the training data (so some highly presented classes in training data are rare in this new task and others are quite common but previously not seen too many times). }}{21}{figure.1.15}\protected@file@percent }
\newlabel{pic:tl_taxonomy}{{1.15}{21}{Figure from \protect \citep {Ruder2019} offers possible taxonomy for transfer learning. Following definiton in \citep {Pan2009}, transfer learning's goal is to improve the performance on task $T_1$ from domain $D_1$ by learning knowledge on task $T_0$ from domain $D_0$. Domain is defined as $D = {\chi ,P(X)}$, where $X \in \chi $, $\chi $ is a feature space and $P(X)$ is a marginal probability distribution over the feature space. Transfer learning allows the use of trained models on tasks with different sets of labels or different input data's nature. Input data can vary in the source they come from (wikipedia text versus a novel or a social network posts), they can learn from different features (e.g. different languages) or the distribution of classes is different than it was in the training data (so some highly presented classes in training data are rare in this new task and others are quite common but previously not seen too many times)}{figure.1.15}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}BERT and its descendants}{21}{section.1.3}\protected@file@percent }
\newlabel{sec:bert}{{1.3}{21}{BERT and its descendants}{section.1.3}{}}
\@writefile{toc}{\contentsline {paragraph}{First attempts}{21}{section*.10}\protected@file@percent }
\citation{Peters2017}
\citation{Devlin2019}
\@writefile{toc}{\contentsline {paragraph}{ELMo}{22}{section*.11}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Flair}{22}{section*.12}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{GPT}{22}{section*.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}BERT}{22}{subsection.1.3.1}\protected@file@percent }
\citation{Devlin2019}
\citation{Devlin2019}
\citation{Devlin2019}
\citation{Wu2016}
\@writefile{toc}{\contentsline {subsubsection}{Input embeddings}{23}{section*.14}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Token embeddings}{23}{section*.15}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.16}{\ignorespaces  Input is representing using tree kinds of embeddings for every input word. Every sequence is also decorated by beggining and ending marker (CLS and SEP token), which are also encoded using a combination of all three embedding types. Source: \textit  {\citep  {Devlin2019}}.}}{24}{figure.1.16}\protected@file@percent }
\newlabel{pic:bert_emb}{{1.16}{24}{Input is representing using tree kinds of embeddings for every input word. Every sequence is also decorated by beggining and ending marker (CLS and SEP token), which are also encoded using a combination of all three embedding types. Source: \textit {\citep {Devlin2019}}}{figure.1.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.17}{\ignorespaces This figure illustrates a transformation of one input sentence (from PDT3) to suit bert input expectations. The sentence is divided into words and then into tokens from wordpiece tokenizer vocabulary. Accents may be removed depending on the used model. The sentence is decorated with special CLS and SEP tokens to mark the beginning and the end of the sentence. All tokens are then converted into numbers. }}{24}{figure.1.17}\protected@file@percent }
\newlabel{pic:bert_inp}{{1.17}{24}{This figure illustrates a transformation of one input sentence (from PDT3) to suit bert input expectations. The sentence is divided into words and then into tokens from wordpiece tokenizer vocabulary. Accents may be removed depending on the used model. The sentence is decorated with special CLS and SEP tokens to mark the beginning and the end of the sentence. All tokens are then converted into numbers}{figure.1.17}{}}
\citation{Devlin2019}
\citation{Liu2019}
\citation{Taylor1953}
\@writefile{toc}{\contentsline {paragraph}{Position embeddings}{25}{section*.16}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Segment embeddings}{25}{section*.17}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Pretraining tasks}{25}{section*.18}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Next Sentence Prediction}{25}{section*.19}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Masked Language Modeling}{25}{section*.20}\protected@file@percent }
\citation{Vaswani2017}
\citation{Devlin2019}
\citation{Devlin2019}
\@writefile{lot}{\contentsline {table}{\numberline {1.1}{\ignorespaces Difference between base and large version of BERT model, as published in \citep  {Devlin2019}. }}{26}{table.1.1}\protected@file@percent }
\newlabel{Tab:base_large}{{1.1}{26}{Difference between base and large version of BERT model, as published in \citep {Devlin2019}}{table.1.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Architecture}{26}{section*.21}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Derived models}{26}{subsection.1.3.2}\protected@file@percent }
\citation{Yang2019a}
\citation{Dai2019}
\citation{Zhang2019}
\citation{Liu2019}
\@writefile{toc}{\contentsline {paragraph}{XLNet}{27}{section*.22}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ERNIE}{27}{section*.23}\protected@file@percent }
\citation{Dong2019}
\citation{Clark2020}
\citation{Raffel2019a}
\@writefile{toc}{\contentsline {paragraph}{RoBERTa}{28}{section*.24}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{UNiLM}{28}{section*.25}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ELECTRA}{28}{section*.26}\protected@file@percent }
\citation{Lewis2019}
\citation{Ganesh2020}
\citation{Lan2019}
\@writefile{toc}{\contentsline {paragraph}{T5}{29}{section*.27}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{BART}{29}{section*.28}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Compression of BERT}{29}{section*.29}\protected@file@percent }
\citation{Liu2020}
\citation{Ruder2018}
\citation{Sun}
\citation{Sun}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.3}How to use language models}{30}{subsection.1.3.3}\protected@file@percent }
\newlabel{sub:howto}{{1.3.3}{30}{How to use language models}{subsection.1.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.4}Why is BERT so special?}{30}{subsection.1.3.4}\protected@file@percent }
\newlabel{sub:specialBert}{{1.3.4}{30}{Why is BERT so special?}{subsection.1.3.4}{}}
\@setckpt{chap01}{
\setcounter{page}{31}
\setcounter{equation}{1}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{14}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{1}
\setcounter{section}{3}
\setcounter{subsection}{4}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{17}
\setcounter{table}{1}
\setcounter{Item}{0}
\setcounter{Hfootnote}{14}
\setcounter{bookmark@seq@number}{20}
\setcounter{@todonotes@numberoftodonotes}{0}
\setcounter{section@level}{2}
\setcounter{parentequation}{0}
\setcounter{FancyVerbLine}{0}
\setcounter{NAT@ctr}{0}
\setcounter{float@type}{4}
\setcounter{su@anzahl}{0}
\setcounter{LT@tables}{0}
\setcounter{LT@chunks}{0}
\setcounter{thm}{0}
\setcounter{defn}{0}
}
