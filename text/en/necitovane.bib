Automatically generated by Mendeley Desktop 1.19.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@inproceedings{Toutanova2003,
abstract = {We present a new part-of-speech tagger that demonstrates the following ideas: (i) explicit use of both preceding and following tag contexts via a dependency network representation, (ii) broad use of lexical features, including jointly conditioning on multiple consecutive words, (iii) effective use of priors in conditional loglinear models, and (iv) ﬁne-grained modeling of unknown word features. Using these ideas together, the resulting tagger gives a 97.24{\%} accuracy on the Penn Treebank WSJ, an error reduction of 4.4{\%} on the best previous single automatically learned tagging result},
address = {Morristown, NJ, USA},
author = {Toutanova, Kristina and Klein, Dan and Manning, Christopher D. and Singer, Yoram},
booktitle = {Proc. 2003 Conf. North Am. Chapter Assoc. Comput. Linguist. Hum. Lang. Technol.  - NAACL '03},
doi = {10.3115/1073445.1073478},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Toutanova et al. - 2003 - Feature-rich part-of-speech tagging with a cyclic dependency network.pdf:pdf},
keywords = {necitovane},
mendeley-tags = {necitovane},
pages = {173--180},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Feature-rich part-of-speech tagging with a cyclic dependency network}},
url = {http://portal.acm.org/citation.cfm?doid=1073445.1073478},
volume = {1},
year = {2003}
}
@misc{Google,
author = {Google},
keywords = {necitovane},
mendeley-tags = {necitovane},
title = {{Google AI Blog: A Neural Network for Machine Translation, at Production Scale}},
url = {https://ai.googleblog.com/2016/09/a-neural-network-for-machine.html},
urldate = {2020-10-25}
}
@article{Santos2016a,
abstract = {In this work, we propose Attentive Pooling (AP), a two-way attention mechanism for discriminative model training. In the context of pair-wise ranking or classification with neural networks, AP enables the pooling layer to be aware of the current input pair, in a way that information from the two input items can directly influence the computation of each other's representations. Along with such representations of the paired inputs, AP jointly learns a similarity measure over projected segments (e.g. trigrams) of the pair, and subsequently, derives the corresponding attention vector for each input to guide the pooling. Our two-way attention mechanism is a general framework independent of the underlying representation learning, and it has been applied to both convolutional neural networks (CNNs) and recurrent neural networks (RNNs) in our studies. The empirical results, from three very different benchmark tasks of question answering/answer selection, demonstrate that our proposed models outperform a variety of strong baselines and achieve state-of-the-art performance in all the benchmarks.},
archivePrefix = {arXiv},
arxivId = {1602.03609},
author = {dos Santos, Cicero and Tan, Ming and Xiang, Bing and Zhou, Bowen},
eprint = {1602.03609},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Santos et al. - 2016 - Attentive Pooling Networks(2).pdf:pdf},
keywords = {necitovane},
mendeley-tags = {necitovane},
month = {feb},
title = {{Attentive Pooling Networks}},
url = {http://arxiv.org/abs/1602.03609},
year = {2016}
}
@techreport{Plank,
abstract = {Bidirectional long short-term memory (bi-LSTM) networks have recently proven successful for various NLP sequence mod-eling tasks, but little is known about their reliance to input representations, target languages, data set size, and label noise. We address these issues and evaluate bi-LSTMs with word, character, and unicode byte embeddings for POS tagging. We compare bi-LSTMs to traditional POS taggers across languages and data sizes. We also present a novel bi-LSTM model, which combines the POS tagging loss function with an auxiliary loss function that accounts for rare words. The model obtains state-of-the-art performance across 22 languages, and works especially well for morphologically complex languages. Our analysis suggests that bi-LSTMs are less sensitive to training data size and label corruptions (at small noise levels) than previously assumed.},
archivePrefix = {arXiv},
arxivId = {1604.05529v3},
author = {Plank, Barbara and S{\o}gaard, Anders and Goldberg, Yoav},
eprint = {1604.05529v3},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Plank, S{\o}gaard, Goldberg - Unknown - Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary.pdf:pdf},
keywords = {necitovane},
mendeley-tags = {necitovane},
title = {{Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss}},
url = {https://github.com/clab/cnn},
year = {2016}
}
@incollection{Sun,
abstract = {Language model pre-training has proven to be useful in learning universal language representations. As a state-of-the-art language model pre-training model, BERT (Bidirectional En-coder Representations from Transformers) has achieved amazing results in many language understanding tasks. In this paper, we conduct exhaustive experiments to investigate different fine-tuning methods of BERT on text classification task and provide a general solution for BERT fine-tuning. Finally, the proposed solution obtains new state-of-the-art results on eight widely-studied text classification datasets. 1},
archivePrefix = {arXiv},
arxivId = {1905.05583v3},
author = {Sun, Chi and Qiu, Xipeng and Xu, Yige and Huang, Xuanjing},
booktitle = {Chinese Comput. Linguist.},
eprint = {1905.05583v3},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Sun et al. - Unknown - How to Fine-Tune BERT for Text Classification.pdf:pdf},
keywords = {bertology,practise},
mendeley-tags = {bertology,practise},
pages = {194--206},
publisher = {Springer International Publishing},
title = {{How to Fine-Tune BERT for Text Classification?}},
url = {https://github.},
year = {2019}
}
@article{Clark2020,
abstract = {Masked language modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK] and then train a model to reconstruct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the original identities of the corrupted tokens, we train a discriminative model that predicts whether each token in the corrupted input was replaced by a generator sample or not. Thorough experiments demonstrate this new pre-training task is more efficient than MLM because the task is defined over all input tokens rather than just the small subset that was masked out. As a result, the contextual representations learned by our approach substantially outperform the ones learned by BERT given the same model size, data, and compute. The gains are particularly strong for small models; for example, we train a model on one GPU for 4 days that outperforms GPT (trained using 30x more compute) on the GLUE natural language understanding benchmark. Our approach also works well at scale, where it performs comparably to RoBERTa and XLNet while using less than 1/4 of their compute and outperforms them when using the same amount of compute.},
archivePrefix = {arXiv},
arxivId = {2003.10555},
author = {Clark, Kevin and Luong, Minh-Thang and Le, Quoc V. and Manning, Christopher D.},
eprint = {2003.10555},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Clark et al. - 2020 - ELECTRA Pre-training Text Encoders as Discriminators Rather Than Generators.pdf:pdf},
journal = {arXiv Prepr.},
keywords = {necitovane},
mendeley-tags = {necitovane},
month = {mar},
title = {{ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators}},
url = {http://arxiv.org/abs/2003.10555},
year = {2020}
}
@article{Wu2020,
abstract = {Multilingual BERT (mBERT) trained on 104 languages has shown surprisingly good cross-lingual performance on several NLP tasks, even without explicit cross-lingual signals. However, these evaluations have focused on cross-lingual transfer with high-resource languages, covering only a third of the languages covered by mBERT. We explore how mBERT performs on a much wider set of languages, focusing on the quality of representation for low-resource languages, measured by within-language performance. We consider three tasks: Named Entity Recognition (99 languages), Part-of-speech Tagging, and Dependency Parsing (54 languages each). mBERT does better than or comparable to baselines on high resource languages but does much worse for low resource languages. Furthermore, monolingual BERT models for these languages do even worse. Paired with similar languages, the performance gap between monolingual BERT and mBERT can be narrowed. We find that better models for low resource languages require more efficient pretraining techniques or more data.},
archivePrefix = {arXiv},
arxivId = {2005.09093},
author = {Wu, Shijie and Dredze, Mark},
eprint = {2005.09093},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Wu, Dredze - 2020 - Are All Languages Created Equal in Multilingual BERT.pdf:pdf},
keywords = {bertology,necitovane},
mendeley-tags = {bertology,necitovane},
month = {may},
pages = {120--130},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Are All Languages Created Equal in Multilingual BERT?}},
url = {http://arxiv.org/abs/2005.09093},
year = {2020}
}
@article{Wang2015,
abstract = {Bidirectional Long Short-Term Memory Recurrent Neural Network (BLSTM-RNN) has been shown to be very effective for tagging sequential data, e.g. speech utterances or handwritten documents. While word embedding has been demoed as a powerful representation for characterizing the statistical properties of natural language. In this study, we propose to use BLSTM-RNN with word embedding for part-of-speech (POS) tagging task. When tested on Penn Treebank WSJ test set, a state-of-the-art performance of 97.40 tagging accuracy is achieved. Without using morphological features, this approach can also achieve a good performance comparable with the Stanford POS tagger.},
archivePrefix = {arXiv},
arxivId = {1510.06168},
author = {Wang, Peilu and Qian, Yao and Soong, Frank K. and He, Lei and Zhao, Hai},
eprint = {1510.06168},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Wang et al. - 2015 - Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Recurrent Neural Network.pdf:pdf},
keywords = {(),necitovane},
mendeley-tags = {necitovane},
month = {oct},
title = {{Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Recurrent Neural Network}},
url = {http://arxiv.org/abs/1510.06168},
year = {2015}
}
@article{Cano2019,
abstract = {In the area of online communication, commerce and transactions, analyzing sentiment polarity of texts written in various natural languages has become crucial. While there have been a lot of contributions in resources and studies for the English language, "smaller" languages like Czech have not received much attention. In this survey, we explore the effectiveness of many existing machine learning algorithms for sentiment analysis of Czech Facebook posts and product reviews. We report the sets of optimal parameter values for each algorithm and the scores in both datasets. We finally observe that support vector machines are the best classifier and efforts to increase performance even more with bagging, boosting or voting ensemble schemes fail to do so.},
archivePrefix = {arXiv},
arxivId = {1901.02780},
author = {{\c{C}}ano, Erion and Bojar, Ondřej},
doi = {10.5220/0007695709730979},
eprint = {1901.02780},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/{\c{C}}ano, Bojar - 2019 - Sentiment Analysis of Czech Texts An Algorithmic Survey.pdf:pdf},
journal = {ICAART 2019 - Proc. 11th Int. Conf. Agents Artif. Intell.},
keywords = {Algorithmic Survey,Czech Text Datasets,Sentiment Analysis,Supervised Learning,necitovane,text},
mendeley-tags = {necitovane,text},
month = {jan},
pages = {973--979},
publisher = {SciTePress},
title = {{Sentiment Analysis of Czech Texts: An Algorithmic Survey}},
url = {http://arxiv.org/abs/1901.02780 http://dx.doi.org/10.5220/0007695709730979},
volume = {2},
year = {2019}
}
@techreport{Hajicova2000,
author = {Haji{\v{c}}ov{\'{a}}, Eva and Panevov{\'{a}}, Jarmila and Sgall, Petr and Ceplov{\'{a}}, M and {Řezn{\'{i}}{\v{c}}kov{\'{a}} Translated by Kirschner}, V Z and Haji{\v{c}}ov{\'{a}}, E and Sgall, P},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Haji{\v{c}}ov{\'{a}} et al. - 2000 - A MANUAL FOR TECTOGRAMMATICAL TAGGING OF THE PRAGUE DEPENDENCY TREEBANK In cooperation with A.pdf:pdf},
keywords = {necitovane},
mendeley-tags = {necitovane},
title = {{A MANUAL FOR TECTOGRAMMATICAL TAGGING OF THE PRAGUE DEPENDENCY TREEBANK In cooperation with A}},
year = {2000}
}
@article{Kittask2020,
abstract = {Recently, large pre-trained language models, such as BERT, have reached state-of-the-art performance in many natural language processing tasks, but for many languages, including Estonian, BERT models are not yet available. However, there exist several multilingual BERT models that can handle multiple languages simultaneously and that have been trained also on Estonian data. In this paper, we evaluate four multilingual models---multilingual BERT, multilingual distilled BERT, XLM and XLM-RoBERTa---on several NLP tasks including POS and morphological tagging, NER and text classification. Our aim is to establish a comparison between these multilingual BERT models and the existing baseline neural models for these tasks. Our results show that multilingual BERT models can generalise well on different Estonian NLP tasks outperforming all baselines models for POS and morphological tagging and text classification, and reaching the comparable level with the best baseline for NER, with XLM-RoBERTa achieving the highest results compared with other multilingual models.},
archivePrefix = {arXiv},
arxivId = {2010.00454},
author = {Kittask, Claudia and Milintsevich, Kirill and Sirts, Kairit},
eprint = {2010.00454},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Kittask, Milintsevich, Sirts - 2020 - Evaluating Multilingual BERT for Estonian.pdf:pdf},
journal = {arXiv},
keywords = {Estonian,NER,POS tagging,multilingual BERT,necitovane,text classification},
mendeley-tags = {necitovane},
month = {oct},
title = {{Evaluating Multilingual BERT for Estonian}},
url = {http://arxiv.org/abs/2010.00454},
year = {2020}
}
@inproceedings{Arkhipov2019,
abstract = {Our paper addresses the problem of multilingual named entity recognition on the material of 4 languages: Russian, Bulgarian, Czech and Polish. We solve this task using the BERT model. We use a hundred languages multilingual model as base for transfer to the mentioned Slavic languages. Unsupervised pre-training of the BERT model on these 4 languages allows to significantly outperform baseline neural approaches and multilingual BERT. Additional improvement is achieved by extending BERT with a word-level CRF layer. Our system was submitted to BSNLP 2019 Shared Task on Multilingual Named Entity Recognition and took the 1st place in 3 competition metrics out of 4 we participated in. We open-sourced NER models and BERT model pre-trained on the four Slavic languages.},
author = {Arkhipov, Mikhail and Trofimova, Maria and Kuratov, Yuri and Sorokin, Alexey},
doi = {10.18653/v1/w19-3712},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Arkhipov et al. - 2019 - Tuning Multilingual Transformers for Language-Specific Named Entity Recognition.pdf:pdf},
keywords = {necitovane},
mendeley-tags = {necitovane},
month = {sep},
pages = {89--93},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Tuning Multilingual Transformers for Language-Specific Named Entity Recognition}},
url = {https://github.com/google-research/},
year = {2019}
}
@article{Chronopoulou2019,
abstract = {A growing number of state-of-the-art transfer learning methods employ language models pretrained on large generic corpora. In this paper we present a conceptually simple and effective transfer learning approach that addresses the problem of catastrophic forgetting. Specifically, we combine the task-specific optimization function with an auxiliary language model objective, which is adjusted during the training process. This preserves language regularities captured by language models, while enabling sufficient adaptation for solving the target task. Our method does not require pretraining or finetuning separate components of the network and we train our models end-to-end in a single step. We present results on a variety of challenging affective and text classification tasks, surpassing well established transfer learning methods with greater level of complexity.},
archivePrefix = {arXiv},
arxivId = {1902.10547},
author = {Chronopoulou, Alexandra and Baziotis, Christos and Potamianos, Alexandros},
eprint = {1902.10547},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Chronopoulou, Baziotis, Potamianos - 2019 - An Embarrassingly Simple Approach for Transfer Learning from Pretrained Language Models.pdf:pdf},
journal = {NAACL HLT 2019 - 2019 Conf. North Am. Chapter Assoc. Comput. Linguist. Hum. Lang. Technol. - Proc. Conf.},
keywords = {moznosti,necitovane},
mendeley-tags = {moznosti,necitovane},
month = {feb},
pages = {2089--2095},
publisher = {Association for Computational Linguistics (ACL)},
title = {{An Embarrassingly Simple Approach for Transfer Learning from Pretrained Language Models}},
url = {http://arxiv.org/abs/1902.10547},
volume = {1},
year = {2019}
}
@inproceedings{Plisson,
abstract = {Lemmatization is the process of finding the normalized form of a word. It is the same as looking for a transformation to apply on a word to get its normalized form. The approach presented in this paper focuses on word endings: what word suffix should be removed and/or added to get the normalized form. This paper compares the results of two word lemmatization algorithms, one based on if-then rules and the other based on ripple down rules induction algorithms. It presents the problem of lemmatization of words from Slovene free text and explains why the Ripple Down Rules (RDR) approach is very well suited for the task. When learning from a corpus of lemmatized Slovene words the RDR approach results in easy to understand rules of improved classification accuracy compared to the results of rule learning achieved in previous work.},
author = {Plisson, Jo{\"{e}}l and Lavrac, Nada and Mladenic, Dunja},
booktitle = {Proc. 7th Int. Multiconference Inf. Soc.},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Plisson, Lavrac, Mladenic - Unknown - A Rule based Approach to Word Lemmatization.pdf:pdf},
keywords = {lemmatization,necitovane},
mendeley-tags = {lemmatization,necitovane},
pages = {83--86},
title = {{A Rule based Approach to Word Lemmatization}},
year = {2004}
}
@article{Hercig2018,
abstract = {Sentiment analysis is a wide area with great potential and many research directions. One direction is stance detection, which is somewhat similar to sentiment analysis. We supplement stance detection dataset with sentiment annotation and explore the similarities of these tasks. We show that stance detection and sentiment analysis can be mutually beneficial by using gold label for one task as features for the other task. We analysed the presence of target entities for stance detection in the dataset. We outperform the state-of-the-art results for stance detection in Czech and set new state-of-the-art results for the newly created sentiment analysis part of the extended dataset.},
author = {Hercig, Tom{\'{a}}{\v{s}} and Krejzl, Peter and Kr{\'{a}}l, Pavel},
doi = {10.13053/CyS-22-3-3014},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Hercig, Krejzl, Kr{\'{a}}l - 2018 - Stance and sentiment in Czech.pdf:pdf},
issn = {20079737},
journal = {Comput. y Sist.},
keywords = {Czech,Natural language processing,Sentiment analysis,Stance detection,necitovane},
mendeley-tags = {necitovane},
number = {3},
pages = {787--794},
publisher = {Instituto Politecnico Nacional},
title = {{Stance and sentiment in Czech}},
url = {http://nlp.kiv.zcu.cz/research/sentiment{\#}stance.},
volume = {22},
year = {2018}
}
@inproceedings{Libovicky,
abstract = {In this work, we focus on three different NLP tasks: image captioning, machine translation, and sentiment analysis. We reimplement successful approaches of other authors and adapt them to the Czech language. We provide end-to-end architectures that achieve state-of-the-art or nearly state-of-the-art results on all of the tasks within a single sequence learning toolkit. The trained models are available both for download as well as in an online demo. 1 End-to-End Training Traditionally, solving tasks such as machine translation or sentiment analysis required complex processing pipelines consisting of tools which transformed one explicit representation of the data into another, with the structure of the internal representations defined by the system designer. In machine translation [24, 6], we would devise explicit word alignment links, extract phrase tables, train a language model, etc.; in sentiment analysis [32, 43], we could label the data with part-of-speech tags, decode their syntactic structure, and/or assign them with semantic labels. All of these more-or-less linguistically motivated internal representations are not inherently required to produce the desired output, but have been devised as clever and useful ways to break down the large and hard task into smaller and manageable substeps. With the advent of end-to-end training of deep neural networks (DNN) [26, 14, 28], the need for most of this has been eliminated. In the end-to-end learning paradigm, there is only one model, directly trained to produce the desired outputs from the inputs, without any explicit intermediate representations. The system designer now only has to design a rather generic architecture of the system. It mostly does not enforce any complex explicit representations and processing steps, but rather offers opportunities for the DNN to devise its own notion of intermediate representations and processing steps through training. This also means that similar architectures can be used to solve very different tasks. Rather than by the nature of the task itself, the structure of the DNN to use is mostly determined by the structure of the input and output -e.g. image inputs are processed by two-dimensional convolutions [27], while text inputs are processed by one-dimensional convolutions, recurrent units [38], and/or attentions [3], typically applied to word or subword embed-dings [5, 10, 33]; classification can produce its output in one step, while text generation is better done iteratively using recurrent decoders; etc. Thanks to that, a single general framework can be used to solve many different tasks. One just needs to transform the inputs and outputs into a suitable format, define an adequate network structure, and let the system train for a few days or weeks. Sadly, the burden of hyperparameter tuning has not been alleviated by DNNs, but rather made worse by the computational costliness of the training. However, with a bit of experience, one is often able to propose a suitable architecture and hyperparameter values at the first attempt, already achieving very competitive results even without any further tuning.},
author = {Libovick{\'{y}}, Jindřich and Rosa, Rudolf and Helcl, Jindřich and Popel, Martin},
booktitle = {Proc. 18th Conf. ITAT 2018 Slov. NLP Work. (SloNLP 2018)},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Libovick{\'{y}} et al. - Unknown - Solving Three Czech NLP Tasks End-to-End with Neural Models.pdf:pdf},
isbn = {11234/12839},
keywords = {necitovane},
mendeley-tags = {necitovane},
pages = {138--143},
publisher = {CreateSpace Independent Publishing Platform, Ko{\v{s}}ice},
title = {{Solving Three Czech NLP Tasks End-to-End with Neural Models}},
url = {https://www.yelp.com/dataset/},
year = {2018}
}
@article{Li,
abstract = {In this paper, we investigate the modeling power of contextualized embeddings from pre-trained language models, e.g. BERT, on the E2E-ABSA task. Specifically, we build a series of simple yet insightful neural base-lines to deal with E2E-ABSA. The experimental results show that even with a simple linear classification layer, our BERT-based architecture can outperform state-of-the-art works. Besides, we also standardize the comparative study by consistently utilizing a hold-out development dataset for model selection, which is largely ignored by previous works. Therefore , our work can serve as a BERT-based benchmark for E2E-ABSA. 1},
author = {Li, Xin and Bing, Lidong and Zhang, Wenxuan and Lam, Wai},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Li et al. - Unknown - Exploiting BERT for End-to-End Aspect-based Sentiment Analysis.pdf:pdf},
journal = {CoRR},
keywords = {necitovane},
mendeley-tags = {necitovane},
title = {{Exploiting BERT for End-to-End Aspect-based Sentiment Analysis *}},
url = {http://arxiv.org/abs/1910.00883},
volume = {abs/1910.0},
year = {2019}
}
@article{Tenney2019,
abstract = {Pre-trained text encoders have rapidly advanced the state of the art on many NLP tasks. We focus on one such model, BERT, and aim to quantify where linguistic information is captured within the network. We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lower-level decisions on the basis of disambiguating information from higher-level representations.},
archivePrefix = {arXiv},
arxivId = {1905.05950},
author = {Tenney, Ian and Das, Dipanjan and Pavlick, Ellie},
eprint = {1905.05950},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Tenney, Das, Pavlick - 2019 - BERT Rediscovers the Classical NLP Pipeline.pdf:pdf},
journal = {ACL 2019 - 57th Annu. Meet. Assoc. Comput. Linguist. Proc. Conf.},
keywords = {necitovane},
mendeley-tags = {necitovane},
month = {may},
pages = {4593--4601},
publisher = {Association for Computational Linguistics (ACL)},
title = {{BERT Rediscovers the Classical NLP Pipeline}},
url = {http://arxiv.org/abs/1905.05950},
year = {2019}
}
@inproceedings{Brill1998,
abstract = {One of the most exciting recent directions in machine learning is the discovery that the combination of multiple classifiers often results in significantly better performance than what can be achieved with a single classifier. In this paper, we first show that the errors made from three different state of the art part of speech taggers are strongly complementary. Next, we show how this complementary behavior can be used to our advantage. By using contextual cues to guide tagger combination, we are able to derive a new tagger that achieves performance significantly greater than any of the individual taggers. Introduction Part of speech tagging has been a central problem in natural language processing for many years. Since the advent of manually tagged corpora such as the Brown Corpus and the Penn Treebank (Francis(1982), Marcus(1993)), the efficacy of machine learning for training a tagger has been demonstrated using a wide array of techniques, including: Markov models, decision trees...},
author = {Brill, Eric and Wu, Jun},
doi = {10.3115/980845.980876},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Brill, Wu - 1998 - Classifier combination for improved lexical disambiguation.pdf:pdf},
keywords = {necitovane},
mendeley-tags = {necitovane},
pages = {191},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Classifier combination for improved lexical disambiguation}},
url = {http://www.cis.upenn.edu/-adwait},
year = {1998}
}
@article{Putra,
abstract = {Compared to English, the amount of labeled data for Indonesian text classification tasks is very small. Recently developed multilingual language models have shown its ability to create multilingual representations effectively. This paper investigates the effect of combining English and Indonesian data on building Indonesian text classification (e.g., sentiment analysis and hate speech) using multilingual language models. Using the feature-based approach, we observe its performance on various data sizes and total added English data. The experiment showed that the addition of English data, especially if the amount of Indonesian data is small, improves performance. Using the fine-tuning approach, we further showed its effectiveness in utilizing the English language to build Indonesian text classification models.},
author = {Putra, Ilham Firdausi and Purwarianti, Ayu and Ai-Vlb, U-Coe},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Putra, Purwarianti, Ai-Vlb - Unknown - Improving Indonesian Text Classification Using Multilingual Language Model.pdf:pdf},
journal = {Int. Conf. Adv. Informatics Concept, Theory Appl.},
keywords = {Indonesian text,hate speech classification,multilingual language model,necitovane,practise,sentiment analysis,text classification},
mendeley-tags = {necitovane,practise},
title = {{Improving Indonesian Text Classification Using Multilingual Language Model}},
url = {https://www.yelp.com/dataset},
year = {2020}
}
@article{Virtanen2019,
abstract = {Deep learning-based language models pretrained on large unannotated text corpora have been demonstrated to allow efficient transfer learning for natural language processing, with recent approaches such as the transformer-based BERT model advancing the state of the art across a variety of tasks. While most work on these models has focused on high-resource languages, in particular English, a number of recent efforts have introduced multilingual models that can be fine-tuned to address tasks in a large number of different languages. However, we still lack a thorough understanding of the capabilities of these models, in particular for lower-resourced languages. In this paper, we focus on Finnish and thoroughly evaluate the multilingual BERT model on a range of tasks, comparing it with a new Finnish BERT model trained from scratch. The new language-specific model is shown to systematically and clearly outperform the multilingual. While the multilingual model largely fails to reach the performance of previously proposed methods, the custom Finnish BERT model establishes new state-of-the-art results on all corpora for all reference tasks: part-of-speech tagging, named entity recognition, and dependency parsing. We release the model and all related resources created for this study with open licenses at https://turkunlp.org/finbert .},
archivePrefix = {arXiv},
arxivId = {1912.07076},
author = {Virtanen, Antti and Kanerva, Jenna and Ilo, Rami and Luoma, Jouni and Luotolahti, Juhani and Salakoski, Tapio and Ginter, Filip and Pyysalo, Sampo},
eprint = {1912.07076},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Virtanen et al. - 2019 - Multilingual is not enough BERT for Finnish.pdf:pdf},
journal = {arXiv},
keywords = {bertology,necitovane,practise},
mendeley-tags = {bertology,necitovane,practise},
month = {dec},
title = {{Multilingual is not enough: BERT for Finnish}},
url = {http://arxiv.org/abs/1912.07076},
year = {2019}
}
@techreport{Goldberg,
author = {Goldberg, Yoav},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Goldberg - Unknown - A Primer on Neural Network Models for Natural Language Processing.pdf:pdf},
keywords = {modeling,necitovane},
mendeley-tags = {modeling,necitovane},
title = {{A Primer on Neural Network Models for Natural Language Processing}},
url = {http://www.cs.biu.},
year = {2015}
}
@article{Liu2019,
abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
archivePrefix = {arXiv},
arxivId = {1907.11692},
author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
eprint = {1907.11692},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Liu et al. - 2019 - RoBERTa A Robustly Optimized BERT Pretraining Approach(2).pdf:pdf},
journal = {arXiv},
keywords = {necitovane},
mendeley-tags = {necitovane},
month = {jul},
publisher = {arXiv},
title = {{RoBERTa: A Robustly Optimized BERT Pretraining Approach}},
url = {http://arxiv.org/abs/1907.11692},
year = {2019}
}
@techreport{Straka2019b,
abstract = {We present our contribution to the SIGMOR-PHON 2019 Shared Task: Crosslinguality and Context in Morphology, Task 2: contextual morphological analysis and lemmatization. We submitted a modification of the UDPipe 2.0, one of best-performing systems of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies and an overall winner of the The 2018 Shared Task on Extrinsic Parser Evaluation. As our first improvement, we use the pre-trained contextualized embeddings (BERT) as additional inputs to the network; secondly, we use individual morphological features as reg-ularization; and finally, we merge the selected corpora of the same language. In the lemmatization task, our system exceeds all the submitted systems by a wide margin with lemmatization accuracy 95.78 (second best was 95.00, third 94.46). In the morphological analysis, our system placed tightly second: our morphological analysis accuracy was 93.19, the winning system's 93.23.},
author = {Straka, Milan and Strakov{\'{a}}, Jana and Haji{\v{c}}, Jan},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Straka, Strakov{\'{a}}, Haji{\v{c}} - 2019 - Regularization with Morphological Categories, Corpora Merging.pdf:pdf},
keywords = {necitovane},
mendeley-tags = {necitovane},
pages = {95--103},
title = {{Regularization with Morphological Categories, Corpora Merging}},
url = {https://github.com/google-research/},
year = {2019}
}
@techreport{Hajic,
abstract = {Statistical modeling is now the prevailing method using in automatic procedures of analysis of a natural language. Such an analysis can be performed at various levels, from phonetics to semantics. Two levels of representation are described: a morphological one and a syntactic one that is further subdivided into a surface syntax and deep syntax (tectogrammatics). The role of linguistically annotated corpora will be stressed as a necessary prerequisite for any supervised machine learning algorithms, showing examples from the Prague Dependency Treebank (PDT) being developed at Charles University, Prague. A possible application of some of the tools created during (and thanks to) the development of the PDT will be shown, namely, a machine translation system translating from Czech to Slovak.},
author = {Haji{\v{c}}, Jan},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Haji{\v{c}} - Unknown - Statistick{\'{e}} modelov{\'{a}}n{\'{i}} a automatick{\'{a}} anal{\'{y}}za přirozen{\'{e}}ho jazyka (morfologie, syntax, překlad).pdf:pdf},
keywords = {necitovane},
mendeley-tags = {necitovane},
title = {{Statistick{\'{e}} modelov{\'{a}}n{\'{i}} a automatick{\'{a}} anal{\'{y}}za přirozen{\'{e}}ho jazyka (morfologie, syntax, překlad)}}
}
@article{Straka2018,
abstract = {UDPipe is a trainable pipeline which performs sentence segmentation, tokeniza-tion, POS tagging, lemmatization and dependency parsing (Straka et al., 2016). We present a prototype for UDPipe 2.0 and evaluate it in the CoNLL 2018 UD Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, which employs three metrics for submission ranking. Out of 26 participants, the prototype placed first in the MLAS ranking, third in the LAS ranking and third in the BLEX ranking. In extrinsic parser evaluation EPE 2018, the system ranked first in the overall score. The prototype utilizes an artificial neu-ral network with a single joint model for POS tagging, lemmatization and dependency parsing, and is trained only using the CoNLL-U training data and pretrained word embeddings, contrary to both systems surpassing the prototype in the LAS and BLEX ranking in the shared task. The open-source code of the prototype is available at http://github.com/ CoNLL-UD-2018/UDPipe-Future. After the shared task, we slightly refined the model architecture, resulting in better performance both in the intrinsic evaluation (corresponding to first, second and second rank in MLAS, LAS and BLEX shared task metrics) and the extrinsic evaluation. The improved models will be available shortly in UDPipe at},
author = {Straka, Milan},
doi = {10.18653/v1/K18-2020},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Straka - Unknown - UDPipe 2.0 Prototype at CoNLL 2018 UD Shared Task.pdf:pdf},
journal = {Proc. CoNLL 2018 Shar. Task Multiling. Parsing from Raw Text to Univers. Depend.},
keywords = {necitovane},
mendeley-tags = {necitovane},
pages = {197--207},
title = {{UDPipe 2.0 Prototype at CoNLL 2018 UD Shared Task}},
url = {http://ufal.mff.cuni.cz/udpipe.},
year = {2018}
}
@article{Straka2019a,
abstract = {We present an extensive evaluation of three recently proposed methods for contextualized embeddings on 89 corpora in 54 languages of the Universal Dependencies 2.3 in three tasks: POS tagging, lemmatization, and dependency parsing. Employing the BERT, Flair and ELMo as pretrained embedding inputs in a strong baseline of UDPipe 2.0, one of the best-performing systems of the CoNLL 2018 Shared Task and an overall winner of the EPE 2018, we present a one-to-one comparison of the three contextualized word embedding methods, as well as a comparison with word2vec-like pretrained embeddings and with end-to-end character-level word embeddings. We report state-of-the-art results in all three tasks as compared to results on UD 2.2 in the CoNLL 2018 Shared Task.},
archivePrefix = {arXiv},
arxivId = {1908.07448},
author = {Straka, Milan and Strakov{\'{a}}, Jana and Haji{\v{c}}, Jan},
eprint = {1908.07448},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Straka, Strakov{\'{a}}, Haji{\v{c}} - 2019 - Evaluating Contextualized Embeddings on 54 Languages in POS Tagging, Lemmatization and Dependency Par.pdf:pdf},
journal = {arXiv},
keywords = {necitovane},
mendeley-tags = {necitovane},
month = {aug},
title = {{Evaluating Contextualized Embeddings on 54 Languages in POS Tagging, Lemmatization and Dependency Parsing}},
url = {http://arxiv.org/abs/1908.07448},
year = {2019}
}
@techreport{Hajic-BarboraHladka,
author = {{Haji{\v{c}} -Barbora Hladk{\'{a}}}, Jan},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Haji{\v{c}} -Barbora Hladk{\'{a}} - Unknown - Morfologick{\'{e}} zna{\v{c}}kov{\'{a}}n{\'{i}} korpusu {\v{c}}esk{\'{y}}ch textů stochastickou metodou.pdf:pdf},
keywords = {necitovane},
mendeley-tags = {necitovane},
title = {{Morfologick{\'{e}} zna{\v{c}}kov{\'{a}}n{\'{i}} korpusu {\v{c}}esk{\'{y}}ch textů stochastickou metodou}}
}
@inproceedings{Horsmann,
abstract = {A recent study by Plank et al. (2016) found that LSTM-based PoS taggers considerably improve over the current state-of-the-art when evaluated on the corpora of the Universal Dependencies project that use a coarse-grained tagset. We replicate this study using a fresh collection of 27 corpora of 21 languages that are annotated with fine-grained tagsets of varying size. Our replication confirms the result in general , and we additionally find that the advantage of LSTMs is even bigger for larger tagsets. However, we also find that for the very large tagsets of morphologically rich languages, hand-crafted morphological lexicons are still necessary to reach state-of-the-art performance.},
author = {Horsmann, Tobias and Zesch, Torsten},
booktitle = {EMNLP 2017 - Conf. Empir. Methods Nat. Lang. Process. Proc.},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Horsmann, Zesch - Unknown - Do LSTMs really work so well for PoS tagging-A replication study.pdf:pdf},
keywords = {necitovane},
mendeley-tags = {necitovane},
pages = {727--736},
title = {{Do LSTMs really work so well for PoS tagging?-A replication study}},
year = {2017}
}
