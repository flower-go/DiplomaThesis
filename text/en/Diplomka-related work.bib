Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Kittask2020,
abstract = {Recently, large pre-trained language models, such as BERT, have reached state-of-the-art performance in many natural language processing tasks, but for many languages, including Estonian, BERT models are not yet available. However, there exist several multilingual BERT models that can handle multiple languages simultaneously and that have been trained also on Estonian data. In this paper, we evaluate four multilingual models---multilingual BERT, multilingual distilled BERT, XLM and XLM-RoBERTa---on several NLP tasks including POS and morphological tagging, NER and text classification. Our aim is to establish a comparison between these multilingual BERT models and the existing baseline neural models for these tasks. Our results show that multilingual BERT models can generalise well on different Estonian NLP tasks outperforming all baselines models for POS and morphological tagging and text classification, and reaching the comparable level with the best baseline for NER, with XLM-RoBERTa achieving the highest results compared with other multilingual models.},
archivePrefix = {arXiv},
arxivId = {2010.00454},
author = {Kittask, Claudia and Milintsevich, Kirill and Sirts, Kairit},
eprint = {2010.00454},
file = {:Users/petravysusilova/Library/Application Support/Mendeley Desktop/Downloaded/Kittask, Milintsevich, Sirts - 2020 - Evaluating Multilingual BERT for Estonian.pdf:pdf},
keywords = {Estonian,NER,POS tagging,multilingual BERT,text classification},
month = {oct},
title = {{Evaluating Multilingual BERT for Estonian}},
url = {http://arxiv.org/abs/2010.00454},
year = {2020}
}
